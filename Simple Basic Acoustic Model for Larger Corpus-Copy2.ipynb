{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In speech processing we may use really huge networks cause majority of the processing bottleneck is on CPU and running the CTC function...\n",
    "\n",
    "Also, CLARIN mobile corpus seems to be too small for advanced network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5523 5536\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python3\n",
    "\n",
    "# For demonstration purposes - Pawe≈Ç Tomasik\n",
    "\n",
    "# for CLARIN_MOBILE - generally it is unnormalized\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Conv1D, Dropout, LeakyReLU, Dense, Input, Lambda, TimeDistributed, Flatten, Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.initializers import Orthogonal\n",
    "\n",
    "from keras.backend import ctc_batch_cost, expand_dims\n",
    "import keras.backend as K\n",
    "\n",
    "import editdistance  # For digit error rate\n",
    "import keras\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "LENGTH = 1904\n",
    "RECS = 1384 * 4\n",
    "TRANSL = 252\n",
    "\n",
    "X = np.zeros([RECS, LENGTH, 257], np.float32)\n",
    "Y = np.zeros([RECS, TRANSL], np.int16)\n",
    "counter = 0\n",
    "\n",
    "for i in range(4):\n",
    "    Xpart = np.load(\"datasets/clarin-long/data/clarin-rec-{}.npy\".format(i))\n",
    "    Ypart = np.load(\"datasets/clarin-long/data/clarin-trans-{}.npy\".format(i))\n",
    "    recs = Xpart.shape[0]\n",
    "    reclen = Xpart.shape[1]\n",
    "    translen = Ypart.shape[1]\n",
    "    X[counter : counter + recs, :reclen, :] = Xpart\n",
    "    Y[counter : counter + recs, :translen] = Ypart\n",
    "    counter += recs\n",
    "    \n",
    "print(counter, RECS)\n",
    "counter //= 32\n",
    "counter *= 32\n",
    "\n",
    "X = X[:counter]\n",
    "Y = Y[:counter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5504, 1904, 257), (5504, 252), (5504, 1), (5504, 1)]\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, None, 257)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, None, 257, 1) 0           input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, None, 253, 5) 30          lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, None, 253, 5) 0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, None, 253, 5) 0           leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, None, 1265)   0           dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, None, 256)    1619456     time_distributed_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, None, 256)    0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, None, 256)    0           leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   (None, None, 256)    525312      dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, None, 256)    65792       dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, None, 256)    0           lstm_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, None, 256)    0           dense_5[0][0]                    \n",
      "                                                                 dropout_15[0][0]                 \n",
      "                                                                 dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 252)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  (None, None, 38)     44840       lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           input_10[0][0]                   \n",
      "                                                                 lstm_10[0][0]                    \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 2,255,430\n",
      "Trainable params: 2,255,430\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1000\n",
      "5504/5504 [==============================] - 410s 74ms/step - loss: 445.0456\n",
      "Epoch 2/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 411.8779\n",
      "Epoch 3/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 405.3813\n",
      "Epoch 4/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 382.7595\n",
      "Epoch 5/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 352.7251\n",
      "Epoch 6/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 345.7082\n",
      "Epoch 7/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 343.6469\n",
      "Epoch 8/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 342.9697\n",
      "Epoch 9/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 342.3038\n",
      "Epoch 10/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 341.7691\n",
      "Epoch 11/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 341.5000\n",
      "Epoch 12/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 340.3163\n",
      "Epoch 13/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 339.6348\n",
      "Epoch 14/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 339.0869\n",
      "Epoch 15/1000\n",
      "5504/5504 [==============================] - 406s 74ms/step - loss: 338.8088\n",
      "Epoch 16/1000\n",
      "5504/5504 [==============================] - 406s 74ms/step - loss: 338.4123\n",
      "Epoch 17/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 338.0071\n",
      "Epoch 18/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 334.6174\n",
      "Epoch 19/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 329.1232\n",
      "Epoch 20/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 327.6498\n",
      "Epoch 21/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 327.0096\n",
      "Epoch 22/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 326.2890\n",
      "Epoch 23/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 325.6621\n",
      "Epoch 24/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 324.9937\n",
      "Epoch 25/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 324.4932\n",
      "Epoch 26/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 323.9700\n",
      "Epoch 27/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 323.5786\n",
      "Epoch 28/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 323.4219\n",
      "Epoch 29/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 323.5298\n",
      "Epoch 30/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 323.1205\n",
      "Epoch 31/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 322.7717\n",
      "Epoch 32/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 322.6893\n",
      "Epoch 33/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 322.4547\n",
      "Epoch 34/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 321.9952\n",
      "Epoch 35/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 322.5864\n",
      "Epoch 36/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 322.1731\n",
      "Epoch 37/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 321.5492\n",
      "Epoch 38/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 321.8847\n",
      "Epoch 39/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 321.7418\n",
      "Epoch 40/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 323.3801\n",
      "Epoch 41/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 322.4152\n",
      "Epoch 42/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 321.4996\n",
      "Epoch 43/1000\n",
      "5504/5504 [==============================] - 406s 74ms/step - loss: 321.5622\n",
      "Epoch 44/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 321.3585\n",
      "Epoch 45/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 320.8222\n",
      "Epoch 46/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 320.9076\n",
      "Epoch 47/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 320.6670\n",
      "Epoch 48/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 320.6826\n",
      "Epoch 49/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 320.6522\n",
      "Epoch 50/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 321.1942\n",
      "Epoch 51/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 320.9820\n",
      "Epoch 52/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 321.7398\n",
      "Epoch 53/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 322.0229\n",
      "Epoch 54/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 320.7061\n",
      "Epoch 55/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 320.4423\n",
      "Epoch 56/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 321.3552\n",
      "Epoch 57/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 321.3156\n",
      "Epoch 58/1000\n",
      "5504/5504 [==============================] - 406s 74ms/step - loss: 321.1653\n",
      "Epoch 59/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 321.0739\n",
      "Epoch 60/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 322.0348\n",
      "Epoch 61/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 321.5819\n",
      "Epoch 62/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 326.0246\n",
      "Epoch 63/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 323.0046\n",
      "Epoch 64/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 322.3606\n",
      "Epoch 65/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 322.0903\n",
      "Epoch 66/1000\n",
      "5504/5504 [==============================] - 409s 74ms/step - loss: 321.7198\n",
      "Epoch 67/1000\n",
      "5504/5504 [==============================] - 409s 74ms/step - loss: 321.5436\n",
      "Epoch 68/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 321.0496\n",
      "Epoch 69/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 320.9432\n",
      "Epoch 70/1000\n",
      "5504/5504 [==============================] - 409s 74ms/step - loss: 320.6686\n",
      "Epoch 71/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 320.4587\n",
      "Epoch 72/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 320.3024\n",
      "Epoch 73/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 320.2417\n",
      "Epoch 74/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 320.1272\n",
      "Epoch 75/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 319.9579\n",
      "Epoch 76/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 319.8009\n",
      "Epoch 77/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 319.3903\n",
      "Epoch 78/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 319.7269\n",
      "Epoch 79/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 319.7115\n",
      "Epoch 80/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 320.1960\n",
      "Epoch 81/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 319.4925\n",
      "Epoch 82/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 319.2309\n",
      "Epoch 83/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 321.3545\n",
      "Epoch 84/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 321.1577\n",
      "Epoch 85/1000\n",
      "5504/5504 [==============================] - 409s 74ms/step - loss: 320.1154\n",
      "Epoch 86/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 320.1566\n",
      "Epoch 87/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 320.1367\n",
      "Epoch 88/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 319.5704\n",
      "Epoch 89/1000\n",
      "5504/5504 [==============================] - 409s 74ms/step - loss: 319.4805\n",
      "Epoch 90/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 320.3392\n",
      "Epoch 91/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 321.8825\n",
      "Epoch 92/1000\n",
      "5504/5504 [==============================] - 409s 74ms/step - loss: 321.3543\n",
      "Epoch 93/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 320.6999\n",
      "Epoch 94/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 320.2344\n",
      "Epoch 95/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 319.6769\n",
      "Epoch 96/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 319.5104\n",
      "Epoch 97/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 319.8633\n",
      "Epoch 98/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 320.5172\n",
      "Epoch 99/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 319.8048\n",
      "Epoch 100/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 319.6883\n",
      "Epoch 101/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 321.4165\n",
      "Epoch 102/1000\n",
      "5504/5504 [==============================] - 409s 74ms/step - loss: 320.4003\n",
      "Epoch 103/1000\n",
      "5504/5504 [==============================] - 409s 74ms/step - loss: 321.5599\n",
      "Epoch 104/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 322.0688\n",
      "Epoch 105/1000\n",
      "5504/5504 [==============================] - 409s 74ms/step - loss: 320.4975\n",
      "Epoch 106/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 319.7353\n",
      "Epoch 107/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 319.3974\n",
      "Epoch 108/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 319.4615\n",
      "Epoch 109/1000\n",
      "5504/5504 [==============================] - 409s 74ms/step - loss: 319.1321\n",
      "Epoch 110/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 318.9794\n",
      "Epoch 111/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 318.8359\n",
      "Epoch 112/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 318.8710\n",
      "Epoch 113/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 318.3381\n",
      "Epoch 114/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 318.6009\n",
      "Epoch 115/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 318.0620\n",
      "Epoch 116/1000\n",
      "5504/5504 [==============================] - 406s 74ms/step - loss: 319.2933\n",
      "Epoch 117/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 319.0094\n",
      "Epoch 118/1000\n",
      "5504/5504 [==============================] - 407s 74ms/step - loss: 319.6093\n",
      "Epoch 119/1000\n",
      "5504/5504 [==============================] - 408s 74ms/step - loss: 320.6866\n",
      "Epoch 120/1000\n",
      "2016/5504 [=========>....................] - ETA: 4:18 - loss: 324.4516"
     ]
    }
   ],
   "source": [
    "def mk_model(max_label_length):\n",
    "    layer = feature_input = Input(shape = (None, NFEATS))\n",
    "    for i in range(2):\n",
    "        layer_0 = Lambda(K.expand_dims)(layer)\n",
    "        layer_1 = Conv1D(8, [1,5], activation='linear', kernel_initializer=Orthogonal())(layer_0)\n",
    "        layer_2 = LeakyReLU(0.01)(layer_1)\n",
    "        layer_3 = Dropout(0.25)(layer_2)\n",
    "        layer_4 = TimeDistributed(Flatten())(layer_3)\n",
    "        layer_5 = Conv1D(512, 5, strides=2, padding = 'same', activation='linear', kernel_initializer=Orthogonal())(layer_4)\n",
    "        layer_6 = LeakyReLU(0.01)(layer_5)\n",
    "        layer = layer_7 = Dropout(0.25)(layer_6)\n",
    "    for i in range(3):\n",
    "        gating = Dense(512, activation='sigmoid')(layer)\n",
    "        layer_10 = LSTM(512, return_sequences = True, kernel_initializer=Orthogonal())(layer)\n",
    "        layer_11 = Dropout(0.25)(layer_10)\n",
    "        layer = layer_12 = Lambda(lambda l: l[0] * l[1] + (1 - l[0]) * l[2])([gating, layer_11, layer])\n",
    "    layer_15 = LSTM(NPHONES + 1, return_sequences = True, activation = 'softmax')(layer)\n",
    "    label_input = Input(shape = (max_label_length,))\n",
    "    input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "    label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "    loss_lambda = Lambda(lambda args:ctc_batch_cost(*args), output_shape=(1,), name='ctc')([label_input, layer_15, input_length, label_length])\n",
    "    model = Model([feature_input, label_input, input_length, label_length], [loss_lambda])\n",
    "    model.summary()\n",
    "    predictive = Model(feature_input, layer_15)\n",
    "    return model, predictive\n",
    "\n",
    "#override\n",
    "def mk_model(max_label_length):\n",
    "    layer = feature_input = Input(shape = (None, NFEATS))\n",
    "    for i in range(1):\n",
    "        layer_0 = Lambda(K.expand_dims)(layer)\n",
    "        layer_1 = Conv2D(5, [1,5], activation='linear', kernel_initializer=Orthogonal())(layer_0)\n",
    "        layer_2 = LeakyReLU(0.01)(layer_1)\n",
    "        layer_3 = Dropout(0.25)(layer_2)\n",
    "        layer_4 = TimeDistributed(Flatten())(layer_3)\n",
    "        layer_5 = Conv1D(256, 5, strides=2, padding = 'same', activation='linear', kernel_initializer=Orthogonal())(layer_4)\n",
    "        layer_6 = LeakyReLU(0.01)(layer_5)\n",
    "        layer = layer_7 = Dropout(0.25)(layer_6)\n",
    "    for i in range(1):\n",
    "        gating = Dense(256, activation='sigmoid')(layer)\n",
    "        layer_10 = LSTM(256, return_sequences = True, kernel_initializer=Orthogonal())(layer)\n",
    "        layer_11 = Dropout(0.25)(layer_10)\n",
    "        layer = layer_12 = Lambda(lambda l: l[0] * l[1] + (1 - l[0]) * l[2])([gating, layer_11, layer])\n",
    "    layer_15 = LSTM(NPHONES + 1, return_sequences = True, activation = 'softmax')(layer)\n",
    "    label_input = Input(shape = (max_label_length,))\n",
    "    input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "    label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "    loss_lambda = Lambda(lambda args:ctc_batch_cost(*args), output_shape=(1,), name='ctc')([label_input, layer_15, input_length, label_length])\n",
    "    model = Model([feature_input, label_input, input_length, label_length], [loss_lambda])\n",
    "    model.summary()\n",
    "    predictive = Model(feature_input, layer_15)\n",
    "    return model, predictive\n",
    "\n",
    "\n",
    "def train(model, trainX, trainy, trainXl, trainyl, epochs = 50):\n",
    "    # important: batch_size=1 bugs Tensorflow\n",
    "    optimizer = Adam(0.0003)\n",
    "    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=optimizer)\n",
    "    return model.fit([trainX, trainy, trainXl, trainyl], np.zeros(trainX.shape[0]), epochs = epochs, batch_size = 32)\n",
    "\n",
    "def validate(predictions, valid_length, groundtruth, target_length):\n",
    "    predictions = keras.backend.ctc_decode(predictions, valid_length, False, 1)\n",
    "    predictions = predictions[0][0].eval(session=keras.backend.get_session())\n",
    "    DERs = []\n",
    "    for index in range(predictions.shape[0]):\n",
    "        dist = float(editdistance.eval(\n",
    "            [x for x in predictions[index, :] if x != -1],\n",
    "            [x for x in groundtruth[index, :] if x != NPHONES]))\n",
    "        DER = dist / target_length[index]\n",
    "        DERs.append((DER, target_length[index]))\n",
    "    return DERs\n",
    "\n",
    "def try_else(exp, exp_else):\n",
    "    try:\n",
    "        return exp()\n",
    "    except:\n",
    "        return exp_else\n",
    "\n",
    "if __name__=='__main__':\n",
    "    data = X, Y\n",
    "    NPHONES = Y.max()\n",
    "    NFEATS = data[0].shape[2]\n",
    "    X_lens = np.array([try_else(\n",
    "            (lambda:np.where((x).mean(1) == (x).std(1))[0][0]),\n",
    "            X.shape[1])\n",
    "        for x in X])\n",
    "    X_lens = np.ceil(X_lens / 4.0)\n",
    "    Y_lens = np.array([np.where(x == NPHONES)[0] for x in data[1]])\n",
    "    Y_lens = np.array([x[0] if len(x) else 0 for x in Y_lens])\n",
    "    data = data[0][np.where(Y_lens)], data[1][np.where(Y_lens)], X_lens[np.where(Y_lens)].reshape(-1, 1), Y_lens[np.where(Y_lens)].reshape(-1, 1)\n",
    "    print(list(map(lambda x:x.shape, data)))\n",
    "    trn, predict = mk_model(data[1].shape[1])\n",
    "    train(trn, *data, epochs=1000) # at 300 it makes sensible predictions\n",
    "    valid_data = make_data(*load_data(VALIDDATA))\n",
    "    predictions = predict.predict(valid_data[0])\n",
    "    DERs = validate(predictions, valid_data[2], valid_data[1], valid_data[3])\n",
    "    print(\"Validation Digit Error Rate: {}\".format(sum([x[0] * x[1] for x in DERs]) / sum([x[1] for x in DERs])))\n",
    "    predict.save('cyfry.pred.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9828605914302957"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_lens >= Y_lens).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_lens == 0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
