{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick recap\n",
    "\n",
    "Last time (first benkyoukai) I have shown my toying with a layer that discovers which activation should it use to represent the data well. It was suggested that the layer may be used to discover useful features in the data. Therefore I want to apply this layer to the blood dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The layer\n",
    "\n",
    "The feature selection layer was based on an experiment I performed. Generally the layer applies different non-linear operators and chooses the best one via softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection layer\n",
    "\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.layers import InputSpec, Layer, initializers, regularizers, constraints\n",
    "\n",
    "class FeatureSelectionLayer(Layer):\n",
    "\n",
    "    _default_transforms = [\n",
    "        lambda x: x,\n",
    "        lambda x: K.pow(x, 2),\n",
    "        lambda x: K.pow(x, 3),\n",
    "#        K.exp,\n",
    "#        K.sqrt,\n",
    "        K.sin,\n",
    "        K.tanh\n",
    "    ]\n",
    "\n",
    "    def __init__(self,\n",
    "                 transforms=None,\n",
    "                 initializer='glorot_uniform',\n",
    "                 regularizer='adaptive',\n",
    "                 constraint=None,\n",
    "                 **kwargs):\n",
    "        super(FeatureSelectionLayer, self).__init__(**kwargs)\n",
    "        self.transforms = transforms if transforms is not None else self._default_transforms\n",
    "        self.initializer = initializers.get(initializer)\n",
    "        self.regularizer = regularizers.get(regularizer) if regularizer != 'adaptive' else 'adaptive'\n",
    "        self.constraint = constraints.get(constraint)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        This has to add weights to the layer via self.add_weight\n",
    "        Also, test input shape and produce self.input_spec\n",
    "        finally, self.built=True\n",
    "        \"\"\"\n",
    "        output_dim = input_shape[-1]\n",
    "        activation_dim = len(self.transforms) * input_shape[-1]\n",
    "        if self.regularizer == 'adaptive':\n",
    "            self.regularizer = regularizers.L1L2(l1=(0.01 / activation_dim))\n",
    "        self.kernel = self.add_weight(shape=(output_dim, len(self.transforms)),\n",
    "                                      initializer=self.initializer,\n",
    "                                      name='fskernel',\n",
    "                                      regularizer=self.regularizer,\n",
    "                                      constraint=self.constraint)\n",
    "        self.input_spec = InputSpec(min_ndim=2, axes={-1: output_dim})\n",
    "        self.built = True\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape\n",
    "        assert input_shape[-1]\n",
    "        return tuple(input_shape) # this layer does not change shape, it's learnable activation\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        this produces a single tensor that is an output of the calculation\n",
    "        it would be good to apply L1 loss here to actually SELECT features\n",
    "        \"\"\"\n",
    "        activations = [transform(inputs) for transform in self.transforms]\n",
    "        stacked = K.stack(activations, axis=-1)\n",
    "        output = K.sum(stacked * self.kernel, axis=-1)\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        base = super(FeatureSelectionLayer, self).get_config()\n",
    "        conf = {}\n",
    "        return dict(list(base.items()) + list(conf.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data\n",
    "\n",
    "I will use some random features from quite large dataset of blood data samples. Unfortunately I have no idea what to expect from this dataset, therefore some basic analysis of the dataset contents should be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (1,2,4,15,16,17,19,20,21,22,23,215,275,291,318,319,321) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"bloody_data.csv\")  # TBD: Dataset to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lots of columns, lots of NaNs, let's choose only relevant columns for some case\n",
    "\n",
    "numerical = ['PCB156 (ng/g)',\n",
    " 'b-cryptoxanthin(ug/dL)',\n",
    " 'LBDBANO_0',\n",
    " '1,2,3,7,8,9-Hexachlorodibenzofuran (hxcdf) Lipid Adjusted',\n",
    " 'Molybdenum, urine (ng/mL)',\n",
    " 'Segmented neutrophils number',\n",
    " '2,3,7,8-Tetrachlorodienzo-p-dioxin (tcdd) Lipid Adjusted',\n",
    " 'PCB118 Lipid Adjusted',\n",
    " '1,2,3,4,7,8-Hexachlorodibenzo-p-dioxin (hxcdd)(fg/g)',\n",
    " 'White blood cell count (1000 cells/uL)',\n",
    " 'Thyroglobulin',\n",
    " 'Eosinophils number',\n",
    " \"3,3',4,4',5,5'-hexachlorobiphenyl (hxcb) Lipid Adjusted\",\n",
    " 'Anti-Mullerian Hormone (ng/ml)',\n",
    " 'PCB149 (ng/g)',\n",
    " 'Coffee/tea fast time (hours)\\n',\n",
    " 'PCB157 Lipid Adjusted',\n",
    " 'CD4 counts (cells/mm3)',\n",
    " '1-pyrene (ng/L)',\n",
    " 'Gamma-hexachloro-cyclohexane (ng/g)',\n",
    " 'MCHC (g/dL)',\n",
    " 'Phthalate Subsample 2 year Mec Weight\\n',\n",
    " 'PCB189 Lipid Adjusted',\n",
    " 'Thyroid hormones Subsample 4 yr Mec Wgt ',\n",
    " 'TELOSTD',\n",
    " 'Iron, frozen (umol/L)',\n",
    " 'Vitamin D (nmol/L)',\n",
    " 'Transferrin saturation (%)',\n",
    " 'Mono-isononyl phthalate (ng/mL)',\n",
    " 'LBXRDW_0',\n",
    " 'PCB177 Lipid Adjusted',\n",
    " 'alpha-tocopherol(ug/dL)',\n",
    " 'Daidzein (ng/mL)',\n",
    " 'Metolachlor mercapturate (ug/L) result',\n",
    " 'Follicle stimulating hormone (IU/L)',\n",
    " 'PCB172 (ng/g)',\n",
    " 'The time (in minutes) between when the examinee last ate or drank anything other than water and the time of the venipuncture.',\n",
    " 'Osmolality: SI (mmol/Kg)',\n",
    " 'Blood Ethylbenzene Result (ng/mL)',\n",
    " 'Inhibin B (pg/ml)',\n",
    " 'Aldrin',\n",
    " 'Plasma glucose (mg/dL)',\n",
    " 'Uric acid (umol/L)',\n",
    " '1,2,3,7,8,9-Hexachlorodibenzo-p-dioxin (hxcdd) Lipid Adjusted',\n",
    " 'Gum, mints, cough fast time (minutes)\\n',\n",
    " '1,2,3,4,6,7,8-Heptachlorodibenzofuran (hpcdf) (fg/g)',\n",
    " 'Total Calcium (mg/dL)',\n",
    " 'SSCMVOD_0',\n",
    " '2-napthol',\n",
    " 'LBXTC_0',\n",
    " 'Aspartate aminotransferase (U/L)',\n",
    " 'b-cryptoxanthin(umol/L)',\n",
    " 'Enterodiol (ng/mL)',\n",
    " 'Creatinine (umol/L)\\n',\n",
    " 'PCB177 (ng/g)',\n",
    " 'Free T4',\n",
    " 'cis-beta-carotene(umol/L)',\n",
    " 'alpha-tocopherol(umol/L)',\n",
    " 'Mono-(3-carboxypropyl) phthalate',\n",
    " '1,2,3,4,6,7,8,9-Octachlorodibenzofuran (ocdf) (fg/g)',\n",
    " 'Thyroperoxidase antibody',\n",
    " 'Oxychlordane (ng/g)',\n",
    " '3,5,6-trichloropyridinol (ug/L) result',\n",
    " 'PCB153 (ng/g)',\n",
    " 'PCB167 (ng/g)',\n",
    " \"p,p'-DDT (ng/g)\",\n",
    " 'PCB87 (ng/g)',\n",
    " 'Mono-(2-ethyl-5-hydroxyhexyl) phthalate',\n",
    " '2,3,7,8,-Tetrachlorodibenzofuran (tcdf)  (fg/g)',\n",
    " 'Follicle stimulating hormone (IU/L)\\n',\n",
    " 'How old {were you/was SP} when {you were/he was} first told that {you/he} had prostate cancer?',\n",
    " 'Total protein (g/dL)',\n",
    " 'Toxoplasma (IgM)',\n",
    " 'Luteinizing hormone (IU/L)\\n',\n",
    " 'VOC subsample 2 yr MEC Weight',\n",
    " 'Beryllium, urine (ng/mL)',\n",
    " 'The time (in hours) between when the examinee last ate or drank anything other than water and the time of the venipuncture.',\n",
    " 'Glucose, serum (mg/dL)',\n",
    " 'PCB151 Lipid Adjusted',\n",
    " '2,3,4,6,7,8,-Hexchlorodibenzofuran (hxcdf) Lipid Adjusted',\n",
    " '1,2,3,4,6,7,8-Heptachlororodibenzo-p-dioxin (hpcdd) (fg/g)',\n",
    " 'alpha-carotene(umol/L)',\n",
    " 'Beta',\n",
    " 'Sex Hormone Binding Globulin (nmol/L)',\n",
    " '1,2,3,4,7,8-Hexachlorodibenzo-p-dioxin (hxcdd) Lipid Adjusted',\n",
    " 'a-carotene(umol/L)',\n",
    " '1,2,3,6,7,8-Hexachlorodibenzofuran (hxcdf) (fg/g)',\n",
    " 'Uranium, urine (ng/mL)',\n",
    " 'PCB87 Lipid Adjusted',\n",
    " '1,2,3,4,7,8-Hexachlorodibenzofuran (hcxdf) Lipid Adjusted',\n",
    " 'Alcohol fast time (minutes)\\n',\n",
    " 'Mono-n-butyl phthalate (ng/mL)',\n",
    " 'trans-beta-carotene(umol/L)',\n",
    " 'Gamma-hexachlorocyclohexane Lipid Adjusted',\n",
    " 'Platelet count (1000 cells/uL)',\n",
    " 'Thyroxine (T4) (ug/dL)',\n",
    " '1-phenanthrene (ng/L)',\n",
    " 'beta-cryptoxanthin(umol/L)',\n",
    " 'PCB99 Lipid Adjusted',\n",
    " 'PCB189 (ng/g)',\n",
    " 'Specimen Index ratio',\n",
    " 'Total T3',\n",
    " 'Blood Toluene Result (ng/mL)',\n",
    " '2,3,7,8,-Tetrachlorodibenzofuran (tcdf) Lipid Adjusted',\n",
    " 'Gum, mints cough drops fast time (hours)\\n',\n",
    " 'Equol (ng/mL)',\n",
    " 'PCB138 Lipid Adjusted',\n",
    " 'Blood m-/p-Xylene Result (ng/mL)',\n",
    " 'LBXMPSI_0',\n",
    " 'PEL_Impression',\n",
    " 'Heavy Metal Subsample 4 Year Mec Weight\\n',\n",
    " '1,2,3,4,7,8-Hexachlorodibenzofuran (hcxdf) (fg/g)',\n",
    " '3-phenanthrene (ng/L)',\n",
    " 'LBXBAPCT_0',\n",
    " 'PCB52 (ng/g)',\n",
    " 'LBXLYPCT_0',\n",
    " 'beta-cryptoxanthin(ug/dL)',\n",
    " 'PCB52 Lipid Adjusted',\n",
    " 'PCB199 Lipid Adjusted',\n",
    " 'g-tocopherol(umol/L)',\n",
    " 'Total cholesterol (mmol/L)\\n',\n",
    " 'Endrin Lipid Adjusted',\n",
    " 'Testostosterone (ng/mL)',\n",
    " 'Alcohol fast time (hours)\\n',\n",
    " 'Toxoplasma (IgG)',\n",
    " 'Glucose, serum (mmol/L)',\n",
    " 'Protein, total (g/dL)',\n",
    " 'Mercury, inorganic (ug/L)',\n",
    " 'Thallium, urine (ng/mL)',\n",
    " 'Mono-n-octyl phthalate (ng/mL)',\n",
    " '1,2,3,7,8-Pentachlorodibenzo-p-dioxin (pncdd) Lipid Adjusted',\n",
    " 'WTSAF2YR_0',\n",
    " '1,2,3,4,6,7,8,9-Octachlorodibenzofuran (ocdf) Lipid Adjusted',\n",
    " 'Sodium (mmol/L)',\n",
    " 'Oxypyrimidine (ug/L) result',\n",
    " 'Thyroid stimulating hormone',\n",
    " 'PCB187 (ng/g)',\n",
    " 'N-Telopeptides (nmol BCE)',\n",
    " '2,3,7,8-Tetrachlorodienzo-p-dioxin (tcdd) (fg/g)',\n",
    " 'LBDHDL_0',\n",
    " '1,2,3,4,6,7,8,9-Octachlorodibenzo-p-dioxin (ocdd) Lipid Adjusted',\n",
    " 'cis-b-carotene(umol/L)',\n",
    " 'SS3ADLG',\n",
    " '3-fluorene (ng/L)',\n",
    " 'K/L Ratio',\n",
    " 'alpha-carotene(ug/dL)',\n",
    " 'Thyroid hormones Subsample 2 yr Mec Wgt ',\n",
    " 'LBDLYMNO_0',\n",
    " 'Floor result in micrograms per square feet by graphite furnace atomic absorption spectrophotometric method.',\n",
    " '2-isopropoxyphenol (ug/L) result',\n",
    " 'PCB105 Lipid Adjusted',\n",
    " 'Carbofuranphenol (ug/L) result',\n",
    " 'IF_Result',\n",
    " \"o,p'-DDT Lipid Adjusted\",\n",
    " 'PCB153 Lipid Adjusted',\n",
    " 'LBDEONO_0',\n",
    " 'Albumin/Gamma',\n",
    " 'HDL-cholesterol (mg/dL)\\n',\n",
    " 'Globulin (g/L)\\n',\n",
    " 'Tungsten, urine (ng/mL)',\n",
    " 'Blood Trichloroethene Result (ng/mL)',\n",
    " 'LBXEOPCT_0',\n",
    " 'Iron, frozen (ug/dL)',\n",
    " 'Continuous variable describing urinary concentration of nitrate (NO3) (ug/L). Limit of detection: 700 ug/L.',\n",
    " 'PCB195 (ng/g)',\n",
    " 'Phosphorus (mg/dL)',\n",
    " 'DEET (ug/L)',\n",
    " 'LBXRBCSI_0',\n",
    " \"3,3',4,4',5-Pentachlorobiphenyl (pncb) (fg/g)\",\n",
    " 'Iodine, urine (ng/mL)',\n",
    " 'Blood Bromoform Result (pg/mL)',\n",
    " 'Blood Chloroform Result (pg/mL)',\n",
    " 'Bicarbonate: SI (mmol/L)',\n",
    " 'Lead, urine (ng/mL)',\n",
    " 'LBXMC_0',\n",
    " 'PCB138 (ng/g)',\n",
    " '1,2,3,7,8,9-Hexachlorodibenzofuran (hxcdf) (fg/g)',\n",
    " '3-chloro-7-hydroxy-4-methyl-2H-chromen-2-one/ol',\n",
    " 'LBDNENO_0',\n",
    " 'LBDTCSI_0',\n",
    " '1,2,3,4,7,8,9-Heptachlorodibenzofuran  (Hpcdf) (fg/g)',\n",
    " 'Mono-benzyl phthalate (ng/mL)',\n",
    " 'Floor result in micrograms per square feet by flame atomic absorption spectrophotometric method.',\n",
    " '2,3,4,7,8-Pentachlorodibenzofuran (pncdf) Lipid Adjusted',\n",
    " 'Thyroxine (T4) (nmol/L) ',\n",
    " 'Chloride: SI (mmol/L)',\n",
    " 'Blood Carbon Tetrachloride Result (ng/mL)',\n",
    " 'Blood 1,4-Dichlorobenzene Result (ng/mL)',\n",
    " 'PCB74 (ng/g)',\n",
    " 'Trans-nonachlor Lipid Adjusted',\n",
    " 'Mono-(2-ethyl)-hexyl phthalate (ng/mL)',\n",
    " 'Rubella',\n",
    " 'C-peptide: pmol/mL',\n",
    " 'Paranitrophenol (ug/L) result',\n",
    " 'PCB110 (ng/g)',\n",
    " 'Sodium: SI (mmol/L)',\n",
    " 'CD8 counts (cells/mm3)',\n",
    " '1,2,3,4,6,7,8,9-Octachlorodibenzo-p-dioxin (ocdd) (fg/g)',\n",
    " 'PCB183 Lipid Adjusted',\n",
    " 'Mirex Lipid Adjusted',\n",
    " 'PCB206 (ng/g)',\n",
    " 'cis-3-(2,2-dichlorovinyl)-2,2-dimethylcyclopropane carboxylic acid',\n",
    " 'Alanine aminotransferase (U/L)',\n",
    " 'Platinum, urine (ng/mL)',\n",
    " 'PCB149 Lipid Adjusted',\n",
    " '2,3,4,7,8-Pentachlorodibenzofuran (pncdf) (fg/g)',\n",
    " 'Dietary supplements fast time (minutes)\\n',\n",
    " 'PCB178 (ng/g)',\n",
    " 'Erythrocyte protoporphyrin (umol/L RBC)',\n",
    " 'WTUIO2YR_0',\n",
    " 'Bicarbonate (mmol/L)',\n",
    " 'PCB183 (ng/g)',\n",
    " 'Glucose, serum (mmol/L)\\n',\n",
    " 'Luteinizing hormone (IU/L)',\n",
    " 'PCB206 Lipid Adjusted',\n",
    " 'Blood Tetrachloroethene Result (ng/mL)',\n",
    " 'Acetochlor mercapturate\\n',\n",
    " 'Potassium (mmol/L)',\n",
    " 'Lymphocyte number',\n",
    " 'VOC subsample 4 yr MEC Weight',\n",
    " 'PCB99 (ng/g)',\n",
    " 'Heptachlor Epoxide Lipid Adjusted',\n",
    " \"o,p'-DDT (ng/g)\",\n",
    " 'TELOMEAN',\n",
    " 'Lambda FLC',\n",
    " \"p,p'-DDE Lipid Adjusted\",\n",
    " 'a-carotene(ug/dL)',\n",
    " '4Yr AM(3-11)  &amp; fasting (12+) weights',\n",
    " 'PCB66 (ng/g)',\n",
    " 'PCB180 Lipid Adjusted',\n",
    " 'Endrin',\n",
    " 'Water Methyl tert. butyl ether (MTBE) Result (ng/mL)',\n",
    " 'trans-beta-carotene(ug/dL)',\n",
    " 'LBXNEPCT_0',\n",
    " 'Mono-ethyl phthalate (ng/mL)',\n",
    " 'CMV optical density from ELISA',\n",
    " '2-fluorene (ng/L)',\n",
    " '1,2,3,4,6,7,8-Heptachlororodibenzo-p-dioxin (hpcdd) Lipid Adjusted',\n",
    " 'PCB151 (ng/g)',\n",
    " 'HDL-cholesterol (mmol/L) ',\n",
    " 'gamma-tocopherol(ug/dL)',\n",
    " 'Red blood cell count (million cells/uL)',\n",
    " 'Dimethylthiophosphate (ug/L) result',\n",
    " 'Alkaline phosphotase (U/L)',\n",
    " '2,3,4,6,7,8,-Hexchlorodibenzofuran (hxcdf) (fg/g)',\n",
    " 'PCB170 Lipid Adjusted',\n",
    " '1,2,3,4,7,8,9-Heptachlorodibenzofuran  (Hpcdf) Lipid Adjusted',\n",
    " 'Lymphocyte number (1000 cells/uL)',\n",
    " 'PCB128 Lipid Adjusted',\n",
    " 'The number of days between the collection of the first and second exam',\n",
    " 'Erythrocyte protoporphyrin (ug/dL RBC)',\n",
    " '1,2,3,4,6,7,8-Heptachlorodibenzofuran (hpcdf) Lipid Adjusted',\n",
    " 'PCB195 Lipid Adjusted',\n",
    " 'ALT: SI (U/L)',\n",
    " 'Glucose (mg/dL)',\n",
    " 'PCB194 Lipid Adjusted',\n",
    " 'Phosphorus  (mmol/L)',\n",
    " 'PCB172 Lipid Adjusted',\n",
    " '1,2,3,7,8-Pentachlorodibenzo-p-dioxin (pncdd) (fg/g)',\n",
    " 'Creatinine (umol/L)',\n",
    " 'Continuous variable describing urinary concentration of  Thiocyanate (SCN) (ug/L). Limit of detection: 20 ug/L.',\n",
    " 'Blood Dibromochloromethane Result (pg/mL)',\n",
    " 'LBXPLTSI_0',\n",
    " 'Water Bromoform Result (ng/mL)',\n",
    " 'PCB178 Lipid Adjusted',\n",
    " 'Molecular Type 1',\n",
    " 'Heptachlor Epoxide (ng/g)',\n",
    " 'Rubella antibody (international units)\\n',\n",
    " 'Globulin (g/dL)',\n",
    " 'N-telopeptides (nmol BCE)',\n",
    " \"3,4,4',5-Tetrachlorobiphenyl (tcb) Lipid Adjusted\",\n",
    " 'cis-beta-carotene(ug/dL)',\n",
    " 'Blood Benzene Result (ng/mL)',\n",
    " '1,2,3,6,7,8-Hexachlorodibenzo-p-dioxin (hxcdd)(fg/g)',\n",
    " 'Phthalate Subsample 4 year Mec',\n",
    " 'Dioxins Subsample 2 Year Mec Weight',\n",
    " 'Iodine Subsample 2 year Mec Weight',\n",
    " 'Surplus sera cystatin 99-02 weights',\n",
    " 'Antacids, laxatives fast time (hours)\\n',\n",
    " 'Mean cell hemoglobin concentration (g/dL)',\n",
    " 'PCB170 (ng/g)',\n",
    " 'PCB101 (ng/g)',\n",
    " '1,2,3,6,7,8-Hexachlorodibenzofuran (hxcdf) Lipid Adjusted',\n",
    " 'Window result in micrograms per square feet by flame atomic absorption spectrophotometric method.',\n",
    " 'Blood o-Xylene Result (ng/mL)',\n",
    " 'estradiol (pg/mL)',\n",
    " \"3,3',4,4',5,5'-hexachlorobiphenyl (hxcb) (fg/g)\",\n",
    " 'PCB157 (ng/g)',\n",
    " 'Alpha1',\n",
    " 'Cytomegalovirus viral load Copies/mL',\n",
    " 'LBXWBCSI_0',\n",
    " 'Water Bromodichloromethane Result (ng/mL)',\n",
    " 'Oxychlordane Lipid Adjusted',\n",
    " 'Basophils number',\n",
    " 'PCB156 Lipid Adjusted',\n",
    " 'Urinary Perchlorate (ug/L)',\n",
    " 'Mono-isobutyl pthalate\\n',\n",
    " 'Water Dibromochloromethane Result (ng/mL)',\n",
    " 'Total_Protein',\n",
    " '4-fluoro-3-phenoxybenzoic (ug/L) acid',\n",
    " 'PCB118 (ng/g)',\n",
    " 'PCB128 (ng/g)',\n",
    " 'Total calcium (mg/dL)',\n",
    " 'Phosphorus  (mg/dL)',\n",
    " 'HDL-cholesterol (mmol/L)\\n',\n",
    " 'Gamma',\n",
    " 'PCB146 (ng/g)',\n",
    " 'Genistein (ng/mL)',\n",
    " 'Diethyldithiophosphate (ug/L) result',\n",
    " 'PCB167 Lipid Adjusted',\n",
    " 'Blood Bromodichloromethane Result (pg/mL)',\n",
    " '1-napthol',\n",
    " 'WTSAF4YR_0',\n",
    " '2-phenanthrene (ng/L)',\n",
    " 'Toxoplasma (Avidity) IgG',\n",
    " 'Dieldrin (ng/g)',\n",
    " 'Pesticides Subsample 4 year Mec Weight\\n',\n",
    " 'Dioxins Subsample 4 Year Mec Weight',\n",
    " 'LBXHCT_0',\n",
    " 'Enterolactone (ng/mL)',\n",
    " 'PCB105 (ng/g)',\n",
    " 'PCB194 (ng/g)',\n",
    " 'Chloride (mmol/L)',\n",
    " 'Antimony, urine (ng/mL)',\n",
    " 'Blood Methyl t-Butyl Ether (MTBE) Result (pg/mL)',\n",
    " 'PCB187 Lipid Adjusted',\n",
    " 'Lactate dehydrogenase (U/L)',\n",
    " 'Triglycerides (mg/dL)',\n",
    " 'Eosinophils number (1000 cells/uL)',\n",
    " 'Kappa FLC',\n",
    " 'Dimethyldithiophosphate (ug/L) result',\n",
    " 'Mono-n-methyl phthalate',\n",
    " 'Serum ferritin (ng/ml)\\n',\n",
    " 'PCB196 (ng/g)',\n",
    " 'Cadmium, urine (ng/mL)',\n",
    " 'Thyroglobulin antibody',\n",
    " 'trans-b-carotene(umol/L)',\n",
    " 'Mercury, urine (ng/mL)\\n',\n",
    " 'Platelet count (%) SI',\n",
    " 'GGT: SI (U/L)',\n",
    " 'Coffee/tea fast time (minutes)\\n',\n",
    " 'LBDMONO_0',\n",
    " 'Dimethylphosphate (ug/L) result',\n",
    " 'Mirex (ng/g)',\n",
    " 'LDH (U/L)',\n",
    " 'Total T4',\n",
    " 'Hexachlorobenzene (ng/g)',\n",
    " 'Total protein (g/L)\\n',\n",
    " 'Diethylthiophosphate (ug/L) result',\n",
    " 'Rubella units',\n",
    " 'Heavy Metal Subsample 2 Year Mec Weight',\n",
    " 'Protoporphyrin(µmol/L RBC)',\n",
    " 'Dietary supplements fast time (hours)\\n',\n",
    " 'PCB101 Lipid Adjusted',\n",
    " 'PCB66 Lipid Adjusted',\n",
    " 'AST: SI (U/L)',\n",
    " '1,2,3,7,8-Pentachlorodibenzofuran (pncdf) Lipid Adjusted',\n",
    " '3-phenoxybenzoic (ug/L) acid result',\n",
    " 'Alkaline phosphatase (U/L)',\n",
    " 'LBXMCHSI_0',\n",
    " 'LBXHGB_0',\n",
    " '2,4-D (ug/L) result',\n",
    " 'Thyroid stimulating hormone (TSH) (IU/L)',\n",
    " 'Antacids, laxatives fast time (minutes)\\n',\n",
    " 'Beta-hexachlorocyclohexane Lipid Adjusted',\n",
    " \"p,p'-DDE (ng/g)\",\n",
    " 'Barium, urine (ng/mL)',\n",
    " 'LBDHDLSI_0',\n",
    " 'trans-3-(2,2-dichlorovinyl)-2,2-dimethylcyclopropane carboxylic acid',\n",
    " '2-(diethylamino)-6-methylpyrimidin-4-ol/one',\n",
    " 'Potassium: SI (mmol/L)',\n",
    " 'Atrazine mercapturate (ug/L) result',\n",
    " \"3,4,4',5-Tetrachlorobiphenyl (tcb) (fg/g)\",\n",
    " 'PCB110 Lipid Adjusted',\n",
    " 'Free T3',\n",
    " 'Phosphorus (mmol/L)',\n",
    " \"3,3',4,4',5-Pentachlorobiphenyl (pncb) Lipid Adjusted\",\n",
    " '1,2,3,6,7,8-Hexachlorodibenzo-p-dioxin (hxcdd) Lipid Adjusted',\n",
    " 'Diethylphosphate (ug/L)result',\n",
    " 'Vitamin E (ug/dL)',\n",
    " 'gamma-tocopherol(umol/L)',\n",
    " 'Total iron binding capacity (umol/L)',\n",
    " 'White blood cell count:  SI',\n",
    " 'Aldrin Lipid Adjusted',\n",
    " 'Alpha2',\n",
    " '1,2,3,7,8-Pentachlorodibenzofuran (pncdf) (fg/g)',\n",
    " 'Dieldrin Lipid Adjusted',\n",
    " 'Hexachlorobenzene Lipid Adjusted',\n",
    " 'Glucose, plasma (mg/dL)',\n",
    " 'Mercury, inorganic (umol/L)',\n",
    " 'Mono-cyclohexyl phthalate (ng/mL)',\n",
    " 'cis-b-carotene(ug/dL)',\n",
    " 'Trans-nonachlor (ng/g)',\n",
    " 'Total cholesterol (mmol/L) ',\n",
    " 'PCB199 (ng/g)',\n",
    " 'PCB74 Lipid Adjusted',\n",
    " 'Monocyte number',\n",
    " 'Mono-(2-ethyl-5-oxohexyl) phthalate',\n",
    " 'Water Chloroform Result',\n",
    " \"p,p'-DDT Lipid Adjusted\",\n",
    " 'Triglycerides (mg/dL) ',\n",
    " 'Albumin',\n",
    " 'PCB146 Lipid Adjusted',\n",
    " 'LBXMOPCT_0',\n",
    " 'Vitamin E (umol/L)',\n",
    " 'Size of peak if available',\n",
    " 'Cystatin C (mg/L)',\n",
    " 'cis-3-(2,2-dibromovinyl)-2,2-dimethylcyclopropane carboxylic acid',\n",
    " 'LBXMCVSI_0',\n",
    " 'Total protein (g/L)',\n",
    " 'Gamma glutamyltransferase',\n",
    " 'Red cell count SI',\n",
    " 'PCB180 (ng/g)',\n",
    " 'TTG IGA value',\n",
    " 'Protoporphyrin(ug/dL RBC)',\n",
    " 'Cesium, urine (ng/mL)',\n",
    " '1,2,3,7,8,9-Hexachlorodibenzo-p-dioxin (hxcdd) (fg/g)',\n",
    " 'Total iron binding capacity (ug/dL)',\n",
    " 'g-tocopherol(ug/dL)',\n",
    " 'Cobalt, urine (ng/mL)',\n",
    " 'HDL-cholesterol (mg/dL) ',\n",
    " 'PCB196 Lipid Adjusted',\n",
    " 'Monocyte number (1000 cells/uL)',\n",
    " 'Blood Styrene Result (ng/mL)',\n",
    " 'Beta-hexachloro-cyclohexane (ng/g)',\n",
    " 'Segmented neutrophils num (1000 cell/uL)',\n",
    " 'trans-b-carotene(ug/dL)',\n",
    " 'Pesticides Subsample 2 year Mec Weight\\n',\n",
    " 'o-Desmethylangolensin (O-DMA) (ng/mL)',\n",
    " 'Serum transferrin receptor (mg/l)\\n',\n",
    " 'Uric acid (umol/L)\\n',\n",
    " '2Yr AM(3-11)  &amp; fasting (12+) weights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also trim to examples we want to actually predict\n",
    "\n",
    "targets = ['Herpes II', 'Herpes I']\n",
    "subset = df[df[targets[0]].notna() & df[targets[1]].notna()]\n",
    "subset = subset.reset_index(False).drop(\"index\", 1)\n",
    "variables = subset[numerical]\n",
    "targets = subset[targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         (True, True)\n",
       "1         (True, True)\n",
       "2        (True, False)\n",
       "3       (False, False)\n",
       "4        (True, False)\n",
       "5        (True, False)\n",
       "6        (True, False)\n",
       "7        (True, False)\n",
       "8        (True, False)\n",
       "9       (False, False)\n",
       "10       (True, False)\n",
       "11       (True, False)\n",
       "12       (True, False)\n",
       "13       (False, True)\n",
       "14       (False, True)\n",
       "15       (False, True)\n",
       "16        (True, True)\n",
       "17      (False, False)\n",
       "18       (True, False)\n",
       "19        (True, True)\n",
       "20       (True, False)\n",
       "21       (True, False)\n",
       "22       (True, False)\n",
       "23       (True, False)\n",
       "24        (True, True)\n",
       "25        (True, True)\n",
       "26       (True, False)\n",
       "27        (True, True)\n",
       "28      (False, False)\n",
       "29       (True, False)\n",
       "             ...      \n",
       "3003    (False, False)\n",
       "3004     (True, False)\n",
       "3005     (True, False)\n",
       "3006     (True, False)\n",
       "3007      (True, True)\n",
       "3008      (True, True)\n",
       "3009    (False, False)\n",
       "3010    (False, False)\n",
       "3011     (True, False)\n",
       "3012     (True, False)\n",
       "3013     (True, False)\n",
       "3014    (False, False)\n",
       "3015     (True, False)\n",
       "3016     (True, False)\n",
       "3017     (True, False)\n",
       "3018     (True, False)\n",
       "3019      (True, True)\n",
       "3020     (True, False)\n",
       "3021     (True, False)\n",
       "3022     (True, False)\n",
       "3023     (False, True)\n",
       "3024      (True, True)\n",
       "3025      (True, True)\n",
       "3026     (True, False)\n",
       "3027     (False, True)\n",
       "3028     (True, False)\n",
       "3029      (True, True)\n",
       "3030     (True, False)\n",
       "3031    (False, False)\n",
       "3032      (True, True)\n",
       "Length: 3033, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View whether targets are reasonable\n",
    "\n",
    "targets = targets.apply(lambda row: str((bool(row[0] - 1), bool(row[1] - 1))), axis=1)\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['LBDHDL_0', 'Molecular Type 1', 'LBXTC_0',\n",
       "       'Total cholesterol (mmol/L)\\n', 'Toxoplasma (IgG)',\n",
       "       'HDL-cholesterol (mmol/L)\\n', 'HDL-cholesterol (mg/dL)\\n', 'LBDTCSI_0',\n",
       "       'LBDHDLSI_0', 'Vitamin E (umol/L)', 'Vitamin E (ug/dL)',\n",
       "       'Rubella units', 'LBXPLTSI_0',\n",
       "       'The time (in minutes) between when the examinee last ate or drank anything other than water and the time of the venipuncture.',\n",
       "       'Sodium: SI (mmol/L)', 'LBXMPSI_0', 'Platelet count (%) SI',\n",
       "       'GGT: SI (U/L)', 'LBXRDW_0', 'LDH (U/L)', 'Glucose, serum (mg/dL)',\n",
       "       'Osmolality: SI (mmol/Kg)', 'LBXHCT_0', 'Total Calcium (mg/dL)',\n",
       "       'White blood cell count:  SI',\n",
       "       'The time (in hours) between when the examinee last ate or drank anything other than water and the time of the venipuncture.',\n",
       "       'AST: SI (U/L)', 'Alkaline phosphatase (U/L)', 'LBXMCHSI_0',\n",
       "       'Creatinine (umol/L)\\n', 'LBXHGB_0', 'Uric acid (umol/L)\\n',\n",
       "       'ALT: SI (U/L)', 'LBXWBCSI_0', 'LBXMCVSI_0',\n",
       "       'Glucose, serum (mmol/L)\\n',\n",
       "       'Mean cell hemoglobin concentration (g/dL)', 'LBXMC_0',\n",
       "       'Bicarbonate: SI (mmol/L)', 'LBXRBCSI_0', 'Red cell count SI',\n",
       "       'Potassium: SI (mmol/L)', 'trans-b-carotene(umol/L)',\n",
       "       'Phosphorus  (mmol/L)', 'trans-b-carotene(ug/dL)',\n",
       "       'Phosphorus  (mg/dL)', 'a-carotene(umol/L)', 'cis-b-carotene(umol/L)',\n",
       "       'a-carotene(ug/dL)', 'Triglycerides (mg/dL) ', 'cis-b-carotene(ug/dL)',\n",
       "       'Total protein (g/L)\\n', 'Globulin (g/L)\\n', 'Chloride: SI (mmol/L)',\n",
       "       'Globulin (g/dL)', 'Total protein (g/dL)', 'Protoporphyrin(ug/dL RBC)',\n",
       "       'Protoporphyrin(µmol/L RBC)', 'Iron, frozen (ug/dL)',\n",
       "       'Iron, frozen (umol/L)', 'CMV optical density from ELISA', 'SSCMVOD_0',\n",
       "       'Monocyte number', 'LBXEOPCT_0', 'LBDNENO_0', 'LBXMOPCT_0',\n",
       "       'LBDLYMNO_0', 'LBXLYPCT_0', 'LBXBAPCT_0', 'Basophils number',\n",
       "       'Eosinophils number', 'Segmented neutrophils number', 'LBDMONO_0',\n",
       "       'LBDBANO_0', 'Lymphocyte number'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick some reasonable number of features\n",
    "\n",
    "series = variables.notna().mean()\n",
    "series = series.sort_values(ascending=False)\n",
    "series = series[:75].index\n",
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = variables[series].drop(\"Molecular Type 1\", axis=1) # Drop bad column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterNAs = frame.notna().apply(all, axis=1)\n",
    "frame = frame[filterNAs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LBDHDL_0</th>\n",
       "      <th>LBXTC_0</th>\n",
       "      <th>Total cholesterol (mmol/L)</th>\n",
       "      <th>Toxoplasma (IgG)</th>\n",
       "      <th>HDL-cholesterol (mmol/L)</th>\n",
       "      <th>HDL-cholesterol (mg/dL)</th>\n",
       "      <th>LBDTCSI_0</th>\n",
       "      <th>LBDHDLSI_0</th>\n",
       "      <th>Vitamin E (umol/L)</th>\n",
       "      <th>Vitamin E (ug/dL)</th>\n",
       "      <th>...</th>\n",
       "      <th>LBXMOPCT_0</th>\n",
       "      <th>LBDLYMNO_0</th>\n",
       "      <th>LBXLYPCT_0</th>\n",
       "      <th>LBXBAPCT_0</th>\n",
       "      <th>Basophils number</th>\n",
       "      <th>Eosinophils number</th>\n",
       "      <th>Segmented neutrophils number</th>\n",
       "      <th>LBDMONO_0</th>\n",
       "      <th>LBDBANO_0</th>\n",
       "      <th>Lymphocyte number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.120806</td>\n",
       "      <td>1.297114</td>\n",
       "      <td>1.299412</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>-1.108549</td>\n",
       "      <td>-1.120806</td>\n",
       "      <td>1.299412</td>\n",
       "      <td>-1.108549</td>\n",
       "      <td>1.492148</td>\n",
       "      <td>1.492187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.743010</td>\n",
       "      <td>0.247271</td>\n",
       "      <td>-0.815872</td>\n",
       "      <td>0.650790</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>0.612234</td>\n",
       "      <td>0.949062</td>\n",
       "      <td>1.867854</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>0.247271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.576499</td>\n",
       "      <td>-0.598006</td>\n",
       "      <td>-0.602122</td>\n",
       "      <td>-0.273949</td>\n",
       "      <td>-0.582225</td>\n",
       "      <td>-0.576499</td>\n",
       "      <td>-0.602122</td>\n",
       "      <td>-0.582225</td>\n",
       "      <td>-0.519125</td>\n",
       "      <td>-0.519151</td>\n",
       "      <td>...</td>\n",
       "      <td>1.021949</td>\n",
       "      <td>-0.069312</td>\n",
       "      <td>1.515540</td>\n",
       "      <td>0.419546</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>0.612234</td>\n",
       "      <td>-1.322379</td>\n",
       "      <td>-0.344523</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.069312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.984729</td>\n",
       "      <td>0.001716</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>-0.976968</td>\n",
       "      <td>-0.984729</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>-0.976968</td>\n",
       "      <td>-0.704827</td>\n",
       "      <td>-0.704788</td>\n",
       "      <td>...</td>\n",
       "      <td>0.882480</td>\n",
       "      <td>0.405562</td>\n",
       "      <td>-0.445237</td>\n",
       "      <td>-0.042941</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>1.180220</td>\n",
       "      <td>0.420820</td>\n",
       "      <td>1.867854</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>0.405562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.825421</td>\n",
       "      <td>0.313571</td>\n",
       "      <td>0.316180</td>\n",
       "      <td>-0.294461</td>\n",
       "      <td>2.812560</td>\n",
       "      <td>2.825421</td>\n",
       "      <td>0.316180</td>\n",
       "      <td>2.812560</td>\n",
       "      <td>-0.401372</td>\n",
       "      <td>-0.401402</td>\n",
       "      <td>...</td>\n",
       "      <td>2.416645</td>\n",
       "      <td>-0.227604</td>\n",
       "      <td>-0.062646</td>\n",
       "      <td>-0.042941</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-1.091723</td>\n",
       "      <td>-0.371543</td>\n",
       "      <td>1.867854</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.227604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.529036</td>\n",
       "      <td>-0.957839</td>\n",
       "      <td>-0.954601</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>-1.529607</td>\n",
       "      <td>-1.529036</td>\n",
       "      <td>-0.954601</td>\n",
       "      <td>-1.529607</td>\n",
       "      <td>-0.445832</td>\n",
       "      <td>-0.445880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.928970</td>\n",
       "      <td>-1.652227</td>\n",
       "      <td>0.774271</td>\n",
       "      <td>-0.042941</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.523737</td>\n",
       "      <td>-1.639325</td>\n",
       "      <td>-1.450712</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-1.652227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.852308</td>\n",
       "      <td>-1.197727</td>\n",
       "      <td>-1.195772</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>0.865164</td>\n",
       "      <td>0.852308</td>\n",
       "      <td>-1.195772</td>\n",
       "      <td>0.865164</td>\n",
       "      <td>-1.124725</td>\n",
       "      <td>-1.124753</td>\n",
       "      <td>...</td>\n",
       "      <td>1.393868</td>\n",
       "      <td>0.088979</td>\n",
       "      <td>1.025346</td>\n",
       "      <td>2.500739</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>1.180220</td>\n",
       "      <td>-1.111083</td>\n",
       "      <td>0.208571</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>0.088979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.324921</td>\n",
       "      <td>-0.933850</td>\n",
       "      <td>-0.936050</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>-1.319078</td>\n",
       "      <td>-1.324921</td>\n",
       "      <td>-0.936050</td>\n",
       "      <td>-1.319078</td>\n",
       "      <td>0.586521</td>\n",
       "      <td>0.586476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045662</td>\n",
       "      <td>0.247271</td>\n",
       "      <td>-0.289810</td>\n",
       "      <td>0.419546</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>-0.523737</td>\n",
       "      <td>0.315172</td>\n",
       "      <td>0.208571</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>0.247271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.644537</td>\n",
       "      <td>-0.621995</td>\n",
       "      <td>-0.620673</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>-0.634858</td>\n",
       "      <td>-0.644537</td>\n",
       "      <td>-0.620673</td>\n",
       "      <td>-0.634858</td>\n",
       "      <td>-0.722067</td>\n",
       "      <td>-0.722111</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.279766</td>\n",
       "      <td>-1.177352</td>\n",
       "      <td>-0.349589</td>\n",
       "      <td>0.188303</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>1.180220</td>\n",
       "      <td>-0.741313</td>\n",
       "      <td>-0.897617</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-1.177352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.120806</td>\n",
       "      <td>-0.526039</td>\n",
       "      <td>-0.527916</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>-1.108549</td>\n",
       "      <td>-1.120806</td>\n",
       "      <td>-0.527916</td>\n",
       "      <td>-1.108549</td>\n",
       "      <td>1.937149</td>\n",
       "      <td>1.937200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.419236</td>\n",
       "      <td>0.405562</td>\n",
       "      <td>0.296032</td>\n",
       "      <td>0.419546</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>0.044249</td>\n",
       "      <td>-0.160246</td>\n",
       "      <td>-0.344523</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>0.405562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.848652</td>\n",
       "      <td>-0.358117</td>\n",
       "      <td>-0.360952</td>\n",
       "      <td>0.526015</td>\n",
       "      <td>-0.845387</td>\n",
       "      <td>-0.848652</td>\n",
       "      <td>-0.360952</td>\n",
       "      <td>-0.845387</td>\n",
       "      <td>0.702358</td>\n",
       "      <td>0.702353</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.116584</td>\n",
       "      <td>-0.227604</td>\n",
       "      <td>0.080825</td>\n",
       "      <td>-0.042941</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.523737</td>\n",
       "      <td>-0.318719</td>\n",
       "      <td>-1.450712</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.227604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.056423</td>\n",
       "      <td>4.487633</td>\n",
       "      <td>4.490280</td>\n",
       "      <td>0.792669</td>\n",
       "      <td>1.049377</td>\n",
       "      <td>1.056423</td>\n",
       "      <td>4.490280</td>\n",
       "      <td>1.049377</td>\n",
       "      <td>3.462793</td>\n",
       "      <td>3.462793</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.581482</td>\n",
       "      <td>-0.702478</td>\n",
       "      <td>-1.066947</td>\n",
       "      <td>0.419546</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>-0.523737</td>\n",
       "      <td>0.684941</td>\n",
       "      <td>-0.897617</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>-0.702478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.035847</td>\n",
       "      <td>-0.046262</td>\n",
       "      <td>-0.045575</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>0.023047</td>\n",
       "      <td>0.035847</td>\n",
       "      <td>-0.045575</td>\n",
       "      <td>0.023047</td>\n",
       "      <td>0.261995</td>\n",
       "      <td>0.262022</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.023604</td>\n",
       "      <td>-1.177352</td>\n",
       "      <td>-1.557141</td>\n",
       "      <td>-0.736672</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-1.091723</td>\n",
       "      <td>0.949062</td>\n",
       "      <td>-0.344523</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-1.177352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.644537</td>\n",
       "      <td>-0.526039</td>\n",
       "      <td>-0.527916</td>\n",
       "      <td>0.772157</td>\n",
       "      <td>-0.634858</td>\n",
       "      <td>-0.644537</td>\n",
       "      <td>-0.527916</td>\n",
       "      <td>-0.634858</td>\n",
       "      <td>-0.910794</td>\n",
       "      <td>-0.910791</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.488502</td>\n",
       "      <td>0.247271</td>\n",
       "      <td>-0.325677</td>\n",
       "      <td>-0.736672</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>0.044249</td>\n",
       "      <td>0.526468</td>\n",
       "      <td>-0.897617</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>0.247271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.780614</td>\n",
       "      <td>0.961270</td>\n",
       "      <td>0.965484</td>\n",
       "      <td>-0.109854</td>\n",
       "      <td>-0.792755</td>\n",
       "      <td>-0.780614</td>\n",
       "      <td>0.965484</td>\n",
       "      <td>-0.792755</td>\n",
       "      <td>0.969016</td>\n",
       "      <td>0.968986</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092152</td>\n",
       "      <td>-1.019061</td>\n",
       "      <td>-1.401714</td>\n",
       "      <td>0.882034</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>0.044249</td>\n",
       "      <td>0.790590</td>\n",
       "      <td>0.761665</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>-1.019061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.103885</td>\n",
       "      <td>-0.166206</td>\n",
       "      <td>-0.166160</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>0.101995</td>\n",
       "      <td>0.103885</td>\n",
       "      <td>-0.166160</td>\n",
       "      <td>0.101995</td>\n",
       "      <td>0.034051</td>\n",
       "      <td>0.034014</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.116584</td>\n",
       "      <td>-0.385895</td>\n",
       "      <td>-0.122426</td>\n",
       "      <td>-0.274184</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.523737</td>\n",
       "      <td>-0.213071</td>\n",
       "      <td>-1.450712</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.385895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.239962</td>\n",
       "      <td>-0.717950</td>\n",
       "      <td>-0.713431</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>0.233576</td>\n",
       "      <td>0.239962</td>\n",
       "      <td>-0.713431</td>\n",
       "      <td>0.233576</td>\n",
       "      <td>-0.411252</td>\n",
       "      <td>-0.411234</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324602</td>\n",
       "      <td>-0.227604</td>\n",
       "      <td>-1.270198</td>\n",
       "      <td>0.188303</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>0.044249</td>\n",
       "      <td>1.318832</td>\n",
       "      <td>1.867854</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>-0.227604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.188844</td>\n",
       "      <td>1.560992</td>\n",
       "      <td>1.559134</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>-1.187497</td>\n",
       "      <td>-1.188844</td>\n",
       "      <td>1.559134</td>\n",
       "      <td>-1.187497</td>\n",
       "      <td>1.658394</td>\n",
       "      <td>1.658394</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.070094</td>\n",
       "      <td>0.405562</td>\n",
       "      <td>-0.839784</td>\n",
       "      <td>-0.042941</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>1.180220</td>\n",
       "      <td>1.318832</td>\n",
       "      <td>0.208571</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>0.405562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.236307</td>\n",
       "      <td>-1.005816</td>\n",
       "      <td>-1.010256</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>-0.240115</td>\n",
       "      <td>-0.236307</td>\n",
       "      <td>-1.010256</td>\n",
       "      <td>-0.240115</td>\n",
       "      <td>-0.829537</td>\n",
       "      <td>-0.829560</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.465726</td>\n",
       "      <td>0.880436</td>\n",
       "      <td>0.439504</td>\n",
       "      <td>1.575765</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>1.748206</td>\n",
       "      <td>-0.107422</td>\n",
       "      <td>0.208571</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>0.880436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.444077</td>\n",
       "      <td>1.105203</td>\n",
       "      <td>1.104621</td>\n",
       "      <td>1.674680</td>\n",
       "      <td>0.444105</td>\n",
       "      <td>0.444077</td>\n",
       "      <td>1.104621</td>\n",
       "      <td>0.444105</td>\n",
       "      <td>0.273287</td>\n",
       "      <td>0.273258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650030</td>\n",
       "      <td>0.088979</td>\n",
       "      <td>1.013390</td>\n",
       "      <td>2.963226</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>-0.523737</td>\n",
       "      <td>-0.952610</td>\n",
       "      <td>-0.344523</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>0.088979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-1.529036</td>\n",
       "      <td>0.145649</td>\n",
       "      <td>0.149216</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>-1.529607</td>\n",
       "      <td>-1.529036</td>\n",
       "      <td>0.149216</td>\n",
       "      <td>-1.529607</td>\n",
       "      <td>1.132136</td>\n",
       "      <td>1.132150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092152</td>\n",
       "      <td>2.305059</td>\n",
       "      <td>2.197030</td>\n",
       "      <td>-0.505428</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>2.884177</td>\n",
       "      <td>-1.058258</td>\n",
       "      <td>0.208571</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>2.305059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.056423</td>\n",
       "      <td>-0.214184</td>\n",
       "      <td>-0.212539</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>1.049377</td>\n",
       "      <td>1.056423</td>\n",
       "      <td>-0.212539</td>\n",
       "      <td>1.049377</td>\n",
       "      <td>-0.457123</td>\n",
       "      <td>-0.457116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.835990</td>\n",
       "      <td>-0.227604</td>\n",
       "      <td>0.200385</td>\n",
       "      <td>0.419546</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.523737</td>\n",
       "      <td>-0.530016</td>\n",
       "      <td>0.208571</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.227604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.168268</td>\n",
       "      <td>-0.598006</td>\n",
       "      <td>-0.602122</td>\n",
       "      <td>-0.273949</td>\n",
       "      <td>-0.161167</td>\n",
       "      <td>-0.168268</td>\n",
       "      <td>-0.602122</td>\n",
       "      <td>-0.161167</td>\n",
       "      <td>-1.105570</td>\n",
       "      <td>-1.105557</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.140297</td>\n",
       "      <td>1.197019</td>\n",
       "      <td>0.463416</td>\n",
       "      <td>0.188303</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>0.044249</td>\n",
       "      <td>0.156699</td>\n",
       "      <td>0.208571</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>1.197019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.256883</td>\n",
       "      <td>-1.605538</td>\n",
       "      <td>-1.603906</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>-1.266446</td>\n",
       "      <td>-1.256883</td>\n",
       "      <td>-1.603906</td>\n",
       "      <td>-1.266446</td>\n",
       "      <td>-1.036511</td>\n",
       "      <td>-1.036499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092152</td>\n",
       "      <td>0.563853</td>\n",
       "      <td>0.272121</td>\n",
       "      <td>-0.042941</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>0.044249</td>\n",
       "      <td>-0.054598</td>\n",
       "      <td>0.208571</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>0.563853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.168268</td>\n",
       "      <td>0.625426</td>\n",
       "      <td>0.622281</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>-0.161167</td>\n",
       "      <td>-0.168268</td>\n",
       "      <td>0.622281</td>\n",
       "      <td>-0.161167</td>\n",
       "      <td>-0.632240</td>\n",
       "      <td>-0.632219</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.674462</td>\n",
       "      <td>-0.227604</td>\n",
       "      <td>-0.708268</td>\n",
       "      <td>-0.505428</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.523737</td>\n",
       "      <td>0.737765</td>\n",
       "      <td>-0.897617</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.227604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.239962</td>\n",
       "      <td>0.361549</td>\n",
       "      <td>0.362559</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>0.233576</td>\n",
       "      <td>0.239962</td>\n",
       "      <td>0.362559</td>\n",
       "      <td>0.233576</td>\n",
       "      <td>0.079418</td>\n",
       "      <td>0.079428</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.465726</td>\n",
       "      <td>-1.493935</td>\n",
       "      <td>-1.748437</td>\n",
       "      <td>-0.736672</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>0.612234</td>\n",
       "      <td>0.526468</td>\n",
       "      <td>-0.344523</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-1.493935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.103885</td>\n",
       "      <td>0.217616</td>\n",
       "      <td>0.214146</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>0.101995</td>\n",
       "      <td>0.103885</td>\n",
       "      <td>0.214146</td>\n",
       "      <td>0.101995</td>\n",
       "      <td>0.153417</td>\n",
       "      <td>0.153402</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.326256</td>\n",
       "      <td>0.563853</td>\n",
       "      <td>-0.182206</td>\n",
       "      <td>-0.505428</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>0.612234</td>\n",
       "      <td>0.420820</td>\n",
       "      <td>0.208571</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>0.563853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.372383</td>\n",
       "      <td>-1.149750</td>\n",
       "      <td>-1.149393</td>\n",
       "      <td>-0.294461</td>\n",
       "      <td>-0.371696</td>\n",
       "      <td>-0.372383</td>\n",
       "      <td>-1.149393</td>\n",
       "      <td>-0.371696</td>\n",
       "      <td>-0.647463</td>\n",
       "      <td>-0.647435</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.744665</td>\n",
       "      <td>1.038728</td>\n",
       "      <td>1.945955</td>\n",
       "      <td>-1.199159</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.523737</td>\n",
       "      <td>-0.899786</td>\n",
       "      <td>-0.897617</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>1.038728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.716231</td>\n",
       "      <td>2.280658</td>\n",
       "      <td>2.282645</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>0.707267</td>\n",
       "      <td>0.716231</td>\n",
       "      <td>2.282645</td>\n",
       "      <td>0.707267</td>\n",
       "      <td>2.034134</td>\n",
       "      <td>2.034115</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.372746</td>\n",
       "      <td>-0.069312</td>\n",
       "      <td>-0.899564</td>\n",
       "      <td>-0.042941</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>-0.523737</td>\n",
       "      <td>1.054711</td>\n",
       "      <td>0.761665</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>-0.069312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-1.188844</td>\n",
       "      <td>0.289582</td>\n",
       "      <td>0.288353</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>-1.187497</td>\n",
       "      <td>-1.188844</td>\n",
       "      <td>0.288353</td>\n",
       "      <td>-1.187497</td>\n",
       "      <td>1.114594</td>\n",
       "      <td>1.114593</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.419236</td>\n",
       "      <td>-0.544187</td>\n",
       "      <td>-0.002867</td>\n",
       "      <td>-0.042941</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>0.044249</td>\n",
       "      <td>-0.530016</td>\n",
       "      <td>-0.897617</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.544187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.580154</td>\n",
       "      <td>-0.262162</td>\n",
       "      <td>-0.258918</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>0.575686</td>\n",
       "      <td>0.580154</td>\n",
       "      <td>-0.258918</td>\n",
       "      <td>0.575686</td>\n",
       "      <td>0.815877</td>\n",
       "      <td>0.815889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.743010</td>\n",
       "      <td>-0.702478</td>\n",
       "      <td>-1.389758</td>\n",
       "      <td>-1.199159</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>0.612234</td>\n",
       "      <td>0.949062</td>\n",
       "      <td>1.867854</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.702478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3001</th>\n",
       "      <td>-0.168268</td>\n",
       "      <td>-1.533571</td>\n",
       "      <td>-1.529700</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>-0.161167</td>\n",
       "      <td>-0.168268</td>\n",
       "      <td>-1.529700</td>\n",
       "      <td>-0.161167</td>\n",
       "      <td>-1.074216</td>\n",
       "      <td>-1.074189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.371091</td>\n",
       "      <td>-0.860769</td>\n",
       "      <td>-0.397413</td>\n",
       "      <td>-0.967915</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.523737</td>\n",
       "      <td>-0.318719</td>\n",
       "      <td>-0.344523</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.860769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3002</th>\n",
       "      <td>-0.508460</td>\n",
       "      <td>-0.502050</td>\n",
       "      <td>-0.500088</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>-0.503277</td>\n",
       "      <td>-0.508460</td>\n",
       "      <td>-0.500088</td>\n",
       "      <td>-0.503277</td>\n",
       "      <td>-0.387863</td>\n",
       "      <td>-0.387824</td>\n",
       "      <td>...</td>\n",
       "      <td>1.905256</td>\n",
       "      <td>1.988476</td>\n",
       "      <td>0.929698</td>\n",
       "      <td>0.419546</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>0.044249</td>\n",
       "      <td>-0.107422</td>\n",
       "      <td>2.974042</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>1.988476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3003</th>\n",
       "      <td>-0.508460</td>\n",
       "      <td>-0.166206</td>\n",
       "      <td>-0.166160</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>-0.503277</td>\n",
       "      <td>-0.508460</td>\n",
       "      <td>-0.166160</td>\n",
       "      <td>-0.503277</td>\n",
       "      <td>-0.476278</td>\n",
       "      <td>-0.476312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045662</td>\n",
       "      <td>0.722145</td>\n",
       "      <td>2.173118</td>\n",
       "      <td>1.113277</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>-0.523737</td>\n",
       "      <td>-1.163907</td>\n",
       "      <td>-0.897617</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>0.722145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3004</th>\n",
       "      <td>-0.508460</td>\n",
       "      <td>0.529471</td>\n",
       "      <td>0.529523</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>-0.503277</td>\n",
       "      <td>-0.508460</td>\n",
       "      <td>0.529523</td>\n",
       "      <td>-0.503277</td>\n",
       "      <td>0.263205</td>\n",
       "      <td>0.263192</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.279766</td>\n",
       "      <td>-0.069312</td>\n",
       "      <td>0.750359</td>\n",
       "      <td>-0.042941</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>0.044249</td>\n",
       "      <td>-0.741313</td>\n",
       "      <td>-0.897617</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.069312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3005</th>\n",
       "      <td>-0.304345</td>\n",
       "      <td>-0.118229</td>\n",
       "      <td>-0.119781</td>\n",
       "      <td>-0.294461</td>\n",
       "      <td>-0.292747</td>\n",
       "      <td>-0.304345</td>\n",
       "      <td>-0.119781</td>\n",
       "      <td>-0.292747</td>\n",
       "      <td>-0.015147</td>\n",
       "      <td>-0.015146</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.326256</td>\n",
       "      <td>-0.702478</td>\n",
       "      <td>0.379724</td>\n",
       "      <td>-0.505428</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>0.612234</td>\n",
       "      <td>-0.952610</td>\n",
       "      <td>-0.897617</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.702478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3006</th>\n",
       "      <td>-1.052767</td>\n",
       "      <td>-1.173738</td>\n",
       "      <td>-1.177220</td>\n",
       "      <td>-0.294461</td>\n",
       "      <td>-1.055916</td>\n",
       "      <td>-1.052767</td>\n",
       "      <td>-1.177220</td>\n",
       "      <td>-1.055916</td>\n",
       "      <td>-0.816027</td>\n",
       "      <td>-0.815983</td>\n",
       "      <td>...</td>\n",
       "      <td>0.464071</td>\n",
       "      <td>0.247271</td>\n",
       "      <td>1.013390</td>\n",
       "      <td>0.419546</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.523737</td>\n",
       "      <td>-0.741313</td>\n",
       "      <td>-0.344523</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>0.247271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3007</th>\n",
       "      <td>0.580154</td>\n",
       "      <td>1.057226</td>\n",
       "      <td>1.058242</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>0.575686</td>\n",
       "      <td>0.580154</td>\n",
       "      <td>1.058242</td>\n",
       "      <td>0.575686</td>\n",
       "      <td>0.858522</td>\n",
       "      <td>0.858494</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.698175</td>\n",
       "      <td>-1.019061</td>\n",
       "      <td>-0.301766</td>\n",
       "      <td>0.419546</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.523737</td>\n",
       "      <td>-0.635665</td>\n",
       "      <td>-1.450712</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-1.019061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3008</th>\n",
       "      <td>0.376039</td>\n",
       "      <td>0.073682</td>\n",
       "      <td>0.075010</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>0.365157</td>\n",
       "      <td>0.376039</td>\n",
       "      <td>0.075010</td>\n",
       "      <td>0.365157</td>\n",
       "      <td>-0.141772</td>\n",
       "      <td>-0.141791</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.070094</td>\n",
       "      <td>2.305059</td>\n",
       "      <td>1.922043</td>\n",
       "      <td>-0.967915</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.523737</td>\n",
       "      <td>-0.424368</td>\n",
       "      <td>-0.897617</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>2.305059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3009</th>\n",
       "      <td>-0.440422</td>\n",
       "      <td>0.289582</td>\n",
       "      <td>0.288353</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>-0.450645</td>\n",
       "      <td>-0.440422</td>\n",
       "      <td>0.288353</td>\n",
       "      <td>-0.450645</td>\n",
       "      <td>0.531274</td>\n",
       "      <td>0.531230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.417581</td>\n",
       "      <td>0.722145</td>\n",
       "      <td>0.929698</td>\n",
       "      <td>0.650790</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>-0.523737</td>\n",
       "      <td>-0.530016</td>\n",
       "      <td>0.208571</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>0.722145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3010</th>\n",
       "      <td>1.124461</td>\n",
       "      <td>-0.070251</td>\n",
       "      <td>-0.073403</td>\n",
       "      <td>0.628574</td>\n",
       "      <td>1.128326</td>\n",
       "      <td>1.124461</td>\n",
       "      <td>-0.073403</td>\n",
       "      <td>1.128326</td>\n",
       "      <td>1.569172</td>\n",
       "      <td>1.569204</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.326256</td>\n",
       "      <td>1.830185</td>\n",
       "      <td>0.690579</td>\n",
       "      <td>0.419546</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>1.180220</td>\n",
       "      <td>0.156699</td>\n",
       "      <td>0.761665</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>1.830185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3011</th>\n",
       "      <td>-2.073343</td>\n",
       "      <td>-0.190195</td>\n",
       "      <td>-0.193988</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>-2.082247</td>\n",
       "      <td>-2.073343</td>\n",
       "      <td>-0.193988</td>\n",
       "      <td>-2.082247</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045662</td>\n",
       "      <td>-0.227604</td>\n",
       "      <td>0.511240</td>\n",
       "      <td>-0.042941</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.523737</td>\n",
       "      <td>-0.635665</td>\n",
       "      <td>-0.344523</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.227604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3012</th>\n",
       "      <td>1.260538</td>\n",
       "      <td>-0.813905</td>\n",
       "      <td>-0.815465</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>1.259906</td>\n",
       "      <td>1.260538</td>\n",
       "      <td>-0.815465</td>\n",
       "      <td>1.259906</td>\n",
       "      <td>-0.486158</td>\n",
       "      <td>-0.486144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510561</td>\n",
       "      <td>-0.860769</td>\n",
       "      <td>-0.768048</td>\n",
       "      <td>-1.199159</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-1.091723</td>\n",
       "      <td>-0.054598</td>\n",
       "      <td>0.208571</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.860769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3014</th>\n",
       "      <td>-0.508460</td>\n",
       "      <td>0.697393</td>\n",
       "      <td>0.696487</td>\n",
       "      <td>4.279689</td>\n",
       "      <td>-0.503277</td>\n",
       "      <td>-0.508460</td>\n",
       "      <td>0.696487</td>\n",
       "      <td>-0.503277</td>\n",
       "      <td>-0.041359</td>\n",
       "      <td>-0.041365</td>\n",
       "      <td>...</td>\n",
       "      <td>4.136769</td>\n",
       "      <td>-1.335644</td>\n",
       "      <td>-0.170250</td>\n",
       "      <td>10.594267</td>\n",
       "      <td>3.004494</td>\n",
       "      <td>-0.523737</td>\n",
       "      <td>-1.269555</td>\n",
       "      <td>0.761665</td>\n",
       "      <td>3.004494</td>\n",
       "      <td>-1.335644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3016</th>\n",
       "      <td>1.328577</td>\n",
       "      <td>1.153181</td>\n",
       "      <td>1.151000</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>1.338855</td>\n",
       "      <td>1.328577</td>\n",
       "      <td>1.151000</td>\n",
       "      <td>1.338855</td>\n",
       "      <td>-0.476077</td>\n",
       "      <td>-0.476078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278112</td>\n",
       "      <td>-0.702478</td>\n",
       "      <td>-0.875652</td>\n",
       "      <td>-0.736672</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>0.044249</td>\n",
       "      <td>0.209523</td>\n",
       "      <td>0.208571</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.702478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3017</th>\n",
       "      <td>0.308001</td>\n",
       "      <td>1.105203</td>\n",
       "      <td>1.104621</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>0.312524</td>\n",
       "      <td>0.308001</td>\n",
       "      <td>1.104621</td>\n",
       "      <td>0.312524</td>\n",
       "      <td>0.117627</td>\n",
       "      <td>0.117585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324602</td>\n",
       "      <td>-0.544187</td>\n",
       "      <td>-0.337633</td>\n",
       "      <td>-0.042941</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.523737</td>\n",
       "      <td>-0.213071</td>\n",
       "      <td>0.208571</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.544187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3018</th>\n",
       "      <td>-1.188844</td>\n",
       "      <td>0.721382</td>\n",
       "      <td>0.724314</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>-1.187497</td>\n",
       "      <td>-1.188844</td>\n",
       "      <td>0.724314</td>\n",
       "      <td>-1.187497</td>\n",
       "      <td>-0.135924</td>\n",
       "      <td>-0.135939</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.140297</td>\n",
       "      <td>0.247271</td>\n",
       "      <td>-1.234330</td>\n",
       "      <td>0.419546</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>0.044249</td>\n",
       "      <td>1.952722</td>\n",
       "      <td>1.867854</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>0.247271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3019</th>\n",
       "      <td>-1.324921</td>\n",
       "      <td>-1.101772</td>\n",
       "      <td>-1.103014</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>-1.319078</td>\n",
       "      <td>-1.324921</td>\n",
       "      <td>-1.103014</td>\n",
       "      <td>-1.319078</td>\n",
       "      <td>-0.870972</td>\n",
       "      <td>-0.870995</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.419236</td>\n",
       "      <td>-0.385895</td>\n",
       "      <td>-0.074602</td>\n",
       "      <td>-0.042941</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.523737</td>\n",
       "      <td>-0.265895</td>\n",
       "      <td>-0.344523</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.385895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3020</th>\n",
       "      <td>-0.100230</td>\n",
       "      <td>0.961270</td>\n",
       "      <td>0.965484</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>-0.108534</td>\n",
       "      <td>-0.100230</td>\n",
       "      <td>0.965484</td>\n",
       "      <td>-0.108534</td>\n",
       "      <td>1.010653</td>\n",
       "      <td>1.010655</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.419236</td>\n",
       "      <td>0.880436</td>\n",
       "      <td>1.097082</td>\n",
       "      <td>-1.199159</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.523737</td>\n",
       "      <td>-0.477192</td>\n",
       "      <td>-0.344523</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>0.880436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3021</th>\n",
       "      <td>0.171924</td>\n",
       "      <td>0.361549</td>\n",
       "      <td>0.362559</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>0.180944</td>\n",
       "      <td>0.171924</td>\n",
       "      <td>0.362559</td>\n",
       "      <td>0.180944</td>\n",
       "      <td>-0.831855</td>\n",
       "      <td>-0.831901</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.326256</td>\n",
       "      <td>0.088979</td>\n",
       "      <td>-0.277854</td>\n",
       "      <td>0.882034</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>-0.523737</td>\n",
       "      <td>0.262347</td>\n",
       "      <td>0.208571</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>0.088979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3022</th>\n",
       "      <td>1.192500</td>\n",
       "      <td>0.769359</td>\n",
       "      <td>0.770693</td>\n",
       "      <td>1.018300</td>\n",
       "      <td>1.180958</td>\n",
       "      <td>1.192500</td>\n",
       "      <td>0.770693</td>\n",
       "      <td>1.180958</td>\n",
       "      <td>0.183359</td>\n",
       "      <td>0.183366</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138642</td>\n",
       "      <td>0.088979</td>\n",
       "      <td>-1.401714</td>\n",
       "      <td>-0.736672</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.523737</td>\n",
       "      <td>2.164019</td>\n",
       "      <td>2.420948</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>0.088979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3023</th>\n",
       "      <td>0.103885</td>\n",
       "      <td>0.121660</td>\n",
       "      <td>0.121389</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>0.101995</td>\n",
       "      <td>0.103885</td>\n",
       "      <td>0.121389</td>\n",
       "      <td>0.101995</td>\n",
       "      <td>0.173076</td>\n",
       "      <td>0.173066</td>\n",
       "      <td>...</td>\n",
       "      <td>1.114929</td>\n",
       "      <td>-0.702478</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2.038252</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>0.612234</td>\n",
       "      <td>-1.163907</td>\n",
       "      <td>-0.344523</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>-0.702478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3024</th>\n",
       "      <td>-0.508460</td>\n",
       "      <td>-0.885872</td>\n",
       "      <td>-0.889671</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>-0.503277</td>\n",
       "      <td>-0.508460</td>\n",
       "      <td>-0.889671</td>\n",
       "      <td>-0.503277</td>\n",
       "      <td>-0.866738</td>\n",
       "      <td>-0.866781</td>\n",
       "      <td>...</td>\n",
       "      <td>1.114929</td>\n",
       "      <td>-0.069312</td>\n",
       "      <td>-0.277854</td>\n",
       "      <td>4.813176</td>\n",
       "      <td>3.004494</td>\n",
       "      <td>-0.523737</td>\n",
       "      <td>-0.054598</td>\n",
       "      <td>1.314760</td>\n",
       "      <td>3.004494</td>\n",
       "      <td>-0.069312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3025</th>\n",
       "      <td>1.260538</td>\n",
       "      <td>1.537003</td>\n",
       "      <td>1.540583</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>1.259906</td>\n",
       "      <td>1.260538</td>\n",
       "      <td>1.540583</td>\n",
       "      <td>1.259906</td>\n",
       "      <td>0.529560</td>\n",
       "      <td>0.529591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.789500</td>\n",
       "      <td>-0.860769</td>\n",
       "      <td>-1.557141</td>\n",
       "      <td>-0.736672</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>0.044249</td>\n",
       "      <td>1.213183</td>\n",
       "      <td>1.867854</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.860769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3026</th>\n",
       "      <td>-0.712575</td>\n",
       "      <td>-1.005816</td>\n",
       "      <td>-1.010256</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>-0.713806</td>\n",
       "      <td>-0.712575</td>\n",
       "      <td>-1.010256</td>\n",
       "      <td>-0.713806</td>\n",
       "      <td>-0.725394</td>\n",
       "      <td>-0.725388</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.651685</td>\n",
       "      <td>-0.702478</td>\n",
       "      <td>-1.808217</td>\n",
       "      <td>-0.505428</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.523737</td>\n",
       "      <td>2.322492</td>\n",
       "      <td>0.761665</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.702478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3027</th>\n",
       "      <td>0.103885</td>\n",
       "      <td>0.361549</td>\n",
       "      <td>0.362559</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>0.101995</td>\n",
       "      <td>0.103885</td>\n",
       "      <td>0.362559</td>\n",
       "      <td>0.101995</td>\n",
       "      <td>-0.699888</td>\n",
       "      <td>-0.699872</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.884134</td>\n",
       "      <td>-0.385895</td>\n",
       "      <td>0.068869</td>\n",
       "      <td>-0.042941</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>0.612234</td>\n",
       "      <td>-0.477192</td>\n",
       "      <td>-0.897617</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.385895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3028</th>\n",
       "      <td>0.580154</td>\n",
       "      <td>-1.173738</td>\n",
       "      <td>-1.177220</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>0.575686</td>\n",
       "      <td>0.580154</td>\n",
       "      <td>-1.177220</td>\n",
       "      <td>0.575686</td>\n",
       "      <td>0.594385</td>\n",
       "      <td>0.594436</td>\n",
       "      <td>...</td>\n",
       "      <td>1.626317</td>\n",
       "      <td>0.563853</td>\n",
       "      <td>1.276421</td>\n",
       "      <td>-0.505428</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.523737</td>\n",
       "      <td>-0.899786</td>\n",
       "      <td>0.761665</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>0.563853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3029</th>\n",
       "      <td>-0.916691</td>\n",
       "      <td>1.992791</td>\n",
       "      <td>1.995096</td>\n",
       "      <td>-0.294461</td>\n",
       "      <td>-0.924336</td>\n",
       "      <td>-0.916691</td>\n",
       "      <td>1.995096</td>\n",
       "      <td>-0.924336</td>\n",
       "      <td>1.448899</td>\n",
       "      <td>1.448880</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.534992</td>\n",
       "      <td>1.355311</td>\n",
       "      <td>-0.361545</td>\n",
       "      <td>-0.042941</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>0.044249</td>\n",
       "      <td>1.424480</td>\n",
       "      <td>-0.344523</td>\n",
       "      <td>1.177752</td>\n",
       "      <td>1.355311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3030</th>\n",
       "      <td>1.940922</td>\n",
       "      <td>-0.142217</td>\n",
       "      <td>-0.138333</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>1.944127</td>\n",
       "      <td>1.940922</td>\n",
       "      <td>-0.138333</td>\n",
       "      <td>1.944127</td>\n",
       "      <td>-0.345218</td>\n",
       "      <td>-0.345219</td>\n",
       "      <td>...</td>\n",
       "      <td>1.951746</td>\n",
       "      <td>1.513602</td>\n",
       "      <td>0.654711</td>\n",
       "      <td>-0.505428</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.523737</td>\n",
       "      <td>-0.054598</td>\n",
       "      <td>2.420948</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>1.513602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3031</th>\n",
       "      <td>-0.848652</td>\n",
       "      <td>0.049694</td>\n",
       "      <td>0.047182</td>\n",
       "      <td>-0.294461</td>\n",
       "      <td>-0.845387</td>\n",
       "      <td>-0.848652</td>\n",
       "      <td>0.047182</td>\n",
       "      <td>-0.845387</td>\n",
       "      <td>-0.111829</td>\n",
       "      <td>-0.111827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510561</td>\n",
       "      <td>-0.069312</td>\n",
       "      <td>0.571020</td>\n",
       "      <td>0.419546</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>0.612234</td>\n",
       "      <td>-0.794137</td>\n",
       "      <td>-0.344523</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>-0.069312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3032</th>\n",
       "      <td>0.716231</td>\n",
       "      <td>-0.981828</td>\n",
       "      <td>-0.982429</td>\n",
       "      <td>-0.314972</td>\n",
       "      <td>0.707267</td>\n",
       "      <td>0.716231</td>\n",
       "      <td>-0.982429</td>\n",
       "      <td>0.707267</td>\n",
       "      <td>-0.811793</td>\n",
       "      <td>-0.811769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510561</td>\n",
       "      <td>0.880436</td>\n",
       "      <td>1.647056</td>\n",
       "      <td>-0.967915</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>0.044249</td>\n",
       "      <td>-0.952610</td>\n",
       "      <td>-0.344523</td>\n",
       "      <td>-0.648990</td>\n",
       "      <td>0.880436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2978 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LBDHDL_0   LBXTC_0  Total cholesterol (mmol/L)\\n  Toxoplasma (IgG)  \\\n",
       "0    -1.120806  1.297114                      1.299412         -0.314972   \n",
       "1    -0.576499 -0.598006                     -0.602122         -0.273949   \n",
       "2    -0.984729  0.001716                      0.000804         -0.314972   \n",
       "3     2.825421  0.313571                      0.316180         -0.294461   \n",
       "4    -1.529036 -0.957839                     -0.954601         -0.314972   \n",
       "5     0.852308 -1.197727                     -1.195772         -0.314972   \n",
       "6    -1.324921 -0.933850                     -0.936050         -0.314972   \n",
       "7    -0.644537 -0.621995                     -0.620673         -0.314972   \n",
       "8    -1.120806 -0.526039                     -0.527916         -0.314972   \n",
       "9    -0.848652 -0.358117                     -0.360952          0.526015   \n",
       "10    1.056423  4.487633                      4.490280          0.792669   \n",
       "11    0.035847 -0.046262                     -0.045575         -0.314972   \n",
       "12   -0.644537 -0.526039                     -0.527916          0.772157   \n",
       "13   -0.780614  0.961270                      0.965484         -0.109854   \n",
       "14    0.103885 -0.166206                     -0.166160         -0.314972   \n",
       "15    0.239962 -0.717950                     -0.713431         -0.314972   \n",
       "16   -1.188844  1.560992                      1.559134         -0.314972   \n",
       "17   -0.236307 -1.005816                     -1.010256         -0.314972   \n",
       "18    0.444077  1.105203                      1.104621          1.674680   \n",
       "19   -1.529036  0.145649                      0.149216         -0.314972   \n",
       "20    1.056423 -0.214184                     -0.212539         -0.314972   \n",
       "21   -0.168268 -0.598006                     -0.602122         -0.273949   \n",
       "22   -1.256883 -1.605538                     -1.603906         -0.314972   \n",
       "23   -0.168268  0.625426                      0.622281         -0.314972   \n",
       "24    0.239962  0.361549                      0.362559         -0.314972   \n",
       "25    0.103885  0.217616                      0.214146         -0.314972   \n",
       "26   -0.372383 -1.149750                     -1.149393         -0.294461   \n",
       "27    0.716231  2.280658                      2.282645         -0.314972   \n",
       "28   -1.188844  0.289582                      0.288353         -0.314972   \n",
       "29    0.580154 -0.262162                     -0.258918         -0.314972   \n",
       "...        ...       ...                           ...               ...   \n",
       "3001 -0.168268 -1.533571                     -1.529700         -0.314972   \n",
       "3002 -0.508460 -0.502050                     -0.500088         -0.314972   \n",
       "3003 -0.508460 -0.166206                     -0.166160         -0.314972   \n",
       "3004 -0.508460  0.529471                      0.529523         -0.314972   \n",
       "3005 -0.304345 -0.118229                     -0.119781         -0.294461   \n",
       "3006 -1.052767 -1.173738                     -1.177220         -0.294461   \n",
       "3007  0.580154  1.057226                      1.058242         -0.314972   \n",
       "3008  0.376039  0.073682                      0.075010         -0.314972   \n",
       "3009 -0.440422  0.289582                      0.288353         -0.314972   \n",
       "3010  1.124461 -0.070251                     -0.073403          0.628574   \n",
       "3011 -2.073343 -0.190195                     -0.193988         -0.314972   \n",
       "3012  1.260538 -0.813905                     -0.815465         -0.314972   \n",
       "3014 -0.508460  0.697393                      0.696487          4.279689   \n",
       "3016  1.328577  1.153181                      1.151000         -0.314972   \n",
       "3017  0.308001  1.105203                      1.104621         -0.314972   \n",
       "3018 -1.188844  0.721382                      0.724314         -0.314972   \n",
       "3019 -1.324921 -1.101772                     -1.103014         -0.314972   \n",
       "3020 -0.100230  0.961270                      0.965484         -0.314972   \n",
       "3021  0.171924  0.361549                      0.362559         -0.314972   \n",
       "3022  1.192500  0.769359                      0.770693          1.018300   \n",
       "3023  0.103885  0.121660                      0.121389         -0.314972   \n",
       "3024 -0.508460 -0.885872                     -0.889671         -0.314972   \n",
       "3025  1.260538  1.537003                      1.540583         -0.314972   \n",
       "3026 -0.712575 -1.005816                     -1.010256         -0.314972   \n",
       "3027  0.103885  0.361549                      0.362559         -0.314972   \n",
       "3028  0.580154 -1.173738                     -1.177220         -0.314972   \n",
       "3029 -0.916691  1.992791                      1.995096         -0.294461   \n",
       "3030  1.940922 -0.142217                     -0.138333         -0.314972   \n",
       "3031 -0.848652  0.049694                      0.047182         -0.294461   \n",
       "3032  0.716231 -0.981828                     -0.982429         -0.314972   \n",
       "\n",
       "      HDL-cholesterol (mmol/L)\\n  HDL-cholesterol (mg/dL)\\n  LBDTCSI_0  \\\n",
       "0                      -1.108549                  -1.120806   1.299412   \n",
       "1                      -0.582225                  -0.576499  -0.602122   \n",
       "2                      -0.976968                  -0.984729   0.000804   \n",
       "3                       2.812560                   2.825421   0.316180   \n",
       "4                      -1.529607                  -1.529036  -0.954601   \n",
       "5                       0.865164                   0.852308  -1.195772   \n",
       "6                      -1.319078                  -1.324921  -0.936050   \n",
       "7                      -0.634858                  -0.644537  -0.620673   \n",
       "8                      -1.108549                  -1.120806  -0.527916   \n",
       "9                      -0.845387                  -0.848652  -0.360952   \n",
       "10                      1.049377                   1.056423   4.490280   \n",
       "11                      0.023047                   0.035847  -0.045575   \n",
       "12                     -0.634858                  -0.644537  -0.527916   \n",
       "13                     -0.792755                  -0.780614   0.965484   \n",
       "14                      0.101995                   0.103885  -0.166160   \n",
       "15                      0.233576                   0.239962  -0.713431   \n",
       "16                     -1.187497                  -1.188844   1.559134   \n",
       "17                     -0.240115                  -0.236307  -1.010256   \n",
       "18                      0.444105                   0.444077   1.104621   \n",
       "19                     -1.529607                  -1.529036   0.149216   \n",
       "20                      1.049377                   1.056423  -0.212539   \n",
       "21                     -0.161167                  -0.168268  -0.602122   \n",
       "22                     -1.266446                  -1.256883  -1.603906   \n",
       "23                     -0.161167                  -0.168268   0.622281   \n",
       "24                      0.233576                   0.239962   0.362559   \n",
       "25                      0.101995                   0.103885   0.214146   \n",
       "26                     -0.371696                  -0.372383  -1.149393   \n",
       "27                      0.707267                   0.716231   2.282645   \n",
       "28                     -1.187497                  -1.188844   0.288353   \n",
       "29                      0.575686                   0.580154  -0.258918   \n",
       "...                          ...                        ...        ...   \n",
       "3001                   -0.161167                  -0.168268  -1.529700   \n",
       "3002                   -0.503277                  -0.508460  -0.500088   \n",
       "3003                   -0.503277                  -0.508460  -0.166160   \n",
       "3004                   -0.503277                  -0.508460   0.529523   \n",
       "3005                   -0.292747                  -0.304345  -0.119781   \n",
       "3006                   -1.055916                  -1.052767  -1.177220   \n",
       "3007                    0.575686                   0.580154   1.058242   \n",
       "3008                    0.365157                   0.376039   0.075010   \n",
       "3009                   -0.450645                  -0.440422   0.288353   \n",
       "3010                    1.128326                   1.124461  -0.073403   \n",
       "3011                   -2.082247                  -2.073343  -0.193988   \n",
       "3012                    1.259906                   1.260538  -0.815465   \n",
       "3014                   -0.503277                  -0.508460   0.696487   \n",
       "3016                    1.338855                   1.328577   1.151000   \n",
       "3017                    0.312524                   0.308001   1.104621   \n",
       "3018                   -1.187497                  -1.188844   0.724314   \n",
       "3019                   -1.319078                  -1.324921  -1.103014   \n",
       "3020                   -0.108534                  -0.100230   0.965484   \n",
       "3021                    0.180944                   0.171924   0.362559   \n",
       "3022                    1.180958                   1.192500   0.770693   \n",
       "3023                    0.101995                   0.103885   0.121389   \n",
       "3024                   -0.503277                  -0.508460  -0.889671   \n",
       "3025                    1.259906                   1.260538   1.540583   \n",
       "3026                   -0.713806                  -0.712575  -1.010256   \n",
       "3027                    0.101995                   0.103885   0.362559   \n",
       "3028                    0.575686                   0.580154  -1.177220   \n",
       "3029                   -0.924336                  -0.916691   1.995096   \n",
       "3030                    1.944127                   1.940922  -0.138333   \n",
       "3031                   -0.845387                  -0.848652   0.047182   \n",
       "3032                    0.707267                   0.716231  -0.982429   \n",
       "\n",
       "      LBDHDLSI_0  Vitamin E (umol/L)  Vitamin E (ug/dL)        ...          \\\n",
       "0      -1.108549            1.492148           1.492187        ...           \n",
       "1      -0.582225           -0.519125          -0.519151        ...           \n",
       "2      -0.976968           -0.704827          -0.704788        ...           \n",
       "3       2.812560           -0.401372          -0.401402        ...           \n",
       "4      -1.529607           -0.445832          -0.445880        ...           \n",
       "5       0.865164           -1.124725          -1.124753        ...           \n",
       "6      -1.319078            0.586521           0.586476        ...           \n",
       "7      -0.634858           -0.722067          -0.722111        ...           \n",
       "8      -1.108549            1.937149           1.937200        ...           \n",
       "9      -0.845387            0.702358           0.702353        ...           \n",
       "10      1.049377            3.462793           3.462793        ...           \n",
       "11      0.023047            0.261995           0.262022        ...           \n",
       "12     -0.634858           -0.910794          -0.910791        ...           \n",
       "13     -0.792755            0.969016           0.968986        ...           \n",
       "14      0.101995            0.034051           0.034014        ...           \n",
       "15      0.233576           -0.411252          -0.411234        ...           \n",
       "16     -1.187497            1.658394           1.658394        ...           \n",
       "17     -0.240115           -0.829537          -0.829560        ...           \n",
       "18      0.444105            0.273287           0.273258        ...           \n",
       "19     -1.529607            1.132136           1.132150        ...           \n",
       "20      1.049377           -0.457123          -0.457116        ...           \n",
       "21     -0.161167           -1.105570          -1.105557        ...           \n",
       "22     -1.266446           -1.036511          -1.036499        ...           \n",
       "23     -0.161167           -0.632240          -0.632219        ...           \n",
       "24      0.233576            0.079418           0.079428        ...           \n",
       "25      0.101995            0.153417           0.153402        ...           \n",
       "26     -0.371696           -0.647463          -0.647435        ...           \n",
       "27      0.707267            2.034134           2.034115        ...           \n",
       "28     -1.187497            1.114594           1.114593        ...           \n",
       "29      0.575686            0.815877           0.815889        ...           \n",
       "...          ...                 ...                ...        ...           \n",
       "3001   -0.161167           -1.074216          -1.074189        ...           \n",
       "3002   -0.503277           -0.387863          -0.387824        ...           \n",
       "3003   -0.503277           -0.476278          -0.476312        ...           \n",
       "3004   -0.503277            0.263205           0.263192        ...           \n",
       "3005   -0.292747           -0.015147          -0.015146        ...           \n",
       "3006   -1.055916           -0.816027          -0.815983        ...           \n",
       "3007    0.575686            0.858522           0.858494        ...           \n",
       "3008    0.365157           -0.141772          -0.141791        ...           \n",
       "3009   -0.450645            0.531274           0.531230        ...           \n",
       "3010    1.128326            1.569172           1.569204        ...           \n",
       "3011   -2.082247            0.000278           0.000304        ...           \n",
       "3012    1.259906           -0.486158          -0.486144        ...           \n",
       "3014   -0.503277           -0.041359          -0.041365        ...           \n",
       "3016    1.338855           -0.476077          -0.476078        ...           \n",
       "3017    0.312524            0.117627           0.117585        ...           \n",
       "3018   -1.187497           -0.135924          -0.135939        ...           \n",
       "3019   -1.319078           -0.870972          -0.870995        ...           \n",
       "3020   -0.108534            1.010653           1.010655        ...           \n",
       "3021    0.180944           -0.831855          -0.831901        ...           \n",
       "3022    1.180958            0.183359           0.183366        ...           \n",
       "3023    0.101995            0.173076           0.173066        ...           \n",
       "3024   -0.503277           -0.866738          -0.866781        ...           \n",
       "3025    1.259906            0.529560           0.529591        ...           \n",
       "3026   -0.713806           -0.725394          -0.725388        ...           \n",
       "3027    0.101995           -0.699888          -0.699872        ...           \n",
       "3028    0.575686            0.594385           0.594436        ...           \n",
       "3029   -0.924336            1.448899           1.448880        ...           \n",
       "3030    1.944127           -0.345218          -0.345219        ...           \n",
       "3031   -0.845387           -0.111829          -0.111827        ...           \n",
       "3032    0.707267           -0.811793          -0.811769        ...           \n",
       "\n",
       "      LBXMOPCT_0  LBDLYMNO_0  LBXLYPCT_0  LBXBAPCT_0  Basophils number  \\\n",
       "0       0.743010    0.247271   -0.815872    0.650790          1.177752   \n",
       "1       1.021949   -0.069312    1.515540    0.419546         -0.648990   \n",
       "2       0.882480    0.405562   -0.445237   -0.042941          1.177752   \n",
       "3       2.416645   -0.227604   -0.062646   -0.042941         -0.648990   \n",
       "4       0.928970   -1.652227    0.774271   -0.042941         -0.648990   \n",
       "5       1.393868    0.088979    1.025346    2.500739          1.177752   \n",
       "6       0.045662    0.247271   -0.289810    0.419546          1.177752   \n",
       "7      -0.279766   -1.177352   -0.349589    0.188303         -0.648990   \n",
       "8      -0.419236    0.405562    0.296032    0.419546          1.177752   \n",
       "9      -1.116584   -0.227604    0.080825   -0.042941         -0.648990   \n",
       "10     -1.581482   -0.702478   -1.066947    0.419546          1.177752   \n",
       "11     -1.023604   -1.177352   -1.557141   -0.736672         -0.648990   \n",
       "12     -1.488502    0.247271   -0.325677   -0.736672         -0.648990   \n",
       "13      0.092152   -1.019061   -1.401714    0.882034          1.177752   \n",
       "14     -1.116584   -0.385895   -0.122426   -0.274184         -0.648990   \n",
       "15      0.324602   -0.227604   -1.270198    0.188303          1.177752   \n",
       "16     -1.070094    0.405562   -0.839784   -0.042941          1.177752   \n",
       "17     -0.465726    0.880436    0.439504    1.575765          1.177752   \n",
       "18      0.650030    0.088979    1.013390    2.963226          1.177752   \n",
       "19      0.092152    2.305059    2.197030   -0.505428         -0.648990   \n",
       "20      0.835990   -0.227604    0.200385    0.419546         -0.648990   \n",
       "21     -0.140297    1.197019    0.463416    0.188303          1.177752   \n",
       "22      0.092152    0.563853    0.272121   -0.042941         -0.648990   \n",
       "23     -1.674462   -0.227604   -0.708268   -0.505428         -0.648990   \n",
       "24     -0.465726   -1.493935   -1.748437   -0.736672         -0.648990   \n",
       "25     -0.326256    0.563853   -0.182206   -0.505428         -0.648990   \n",
       "26     -0.744665    1.038728    1.945955   -1.199159         -0.648990   \n",
       "27     -0.372746   -0.069312   -0.899564   -0.042941          1.177752   \n",
       "28     -0.419236   -0.544187   -0.002867   -0.042941         -0.648990   \n",
       "29      0.743010   -0.702478   -1.389758   -1.199159         -0.648990   \n",
       "...          ...         ...         ...         ...               ...   \n",
       "3001    0.371091   -0.860769   -0.397413   -0.967915         -0.648990   \n",
       "3002    1.905256    1.988476    0.929698    0.419546          1.177752   \n",
       "3003    0.045662    0.722145    2.173118    1.113277          1.177752   \n",
       "3004   -0.279766   -0.069312    0.750359   -0.042941         -0.648990   \n",
       "3005   -0.326256   -0.702478    0.379724   -0.505428         -0.648990   \n",
       "3006    0.464071    0.247271    1.013390    0.419546         -0.648990   \n",
       "3007   -0.698175   -1.019061   -0.301766    0.419546         -0.648990   \n",
       "3008   -1.070094    2.305059    1.922043   -0.967915         -0.648990   \n",
       "3009    0.417581    0.722145    0.929698    0.650790          1.177752   \n",
       "3010   -0.326256    1.830185    0.690579    0.419546          1.177752   \n",
       "3011    0.045662   -0.227604    0.511240   -0.042941         -0.648990   \n",
       "3012    0.510561   -0.860769   -0.768048   -1.199159         -0.648990   \n",
       "3014    4.136769   -1.335644   -0.170250   10.594267          3.004494   \n",
       "3016    0.278112   -0.702478   -0.875652   -0.736672         -0.648990   \n",
       "3017    0.324602   -0.544187   -0.337633   -0.042941         -0.648990   \n",
       "3018   -0.140297    0.247271   -1.234330    0.419546          1.177752   \n",
       "3019   -0.419236   -0.385895   -0.074602   -0.042941         -0.648990   \n",
       "3020   -0.419236    0.880436    1.097082   -1.199159         -0.648990   \n",
       "3021   -0.326256    0.088979   -0.277854    0.882034          1.177752   \n",
       "3022    0.138642    0.088979   -1.401714   -0.736672         -0.648990   \n",
       "3023    1.114929   -0.702478    0.666667    2.038252          1.177752   \n",
       "3024    1.114929   -0.069312   -0.277854    4.813176          3.004494   \n",
       "3025    0.789500   -0.860769   -1.557141   -0.736672         -0.648990   \n",
       "3026   -0.651685   -0.702478   -1.808217   -0.505428         -0.648990   \n",
       "3027   -0.884134   -0.385895    0.068869   -0.042941         -0.648990   \n",
       "3028    1.626317    0.563853    1.276421   -0.505428         -0.648990   \n",
       "3029   -1.534992    1.355311   -0.361545   -0.042941          1.177752   \n",
       "3030    1.951746    1.513602    0.654711   -0.505428         -0.648990   \n",
       "3031    0.510561   -0.069312    0.571020    0.419546         -0.648990   \n",
       "3032    0.510561    0.880436    1.647056   -0.967915         -0.648990   \n",
       "\n",
       "      Eosinophils number  Segmented neutrophils number  LBDMONO_0  LBDBANO_0  \\\n",
       "0               0.612234                      0.949062   1.867854   1.177752   \n",
       "1               0.612234                     -1.322379  -0.344523  -0.648990   \n",
       "2               1.180220                      0.420820   1.867854   1.177752   \n",
       "3              -1.091723                     -0.371543   1.867854  -0.648990   \n",
       "4              -0.523737                     -1.639325  -1.450712  -0.648990   \n",
       "5               1.180220                     -1.111083   0.208571   1.177752   \n",
       "6              -0.523737                      0.315172   0.208571   1.177752   \n",
       "7               1.180220                     -0.741313  -0.897617  -0.648990   \n",
       "8               0.044249                     -0.160246  -0.344523   1.177752   \n",
       "9              -0.523737                     -0.318719  -1.450712  -0.648990   \n",
       "10             -0.523737                      0.684941  -0.897617   1.177752   \n",
       "11             -1.091723                      0.949062  -0.344523  -0.648990   \n",
       "12              0.044249                      0.526468  -0.897617  -0.648990   \n",
       "13              0.044249                      0.790590   0.761665   1.177752   \n",
       "14             -0.523737                     -0.213071  -1.450712  -0.648990   \n",
       "15              0.044249                      1.318832   1.867854   1.177752   \n",
       "16              1.180220                      1.318832   0.208571   1.177752   \n",
       "17              1.748206                     -0.107422   0.208571   1.177752   \n",
       "18             -0.523737                     -0.952610  -0.344523   1.177752   \n",
       "19              2.884177                     -1.058258   0.208571  -0.648990   \n",
       "20             -0.523737                     -0.530016   0.208571  -0.648990   \n",
       "21              0.044249                      0.156699   0.208571   1.177752   \n",
       "22              0.044249                     -0.054598   0.208571  -0.648990   \n",
       "23             -0.523737                      0.737765  -0.897617  -0.648990   \n",
       "24              0.612234                      0.526468  -0.344523  -0.648990   \n",
       "25              0.612234                      0.420820   0.208571  -0.648990   \n",
       "26             -0.523737                     -0.899786  -0.897617  -0.648990   \n",
       "27             -0.523737                      1.054711   0.761665   1.177752   \n",
       "28              0.044249                     -0.530016  -0.897617  -0.648990   \n",
       "29              0.612234                      0.949062   1.867854  -0.648990   \n",
       "...                  ...                           ...        ...        ...   \n",
       "3001           -0.523737                     -0.318719  -0.344523  -0.648990   \n",
       "3002            0.044249                     -0.107422   2.974042   1.177752   \n",
       "3003           -0.523737                     -1.163907  -0.897617   1.177752   \n",
       "3004            0.044249                     -0.741313  -0.897617  -0.648990   \n",
       "3005            0.612234                     -0.952610  -0.897617  -0.648990   \n",
       "3006           -0.523737                     -0.741313  -0.344523  -0.648990   \n",
       "3007           -0.523737                     -0.635665  -1.450712  -0.648990   \n",
       "3008           -0.523737                     -0.424368  -0.897617  -0.648990   \n",
       "3009           -0.523737                     -0.530016   0.208571   1.177752   \n",
       "3010            1.180220                      0.156699   0.761665   1.177752   \n",
       "3011           -0.523737                     -0.635665  -0.344523  -0.648990   \n",
       "3012           -1.091723                     -0.054598   0.208571  -0.648990   \n",
       "3014           -0.523737                     -1.269555   0.761665   3.004494   \n",
       "3016            0.044249                      0.209523   0.208571  -0.648990   \n",
       "3017           -0.523737                     -0.213071   0.208571  -0.648990   \n",
       "3018            0.044249                      1.952722   1.867854   1.177752   \n",
       "3019           -0.523737                     -0.265895  -0.344523  -0.648990   \n",
       "3020           -0.523737                     -0.477192  -0.344523  -0.648990   \n",
       "3021           -0.523737                      0.262347   0.208571   1.177752   \n",
       "3022           -0.523737                      2.164019   2.420948  -0.648990   \n",
       "3023            0.612234                     -1.163907  -0.344523   1.177752   \n",
       "3024           -0.523737                     -0.054598   1.314760   3.004494   \n",
       "3025            0.044249                      1.213183   1.867854  -0.648990   \n",
       "3026           -0.523737                      2.322492   0.761665  -0.648990   \n",
       "3027            0.612234                     -0.477192  -0.897617  -0.648990   \n",
       "3028           -0.523737                     -0.899786   0.761665  -0.648990   \n",
       "3029            0.044249                      1.424480  -0.344523   1.177752   \n",
       "3030           -0.523737                     -0.054598   2.420948  -0.648990   \n",
       "3031            0.612234                     -0.794137  -0.344523  -0.648990   \n",
       "3032            0.044249                     -0.952610  -0.344523  -0.648990   \n",
       "\n",
       "      Lymphocyte number  \n",
       "0              0.247271  \n",
       "1             -0.069312  \n",
       "2              0.405562  \n",
       "3             -0.227604  \n",
       "4             -1.652227  \n",
       "5              0.088979  \n",
       "6              0.247271  \n",
       "7             -1.177352  \n",
       "8              0.405562  \n",
       "9             -0.227604  \n",
       "10            -0.702478  \n",
       "11            -1.177352  \n",
       "12             0.247271  \n",
       "13            -1.019061  \n",
       "14            -0.385895  \n",
       "15            -0.227604  \n",
       "16             0.405562  \n",
       "17             0.880436  \n",
       "18             0.088979  \n",
       "19             2.305059  \n",
       "20            -0.227604  \n",
       "21             1.197019  \n",
       "22             0.563853  \n",
       "23            -0.227604  \n",
       "24            -1.493935  \n",
       "25             0.563853  \n",
       "26             1.038728  \n",
       "27            -0.069312  \n",
       "28            -0.544187  \n",
       "29            -0.702478  \n",
       "...                 ...  \n",
       "3001          -0.860769  \n",
       "3002           1.988476  \n",
       "3003           0.722145  \n",
       "3004          -0.069312  \n",
       "3005          -0.702478  \n",
       "3006           0.247271  \n",
       "3007          -1.019061  \n",
       "3008           2.305059  \n",
       "3009           0.722145  \n",
       "3010           1.830185  \n",
       "3011          -0.227604  \n",
       "3012          -0.860769  \n",
       "3014          -1.335644  \n",
       "3016          -0.702478  \n",
       "3017          -0.544187  \n",
       "3018           0.247271  \n",
       "3019          -0.385895  \n",
       "3020           0.880436  \n",
       "3021           0.088979  \n",
       "3022           0.088979  \n",
       "3023          -0.702478  \n",
       "3024          -0.069312  \n",
       "3025          -0.860769  \n",
       "3026          -0.702478  \n",
       "3027          -0.385895  \n",
       "3028           0.563853  \n",
       "3029           1.355311  \n",
       "3030           1.513602  \n",
       "3031          -0.069312  \n",
       "3032           0.880436  \n",
       "\n",
       "[2978 rows x 74 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = frame - frame.mean()\n",
    "norm = norm / norm.std()\n",
    "norm # normalized features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(True, False)', '(False, False)', '(False, True)', '(True, True)']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras.utils\n",
    "\n",
    "available = list(set(targets))\n",
    "target_matrix = keras.utils.to_categorical([available.index(x) for x in targets])\n",
    "target_matrix = target_matrix[filterNAs]\n",
    "\n",
    "print(available)\n",
    "target_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = 2500\n",
    "valid = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The baseline\n",
    "\n",
    "AS the baseline, we will create a basic network with one hidden layer with 1024 units and sigmoid activations (since the task is classification...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2500 samples, validate on 250 samples\n",
      "Epoch 1/100\n",
      "2500/2500 [==============================] - 1s 363us/step - loss: 1.1714 - acc: 0.5196 - val_loss: 1.0656 - val_acc: 0.5600\n",
      "Epoch 2/100\n",
      "2500/2500 [==============================] - 0s 113us/step - loss: 1.0933 - acc: 0.5244 - val_loss: 1.0203 - val_acc: 0.5520\n",
      "Epoch 3/100\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 1.0719 - acc: 0.5432 - val_loss: 1.0007 - val_acc: 0.5760\n",
      "Epoch 4/100\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 1.0635 - acc: 0.5392 - val_loss: 1.0039 - val_acc: 0.5440\n",
      "Epoch 5/100\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 1.0591 - acc: 0.5496 - val_loss: 0.9973 - val_acc: 0.5720\n",
      "Epoch 6/100\n",
      "2500/2500 [==============================] - 0s 113us/step - loss: 1.0539 - acc: 0.5532 - val_loss: 1.0176 - val_acc: 0.5360\n",
      "Epoch 7/100\n",
      "2500/2500 [==============================] - 0s 118us/step - loss: 1.0485 - acc: 0.5528 - val_loss: 1.0201 - val_acc: 0.5360\n",
      "Epoch 8/100\n",
      "2500/2500 [==============================] - 0s 115us/step - loss: 1.0460 - acc: 0.5592 - val_loss: 1.0087 - val_acc: 0.5440\n",
      "Epoch 9/100\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 1.0439 - acc: 0.5504 - val_loss: 1.0069 - val_acc: 0.5640\n",
      "Epoch 10/100\n",
      "2500/2500 [==============================] - 0s 116us/step - loss: 1.0428 - acc: 0.5496 - val_loss: 1.0053 - val_acc: 0.5320\n",
      "Epoch 11/100\n",
      "2500/2500 [==============================] - 0s 116us/step - loss: 1.0393 - acc: 0.5536 - val_loss: 0.9991 - val_acc: 0.5520\n",
      "Epoch 12/100\n",
      "2500/2500 [==============================] - 0s 113us/step - loss: 1.0381 - acc: 0.5568 - val_loss: 1.0127 - val_acc: 0.5880\n",
      "Epoch 13/100\n",
      "2500/2500 [==============================] - 0s 115us/step - loss: 1.0386 - acc: 0.5564 - val_loss: 1.0113 - val_acc: 0.5760\n",
      "Epoch 14/100\n",
      "2500/2500 [==============================] - 0s 109us/step - loss: 1.0353 - acc: 0.5608 - val_loss: 0.9911 - val_acc: 0.5480\n",
      "Epoch 15/100\n",
      "2500/2500 [==============================] - 0s 112us/step - loss: 1.0343 - acc: 0.5528 - val_loss: 1.0108 - val_acc: 0.5480\n",
      "Epoch 16/100\n",
      "2500/2500 [==============================] - 0s 115us/step - loss: 1.0312 - acc: 0.5548 - val_loss: 1.0081 - val_acc: 0.5600\n",
      "Epoch 17/100\n",
      "2500/2500 [==============================] - 0s 113us/step - loss: 1.0279 - acc: 0.5592 - val_loss: 0.9977 - val_acc: 0.5640\n",
      "Epoch 18/100\n",
      "2500/2500 [==============================] - 0s 115us/step - loss: 1.0266 - acc: 0.5592 - val_loss: 1.0247 - val_acc: 0.5360\n",
      "Epoch 19/100\n",
      "2500/2500 [==============================] - 0s 115us/step - loss: 1.0260 - acc: 0.5604 - val_loss: 1.0126 - val_acc: 0.5560\n",
      "Epoch 20/100\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 1.0240 - acc: 0.5640 - val_loss: 1.0016 - val_acc: 0.5560\n",
      "Epoch 21/100\n",
      "2500/2500 [==============================] - 0s 112us/step - loss: 1.0224 - acc: 0.5552 - val_loss: 1.0039 - val_acc: 0.5360\n",
      "Epoch 22/100\n",
      "2500/2500 [==============================] - 0s 111us/step - loss: 1.0224 - acc: 0.5596 - val_loss: 1.0114 - val_acc: 0.5360\n",
      "Epoch 23/100\n",
      "2500/2500 [==============================] - 0s 109us/step - loss: 1.0161 - acc: 0.5612 - val_loss: 1.0161 - val_acc: 0.5720\n",
      "Epoch 24/100\n",
      "2500/2500 [==============================] - 0s 116us/step - loss: 1.0187 - acc: 0.5624 - val_loss: 1.0101 - val_acc: 0.5480\n",
      "Epoch 25/100\n",
      "2500/2500 [==============================] - 0s 113us/step - loss: 1.0133 - acc: 0.5632 - val_loss: 1.0012 - val_acc: 0.5680\n",
      "Epoch 26/100\n",
      "2500/2500 [==============================] - 0s 111us/step - loss: 1.0134 - acc: 0.5712 - val_loss: 1.0037 - val_acc: 0.5640\n",
      "Epoch 27/100\n",
      "2500/2500 [==============================] - 0s 110us/step - loss: 1.0115 - acc: 0.5676 - val_loss: 1.0038 - val_acc: 0.5560\n",
      "Epoch 28/100\n",
      "2500/2500 [==============================] - 0s 109us/step - loss: 1.0050 - acc: 0.5692 - val_loss: 1.0278 - val_acc: 0.5400\n",
      "Epoch 29/100\n",
      "2500/2500 [==============================] - 0s 115us/step - loss: 1.0049 - acc: 0.5728 - val_loss: 1.0034 - val_acc: 0.5760\n",
      "Epoch 30/100\n",
      "2500/2500 [==============================] - 0s 115us/step - loss: 1.0034 - acc: 0.5736 - val_loss: 0.9969 - val_acc: 0.5560\n",
      "Epoch 31/100\n",
      "2500/2500 [==============================] - 0s 113us/step - loss: 1.0046 - acc: 0.5768 - val_loss: 1.0099 - val_acc: 0.5760\n",
      "Epoch 32/100\n",
      "2500/2500 [==============================] - 0s 112us/step - loss: 0.9994 - acc: 0.5776 - val_loss: 1.0267 - val_acc: 0.5560\n",
      "Epoch 33/100\n",
      "2500/2500 [==============================] - 0s 116us/step - loss: 0.9959 - acc: 0.5776 - val_loss: 1.0111 - val_acc: 0.5720\n",
      "Epoch 34/100\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 0.9963 - acc: 0.5724 - val_loss: 0.9981 - val_acc: 0.5720\n",
      "Epoch 35/100\n",
      "2500/2500 [==============================] - 0s 118us/step - loss: 0.9917 - acc: 0.5800 - val_loss: 1.0113 - val_acc: 0.5560\n",
      "Epoch 36/100\n",
      "2500/2500 [==============================] - 0s 116us/step - loss: 0.9907 - acc: 0.5788 - val_loss: 1.0047 - val_acc: 0.5640\n",
      "Epoch 37/100\n",
      "2500/2500 [==============================] - 0s 115us/step - loss: 0.9862 - acc: 0.5788 - val_loss: 0.9970 - val_acc: 0.5520\n",
      "Epoch 38/100\n",
      "2500/2500 [==============================] - 0s 116us/step - loss: 0.9826 - acc: 0.5816 - val_loss: 1.0086 - val_acc: 0.5640\n",
      "Epoch 39/100\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 0.9795 - acc: 0.5872 - val_loss: 1.0134 - val_acc: 0.5560\n",
      "Epoch 40/100\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 0.9776 - acc: 0.5952 - val_loss: 1.0019 - val_acc: 0.5840\n",
      "Epoch 41/100\n",
      "2500/2500 [==============================] - 0s 117us/step - loss: 0.9733 - acc: 0.5964 - val_loss: 1.0149 - val_acc: 0.5400\n",
      "Epoch 42/100\n",
      "2500/2500 [==============================] - 0s 115us/step - loss: 0.9727 - acc: 0.5896 - val_loss: 1.0093 - val_acc: 0.5360\n",
      "Epoch 43/100\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 0.9708 - acc: 0.6064 - val_loss: 1.0022 - val_acc: 0.5480\n",
      "Epoch 44/100\n",
      "2500/2500 [==============================] - 0s 110us/step - loss: 0.9638 - acc: 0.6024 - val_loss: 1.0062 - val_acc: 0.5760\n",
      "Epoch 45/100\n",
      "2500/2500 [==============================] - 0s 107us/step - loss: 0.9612 - acc: 0.5980 - val_loss: 1.0269 - val_acc: 0.5160\n",
      "Epoch 46/100\n",
      "2500/2500 [==============================] - 0s 113us/step - loss: 0.9586 - acc: 0.6052 - val_loss: 1.0147 - val_acc: 0.5360\n",
      "Epoch 47/100\n",
      "2500/2500 [==============================] - 0s 113us/step - loss: 0.9538 - acc: 0.6072 - val_loss: 1.0218 - val_acc: 0.5680\n",
      "Epoch 48/100\n",
      "2500/2500 [==============================] - 0s 115us/step - loss: 0.9512 - acc: 0.6004 - val_loss: 1.0142 - val_acc: 0.5480\n",
      "Epoch 49/100\n",
      "2500/2500 [==============================] - 0s 115us/step - loss: 0.9472 - acc: 0.6132 - val_loss: 1.0198 - val_acc: 0.5680\n",
      "Epoch 50/100\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 0.9441 - acc: 0.6116 - val_loss: 1.0120 - val_acc: 0.5680\n",
      "Epoch 51/100\n",
      "2500/2500 [==============================] - 0s 116us/step - loss: 0.9408 - acc: 0.6160 - val_loss: 1.0213 - val_acc: 0.5560\n",
      "Epoch 52/100\n",
      "2500/2500 [==============================] - 0s 113us/step - loss: 0.9341 - acc: 0.6184 - val_loss: 1.0254 - val_acc: 0.5720\n",
      "Epoch 53/100\n",
      "2500/2500 [==============================] - 0s 111us/step - loss: 0.9291 - acc: 0.6148 - val_loss: 1.0166 - val_acc: 0.5600\n",
      "Epoch 54/100\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 0.9262 - acc: 0.6212 - val_loss: 1.0211 - val_acc: 0.5560\n",
      "Epoch 55/100\n",
      "2500/2500 [==============================] - 0s 115us/step - loss: 0.9233 - acc: 0.6288 - val_loss: 1.0307 - val_acc: 0.5320\n",
      "Epoch 56/100\n",
      "2500/2500 [==============================] - 0s 115us/step - loss: 0.9166 - acc: 0.6360 - val_loss: 1.0265 - val_acc: 0.5560\n",
      "Epoch 57/100\n",
      "2500/2500 [==============================] - 0s 115us/step - loss: 0.9121 - acc: 0.6360 - val_loss: 1.0279 - val_acc: 0.5760\n",
      "Epoch 58/100\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 0.9073 - acc: 0.6396 - val_loss: 1.0379 - val_acc: 0.5480\n",
      "Epoch 59/100\n",
      "2500/2500 [==============================] - 0s 112us/step - loss: 0.9052 - acc: 0.6388 - val_loss: 1.0289 - val_acc: 0.5560\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 0s 117us/step - loss: 0.9002 - acc: 0.6352 - val_loss: 1.0350 - val_acc: 0.5480\n",
      "Epoch 61/100\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 0.8930 - acc: 0.6488 - val_loss: 1.0337 - val_acc: 0.5600\n",
      "Epoch 62/100\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 0.8848 - acc: 0.6572 - val_loss: 1.0273 - val_acc: 0.5600\n",
      "Epoch 63/100\n",
      "2500/2500 [==============================] - 0s 116us/step - loss: 0.8823 - acc: 0.6512 - val_loss: 1.0372 - val_acc: 0.5440\n",
      "Epoch 64/100\n",
      "2500/2500 [==============================] - 0s 116us/step - loss: 0.8773 - acc: 0.6612 - val_loss: 1.0290 - val_acc: 0.5440\n",
      "Epoch 65/100\n",
      "2500/2500 [==============================] - 0s 113us/step - loss: 0.8733 - acc: 0.6600 - val_loss: 1.0408 - val_acc: 0.5560\n",
      "Epoch 66/100\n",
      "2500/2500 [==============================] - 0s 113us/step - loss: 0.8665 - acc: 0.6608 - val_loss: 1.0482 - val_acc: 0.5560\n",
      "Epoch 67/100\n",
      "2500/2500 [==============================] - 0s 115us/step - loss: 0.8614 - acc: 0.6704 - val_loss: 1.0402 - val_acc: 0.5320\n",
      "Epoch 68/100\n",
      "2500/2500 [==============================] - 0s 116us/step - loss: 0.8572 - acc: 0.6700 - val_loss: 1.0491 - val_acc: 0.5240\n",
      "Epoch 69/100\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 0.8517 - acc: 0.6732 - val_loss: 1.0574 - val_acc: 0.5360\n",
      "Epoch 70/100\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 0.8445 - acc: 0.6852 - val_loss: 1.0484 - val_acc: 0.5360\n",
      "Epoch 71/100\n",
      "2500/2500 [==============================] - 0s 112us/step - loss: 0.8403 - acc: 0.6860 - val_loss: 1.0548 - val_acc: 0.5440\n",
      "Epoch 72/100\n",
      "2500/2500 [==============================] - 0s 112us/step - loss: 0.8368 - acc: 0.6824 - val_loss: 1.0457 - val_acc: 0.5440\n",
      "Epoch 73/100\n",
      "2500/2500 [==============================] - 0s 116us/step - loss: 0.8297 - acc: 0.6904 - val_loss: 1.0450 - val_acc: 0.5360\n",
      "Epoch 74/100\n",
      "2500/2500 [==============================] - 0s 116us/step - loss: 0.8249 - acc: 0.6952 - val_loss: 1.0576 - val_acc: 0.5600\n",
      "Epoch 75/100\n",
      "2500/2500 [==============================] - 0s 115us/step - loss: 0.8194 - acc: 0.6936 - val_loss: 1.0566 - val_acc: 0.5400\n",
      "Epoch 76/100\n",
      "2500/2500 [==============================] - 0s 106us/step - loss: 0.8112 - acc: 0.6968 - val_loss: 1.0741 - val_acc: 0.5280\n",
      "Epoch 77/100\n",
      "2500/2500 [==============================] - 0s 108us/step - loss: 0.8083 - acc: 0.7052 - val_loss: 1.0616 - val_acc: 0.5200\n",
      "Epoch 78/100\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 0.7995 - acc: 0.7104 - val_loss: 1.0619 - val_acc: 0.5360\n",
      "Epoch 79/100\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 0.7935 - acc: 0.7072 - val_loss: 1.0785 - val_acc: 0.5120\n",
      "Epoch 80/100\n",
      "2500/2500 [==============================] - 0s 113us/step - loss: 0.7864 - acc: 0.7140 - val_loss: 1.0601 - val_acc: 0.5360\n",
      "Epoch 81/100\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 0.7820 - acc: 0.7168 - val_loss: 1.0725 - val_acc: 0.5440\n",
      "Epoch 82/100\n",
      "2500/2500 [==============================] - 0s 117us/step - loss: 0.7743 - acc: 0.7208 - val_loss: 1.0646 - val_acc: 0.5360\n",
      "Epoch 83/100\n",
      "2500/2500 [==============================] - 0s 116us/step - loss: 0.7715 - acc: 0.7192 - val_loss: 1.0750 - val_acc: 0.5320\n",
      "Epoch 84/100\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 0.7633 - acc: 0.7288 - val_loss: 1.0832 - val_acc: 0.5240\n",
      "Epoch 85/100\n",
      "2500/2500 [==============================] - 0s 113us/step - loss: 0.7592 - acc: 0.7344 - val_loss: 1.0675 - val_acc: 0.5280\n",
      "Epoch 86/100\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 0.7507 - acc: 0.7236 - val_loss: 1.0883 - val_acc: 0.5200\n",
      "Epoch 87/100\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 0.7444 - acc: 0.7424 - val_loss: 1.0821 - val_acc: 0.5160\n",
      "Epoch 88/100\n",
      "2500/2500 [==============================] - 0s 115us/step - loss: 0.7379 - acc: 0.7504 - val_loss: 1.0969 - val_acc: 0.5360\n",
      "Epoch 89/100\n",
      "2500/2500 [==============================] - 0s 117us/step - loss: 0.7328 - acc: 0.7464 - val_loss: 1.0906 - val_acc: 0.5240\n",
      "Epoch 90/100\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 0.7263 - acc: 0.7532 - val_loss: 1.0932 - val_acc: 0.5240\n",
      "Epoch 91/100\n",
      "2500/2500 [==============================] - 0s 116us/step - loss: 0.7194 - acc: 0.7524 - val_loss: 1.0942 - val_acc: 0.5200\n",
      "Epoch 92/100\n",
      "2500/2500 [==============================] - 0s 115us/step - loss: 0.7146 - acc: 0.7596 - val_loss: 1.1012 - val_acc: 0.5200\n",
      "Epoch 93/100\n",
      "2500/2500 [==============================] - 0s 117us/step - loss: 0.7092 - acc: 0.7624 - val_loss: 1.0963 - val_acc: 0.5160\n",
      "Epoch 94/100\n",
      "2500/2500 [==============================] - 0s 115us/step - loss: 0.7032 - acc: 0.7612 - val_loss: 1.0971 - val_acc: 0.5320\n",
      "Epoch 95/100\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 0.6943 - acc: 0.7732 - val_loss: 1.1027 - val_acc: 0.5200\n",
      "Epoch 96/100\n",
      "2500/2500 [==============================] - 0s 104us/step - loss: 0.6854 - acc: 0.7740 - val_loss: 1.1095 - val_acc: 0.5160\n",
      "Epoch 97/100\n",
      "2500/2500 [==============================] - 0s 110us/step - loss: 0.6809 - acc: 0.7824 - val_loss: 1.1147 - val_acc: 0.5120\n",
      "Epoch 98/100\n",
      "2500/2500 [==============================] - 0s 113us/step - loss: 0.6742 - acc: 0.7792 - val_loss: 1.1215 - val_acc: 0.4920\n",
      "Epoch 99/100\n",
      "2500/2500 [==============================] - 0s 116us/step - loss: 0.6686 - acc: 0.7820 - val_loss: 1.1186 - val_acc: 0.5080\n",
      "Epoch 100/100\n",
      "2500/2500 [==============================] - 0s 107us/step - loss: 0.6634 - acc: 0.7896 - val_loss: 1.1211 - val_acc: 0.5320\n"
     ]
    }
   ],
   "source": [
    "baseline = keras.models.Sequential()\n",
    "baseline.add(keras.layers.Dense(128, input_shape=(74,), activation='sigmoid', kernel_regularizer=keras.regularizers.L1L2(l1=3e-5)))\n",
    "baseline.add(keras.layers.Dense(4, activation='softmax'))\n",
    "baseline.compile('adam', 'categorical_crossentropy', metrics=['acc'])\n",
    "hist = baseline.fit(norm[:train], target_matrix[:train],\n",
    "             epochs=100,\n",
    "             validation_data=(norm[train:train+valid], target_matrix[train:train+valid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7feff86e1908>]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.plot(hist.history['acc'])\n",
    "plt.plot(hist.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2500 samples, validate on 250 samples\n",
      "Epoch 1/35\n",
      "2500/2500 [==============================] - 1s 405us/step - loss: 1.1931 - acc: 0.4740 - val_loss: 1.0084 - val_acc: 0.5800\n",
      "Epoch 2/35\n",
      "2500/2500 [==============================] - 0s 108us/step - loss: 1.0838 - acc: 0.5292 - val_loss: 1.0042 - val_acc: 0.5440\n",
      "Epoch 3/35\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 1.0701 - acc: 0.5364 - val_loss: 0.9974 - val_acc: 0.5720\n",
      "Epoch 4/35\n",
      "2500/2500 [==============================] - 0s 115us/step - loss: 1.0621 - acc: 0.5444 - val_loss: 1.0172 - val_acc: 0.5440\n",
      "Epoch 5/35\n",
      "2500/2500 [==============================] - 0s 112us/step - loss: 1.0566 - acc: 0.5464 - val_loss: 1.0128 - val_acc: 0.5440\n",
      "Epoch 6/35\n",
      "2500/2500 [==============================] - 0s 107us/step - loss: 1.0509 - acc: 0.5432 - val_loss: 1.0300 - val_acc: 0.5520\n",
      "Epoch 7/35\n",
      "2500/2500 [==============================] - 0s 110us/step - loss: 1.0507 - acc: 0.5460 - val_loss: 1.0065 - val_acc: 0.5360\n",
      "Epoch 8/35\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 1.0464 - acc: 0.5472 - val_loss: 1.0165 - val_acc: 0.5440\n",
      "Epoch 9/35\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 1.0468 - acc: 0.5508 - val_loss: 1.0039 - val_acc: 0.5560\n",
      "Epoch 10/35\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 1.0430 - acc: 0.5508 - val_loss: 1.0141 - val_acc: 0.5800\n",
      "Epoch 11/35\n",
      "2500/2500 [==============================] - 0s 115us/step - loss: 1.0416 - acc: 0.5500 - val_loss: 1.0204 - val_acc: 0.5480\n",
      "Epoch 12/35\n",
      "2500/2500 [==============================] - 0s 113us/step - loss: 1.0412 - acc: 0.5596 - val_loss: 1.0202 - val_acc: 0.5560\n",
      "Epoch 13/35\n",
      "2500/2500 [==============================] - 0s 116us/step - loss: 1.0381 - acc: 0.5548 - val_loss: 1.0072 - val_acc: 0.5680\n",
      "Epoch 14/35\n",
      "2500/2500 [==============================] - 0s 115us/step - loss: 1.0346 - acc: 0.5560 - val_loss: 1.0093 - val_acc: 0.5520\n",
      "Epoch 15/35\n",
      "2500/2500 [==============================] - 0s 116us/step - loss: 1.0322 - acc: 0.5584 - val_loss: 1.0118 - val_acc: 0.5400\n",
      "Epoch 16/35\n",
      "2500/2500 [==============================] - 0s 117us/step - loss: 1.0310 - acc: 0.5580 - val_loss: 1.0177 - val_acc: 0.5440\n",
      "Epoch 17/35\n",
      "2500/2500 [==============================] - 0s 116us/step - loss: 1.0316 - acc: 0.5632 - val_loss: 1.0118 - val_acc: 0.5600\n",
      "Epoch 18/35\n",
      "2500/2500 [==============================] - 0s 116us/step - loss: 1.0263 - acc: 0.5672 - val_loss: 1.0164 - val_acc: 0.5520\n",
      "Epoch 19/35\n",
      "2500/2500 [==============================] - 0s 117us/step - loss: 1.0266 - acc: 0.5564 - val_loss: 1.0153 - val_acc: 0.5480\n",
      "Epoch 20/35\n",
      "2500/2500 [==============================] - 0s 115us/step - loss: 1.0223 - acc: 0.5616 - val_loss: 1.0094 - val_acc: 0.5440\n",
      "Epoch 21/35\n",
      "2500/2500 [==============================] - 0s 116us/step - loss: 1.0203 - acc: 0.5632 - val_loss: 1.0300 - val_acc: 0.5520\n",
      "Epoch 22/35\n",
      "2500/2500 [==============================] - 0s 116us/step - loss: 1.0191 - acc: 0.5584 - val_loss: 1.0145 - val_acc: 0.5560\n",
      "Epoch 23/35\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 1.0200 - acc: 0.5704 - val_loss: 1.0253 - val_acc: 0.5400\n",
      "Epoch 24/35\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 1.0166 - acc: 0.5684 - val_loss: 1.0303 - val_acc: 0.5360\n",
      "Epoch 25/35\n",
      "2500/2500 [==============================] - 0s 115us/step - loss: 1.0132 - acc: 0.5676 - val_loss: 1.0230 - val_acc: 0.5560\n",
      "Epoch 26/35\n",
      "2500/2500 [==============================] - 0s 119us/step - loss: 1.0143 - acc: 0.5584 - val_loss: 1.0203 - val_acc: 0.5520\n",
      "Epoch 27/35\n",
      "2500/2500 [==============================] - 0s 115us/step - loss: 1.0091 - acc: 0.5724 - val_loss: 1.0081 - val_acc: 0.5520\n",
      "Epoch 28/35\n",
      "2500/2500 [==============================] - 0s 115us/step - loss: 1.0064 - acc: 0.5748 - val_loss: 1.0266 - val_acc: 0.5560\n",
      "Epoch 29/35\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 1.0035 - acc: 0.5752 - val_loss: 1.0183 - val_acc: 0.5560\n",
      "Epoch 30/35\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 1.0036 - acc: 0.5720 - val_loss: 1.0218 - val_acc: 0.5440\n",
      "Epoch 31/35\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 1.0033 - acc: 0.5684 - val_loss: 1.0133 - val_acc: 0.5440\n",
      "Epoch 32/35\n",
      "2500/2500 [==============================] - 0s 115us/step - loss: 0.9983 - acc: 0.5780 - val_loss: 1.0087 - val_acc: 0.5400\n",
      "Epoch 33/35\n",
      "2500/2500 [==============================] - 0s 115us/step - loss: 0.9940 - acc: 0.5812 - val_loss: 1.0327 - val_acc: 0.5520\n",
      "Epoch 34/35\n",
      "2500/2500 [==============================] - 0s 116us/step - loss: 0.9935 - acc: 0.5744 - val_loss: 1.0207 - val_acc: 0.5720\n",
      "Epoch 35/35\n",
      "2500/2500 [==============================] - 0s 114us/step - loss: 0.9915 - acc: 0.5812 - val_loss: 1.0130 - val_acc: 0.5440\n"
     ]
    }
   ],
   "source": [
    "# retrain with 35 epochs\n",
    "\n",
    "baseline = keras.models.Sequential()\n",
    "baseline.add(keras.layers.Dense(128, input_shape=(74,), activation='sigmoid', kernel_regularizer=keras.regularizers.L1L2(l1=3e-5)))\n",
    "baseline.add(keras.layers.Dense(4, activation='softmax'))\n",
    "baseline.compile('adam', 'categorical_crossentropy', metrics=['acc'])\n",
    "hist = baseline.fit(norm[:train], target_matrix[:train],\n",
    "             epochs=35,\n",
    "             validation_data=(norm[train:train+valid], target_matrix[train:train+valid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_50 (Dense)             (None, 128)               9600      \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 10,116\n",
      "Trainable params: 10,116\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "baseline.summary() # get number of weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My model\n",
    "\n",
    "The feature discovery layer in the first stage will replace Dense in such a way as to retain total number of parameters similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "feature_selection_layer_9 (F (None, 74)                370       \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 123)               9225      \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 4)                 496       \n",
      "=================================================================\n",
      "Total params: 10,091\n",
      "Trainable params: 10,091\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mine = keras.models.Sequential()\n",
    "mine.add(FeatureSelectionLayer(input_shape=(74,)))\n",
    "mine.add(keras.layers.Dense(123, activation='sigmoid', kernel_regularizer=keras.regularizers.L1L2(l1=3e-5)))\n",
    "mine.add(keras.layers.Dense(4, activation='softmax'))\n",
    "mine.compile('adam', 'categorical_crossentropy', metrics=['acc'])\n",
    "mine.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2500 samples, validate on 250 samples\n",
      "Epoch 1/100\n",
      "2500/2500 [==============================] - 1s 464us/step - loss: 1.2254 - acc: 0.4772 - val_loss: 1.0532 - val_acc: 0.5560\n",
      "Epoch 2/100\n",
      "2500/2500 [==============================] - 0s 130us/step - loss: 1.1146 - acc: 0.5260 - val_loss: 1.0438 - val_acc: 0.5560\n",
      "Epoch 3/100\n",
      "2500/2500 [==============================] - 0s 132us/step - loss: 1.0775 - acc: 0.5368 - val_loss: 1.0773 - val_acc: 0.5160\n",
      "Epoch 4/100\n",
      "2500/2500 [==============================] - 0s 132us/step - loss: 1.0681 - acc: 0.5448 - val_loss: 1.0246 - val_acc: 0.5320\n",
      "Epoch 5/100\n",
      "2500/2500 [==============================] - 0s 131us/step - loss: 1.0587 - acc: 0.5532 - val_loss: 1.0251 - val_acc: 0.5640\n",
      "Epoch 6/100\n",
      "2500/2500 [==============================] - 0s 129us/step - loss: 1.0485 - acc: 0.5532 - val_loss: 1.0297 - val_acc: 0.5320\n",
      "Epoch 7/100\n",
      "2500/2500 [==============================] - 0s 132us/step - loss: 1.0400 - acc: 0.5656 - val_loss: 1.0275 - val_acc: 0.5360\n",
      "Epoch 8/100\n",
      "2500/2500 [==============================] - 0s 132us/step - loss: 1.0322 - acc: 0.5660 - val_loss: 1.0443 - val_acc: 0.5360\n",
      "Epoch 9/100\n",
      "2500/2500 [==============================] - 0s 129us/step - loss: 1.0286 - acc: 0.5600 - val_loss: 1.0299 - val_acc: 0.5240\n",
      "Epoch 10/100\n",
      "2500/2500 [==============================] - 0s 127us/step - loss: 1.0249 - acc: 0.5640 - val_loss: 1.0398 - val_acc: 0.5240\n",
      "Epoch 11/100\n",
      "2500/2500 [==============================] - 0s 132us/step - loss: 1.0193 - acc: 0.5684 - val_loss: 1.0329 - val_acc: 0.5240\n",
      "Epoch 12/100\n",
      "2500/2500 [==============================] - 0s 135us/step - loss: 1.0139 - acc: 0.5664 - val_loss: 1.0395 - val_acc: 0.5280\n",
      "Epoch 13/100\n",
      "2500/2500 [==============================] - 0s 133us/step - loss: 1.0095 - acc: 0.5728 - val_loss: 1.0286 - val_acc: 0.5160\n",
      "Epoch 14/100\n",
      "2500/2500 [==============================] - 0s 135us/step - loss: 1.0088 - acc: 0.5720 - val_loss: 1.0324 - val_acc: 0.5400\n",
      "Epoch 15/100\n",
      "2500/2500 [==============================] - 0s 132us/step - loss: 0.9980 - acc: 0.5764 - val_loss: 1.0241 - val_acc: 0.5480\n",
      "Epoch 16/100\n",
      "2500/2500 [==============================] - 0s 131us/step - loss: 0.9961 - acc: 0.5796 - val_loss: 1.0369 - val_acc: 0.5400\n",
      "Epoch 17/100\n",
      "2500/2500 [==============================] - 0s 127us/step - loss: 0.9952 - acc: 0.5764 - val_loss: 1.0495 - val_acc: 0.5280\n",
      "Epoch 18/100\n",
      "2500/2500 [==============================] - 0s 131us/step - loss: 0.9905 - acc: 0.5780 - val_loss: 1.0431 - val_acc: 0.5280\n",
      "Epoch 19/100\n",
      "2500/2500 [==============================] - 0s 130us/step - loss: 0.9859 - acc: 0.5816 - val_loss: 1.0688 - val_acc: 0.5160\n",
      "Epoch 20/100\n",
      "2500/2500 [==============================] - 0s 133us/step - loss: 0.9793 - acc: 0.5872 - val_loss: 1.0479 - val_acc: 0.5320\n",
      "Epoch 21/100\n",
      "2500/2500 [==============================] - 0s 133us/step - loss: 0.9787 - acc: 0.5832 - val_loss: 1.0474 - val_acc: 0.5360\n",
      "Epoch 22/100\n",
      "2500/2500 [==============================] - 0s 133us/step - loss: 0.9740 - acc: 0.5888 - val_loss: 1.0401 - val_acc: 0.5440\n",
      "Epoch 23/100\n",
      "2500/2500 [==============================] - 0s 134us/step - loss: 0.9684 - acc: 0.5900 - val_loss: 1.0614 - val_acc: 0.5280\n",
      "Epoch 24/100\n",
      "2500/2500 [==============================] - 0s 133us/step - loss: 0.9671 - acc: 0.5928 - val_loss: 1.0529 - val_acc: 0.5280\n",
      "Epoch 25/100\n",
      "2500/2500 [==============================] - 0s 131us/step - loss: 0.9638 - acc: 0.5960 - val_loss: 1.0647 - val_acc: 0.5320\n",
      "Epoch 26/100\n",
      "2500/2500 [==============================] - 0s 132us/step - loss: 0.9559 - acc: 0.6040 - val_loss: 1.0377 - val_acc: 0.5440\n",
      "Epoch 27/100\n",
      "2500/2500 [==============================] - 0s 133us/step - loss: 0.9528 - acc: 0.6032 - val_loss: 1.0560 - val_acc: 0.5280\n",
      "Epoch 28/100\n",
      "2500/2500 [==============================] - 0s 131us/step - loss: 0.9476 - acc: 0.6056 - val_loss: 1.0757 - val_acc: 0.5080\n",
      "Epoch 29/100\n",
      "2500/2500 [==============================] - 0s 125us/step - loss: 0.9439 - acc: 0.6140 - val_loss: 1.0410 - val_acc: 0.5560\n",
      "Epoch 30/100\n",
      "2500/2500 [==============================] - 0s 136us/step - loss: 0.9459 - acc: 0.5988 - val_loss: 1.0731 - val_acc: 0.5120\n",
      "Epoch 31/100\n",
      "2500/2500 [==============================] - 0s 127us/step - loss: 0.9353 - acc: 0.6080 - val_loss: 1.0536 - val_acc: 0.5560\n",
      "Epoch 32/100\n",
      "2500/2500 [==============================] - 0s 132us/step - loss: 0.9307 - acc: 0.6104 - val_loss: 1.0749 - val_acc: 0.5000\n",
      "Epoch 33/100\n",
      "2500/2500 [==============================] - 0s 131us/step - loss: 0.9273 - acc: 0.6120 - val_loss: 1.0628 - val_acc: 0.5240\n",
      "Epoch 34/100\n",
      "2500/2500 [==============================] - 0s 130us/step - loss: 0.9225 - acc: 0.6192 - val_loss: 1.0545 - val_acc: 0.5280\n",
      "Epoch 35/100\n",
      "2500/2500 [==============================] - 0s 132us/step - loss: 0.9159 - acc: 0.6184 - val_loss: 1.0527 - val_acc: 0.5280\n",
      "Epoch 36/100\n",
      "2500/2500 [==============================] - 0s 133us/step - loss: 0.9102 - acc: 0.6208 - val_loss: 1.0643 - val_acc: 0.5440\n",
      "Epoch 37/100\n",
      "2500/2500 [==============================] - 0s 132us/step - loss: 0.9081 - acc: 0.6252 - val_loss: 1.0634 - val_acc: 0.5080\n",
      "Epoch 38/100\n",
      "2500/2500 [==============================] - 0s 132us/step - loss: 0.9036 - acc: 0.6232 - val_loss: 1.0587 - val_acc: 0.5320\n",
      "Epoch 39/100\n",
      "2500/2500 [==============================] - 0s 133us/step - loss: 0.8943 - acc: 0.6344 - val_loss: 1.0748 - val_acc: 0.5160\n",
      "Epoch 40/100\n",
      "2500/2500 [==============================] - 0s 132us/step - loss: 0.8906 - acc: 0.6268 - val_loss: 1.0695 - val_acc: 0.5520\n",
      "Epoch 41/100\n",
      "2500/2500 [==============================] - 0s 133us/step - loss: 0.8839 - acc: 0.6396 - val_loss: 1.0606 - val_acc: 0.5360\n",
      "Epoch 42/100\n",
      "2500/2500 [==============================] - 0s 131us/step - loss: 0.8795 - acc: 0.6364 - val_loss: 1.0740 - val_acc: 0.5440\n",
      "Epoch 43/100\n",
      "2500/2500 [==============================] - 0s 133us/step - loss: 0.8721 - acc: 0.6464 - val_loss: 1.0794 - val_acc: 0.5240\n",
      "Epoch 44/100\n",
      "2500/2500 [==============================] - 0s 131us/step - loss: 0.8672 - acc: 0.6512 - val_loss: 1.0620 - val_acc: 0.5440\n",
      "Epoch 45/100\n",
      "2500/2500 [==============================] - 0s 132us/step - loss: 0.8570 - acc: 0.6520 - val_loss: 1.0885 - val_acc: 0.5240\n",
      "Epoch 46/100\n",
      "2500/2500 [==============================] - 0s 130us/step - loss: 0.8534 - acc: 0.6560 - val_loss: 1.0766 - val_acc: 0.5320\n",
      "Epoch 47/100\n",
      "2500/2500 [==============================] - 0s 131us/step - loss: 0.8419 - acc: 0.6656 - val_loss: 1.0749 - val_acc: 0.5120\n",
      "Epoch 48/100\n",
      "2500/2500 [==============================] - 0s 130us/step - loss: 0.8371 - acc: 0.6640 - val_loss: 1.0880 - val_acc: 0.5280\n",
      "Epoch 49/100\n",
      "2500/2500 [==============================] - 0s 133us/step - loss: 0.8306 - acc: 0.6748 - val_loss: 1.0787 - val_acc: 0.5240\n",
      "Epoch 50/100\n",
      "2500/2500 [==============================] - 0s 132us/step - loss: 0.8238 - acc: 0.6756 - val_loss: 1.0807 - val_acc: 0.5200\n",
      "Epoch 51/100\n",
      "2500/2500 [==============================] - 0s 130us/step - loss: 0.8138 - acc: 0.6888 - val_loss: 1.0856 - val_acc: 0.5280\n",
      "Epoch 52/100\n",
      "2500/2500 [==============================] - 0s 131us/step - loss: 0.8040 - acc: 0.6908 - val_loss: 1.0966 - val_acc: 0.4840\n",
      "Epoch 53/100\n",
      "2500/2500 [==============================] - 0s 130us/step - loss: 0.7941 - acc: 0.7012 - val_loss: 1.1077 - val_acc: 0.4800\n",
      "Epoch 54/100\n",
      "2500/2500 [==============================] - 0s 133us/step - loss: 0.7888 - acc: 0.7044 - val_loss: 1.1011 - val_acc: 0.5080\n",
      "Epoch 55/100\n",
      "2500/2500 [==============================] - 0s 134us/step - loss: 0.7770 - acc: 0.7044 - val_loss: 1.0931 - val_acc: 0.5200\n",
      "Epoch 56/100\n",
      "2500/2500 [==============================] - 0s 131us/step - loss: 0.7702 - acc: 0.7184 - val_loss: 1.1013 - val_acc: 0.5520\n",
      "Epoch 57/100\n",
      "2500/2500 [==============================] - 0s 132us/step - loss: 0.7588 - acc: 0.7204 - val_loss: 1.1141 - val_acc: 0.5040\n",
      "Epoch 58/100\n",
      "2500/2500 [==============================] - 0s 131us/step - loss: 0.7480 - acc: 0.7324 - val_loss: 1.1085 - val_acc: 0.5040\n",
      "Epoch 59/100\n",
      "2500/2500 [==============================] - 0s 131us/step - loss: 0.7418 - acc: 0.7368 - val_loss: 1.1175 - val_acc: 0.4960\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 0s 133us/step - loss: 0.7280 - acc: 0.7436 - val_loss: 1.1046 - val_acc: 0.5120\n",
      "Epoch 61/100\n",
      "2500/2500 [==============================] - 0s 130us/step - loss: 0.7201 - acc: 0.7408 - val_loss: 1.1267 - val_acc: 0.4960\n",
      "Epoch 62/100\n",
      "2500/2500 [==============================] - 0s 133us/step - loss: 0.7096 - acc: 0.7536 - val_loss: 1.1249 - val_acc: 0.5040\n",
      "Epoch 63/100\n",
      "2500/2500 [==============================] - 0s 132us/step - loss: 0.6969 - acc: 0.7648 - val_loss: 1.1145 - val_acc: 0.5160\n",
      "Epoch 64/100\n",
      "2500/2500 [==============================] - 0s 126us/step - loss: 0.6913 - acc: 0.7644 - val_loss: 1.1290 - val_acc: 0.5000\n",
      "Epoch 65/100\n",
      "2500/2500 [==============================] - 0s 128us/step - loss: 0.6768 - acc: 0.7720 - val_loss: 1.1481 - val_acc: 0.4920\n",
      "Epoch 66/100\n",
      "2500/2500 [==============================] - 0s 132us/step - loss: 0.6664 - acc: 0.7872 - val_loss: 1.1277 - val_acc: 0.4880\n",
      "Epoch 67/100\n",
      "2500/2500 [==============================] - 0s 128us/step - loss: 0.6543 - acc: 0.7828 - val_loss: 1.1523 - val_acc: 0.5040\n",
      "Epoch 68/100\n",
      "2500/2500 [==============================] - 0s 131us/step - loss: 0.6440 - acc: 0.7952 - val_loss: 1.1454 - val_acc: 0.5080\n",
      "Epoch 69/100\n",
      "2500/2500 [==============================] - 0s 133us/step - loss: 0.6314 - acc: 0.7980 - val_loss: 1.1441 - val_acc: 0.4840\n",
      "Epoch 70/100\n",
      "2500/2500 [==============================] - 0s 133us/step - loss: 0.6178 - acc: 0.8052 - val_loss: 1.1434 - val_acc: 0.4920\n",
      "Epoch 71/100\n",
      "2500/2500 [==============================] - 0s 131us/step - loss: 0.6058 - acc: 0.8176 - val_loss: 1.1718 - val_acc: 0.4840\n",
      "Epoch 72/100\n",
      "2500/2500 [==============================] - 0s 133us/step - loss: 0.5958 - acc: 0.8172 - val_loss: 1.1626 - val_acc: 0.4760\n",
      "Epoch 73/100\n",
      "2500/2500 [==============================] - 0s 127us/step - loss: 0.5846 - acc: 0.8260 - val_loss: 1.1751 - val_acc: 0.4840\n",
      "Epoch 74/100\n",
      "2500/2500 [==============================] - 0s 129us/step - loss: 0.5745 - acc: 0.8300 - val_loss: 1.1810 - val_acc: 0.4840\n",
      "Epoch 75/100\n",
      "2500/2500 [==============================] - 0s 127us/step - loss: 0.5598 - acc: 0.8372 - val_loss: 1.1947 - val_acc: 0.4800\n",
      "Epoch 76/100\n",
      "2500/2500 [==============================] - 0s 130us/step - loss: 0.5487 - acc: 0.8440 - val_loss: 1.1985 - val_acc: 0.4720\n",
      "Epoch 77/100\n",
      "2500/2500 [==============================] - 0s 133us/step - loss: 0.5360 - acc: 0.8520 - val_loss: 1.2050 - val_acc: 0.4800\n",
      "Epoch 78/100\n",
      "2500/2500 [==============================] - 0s 132us/step - loss: 0.5259 - acc: 0.8480 - val_loss: 1.2143 - val_acc: 0.4720\n",
      "Epoch 79/100\n",
      "2500/2500 [==============================] - 0s 129us/step - loss: 0.5136 - acc: 0.8564 - val_loss: 1.2298 - val_acc: 0.4480\n",
      "Epoch 80/100\n",
      "2500/2500 [==============================] - 0s 127us/step - loss: 0.5009 - acc: 0.8664 - val_loss: 1.2441 - val_acc: 0.4760\n",
      "Epoch 81/100\n",
      "2500/2500 [==============================] - 0s 122us/step - loss: 0.4911 - acc: 0.8752 - val_loss: 1.2221 - val_acc: 0.4840\n",
      "Epoch 82/100\n",
      "2500/2500 [==============================] - 0s 132us/step - loss: 0.4769 - acc: 0.8788 - val_loss: 1.2576 - val_acc: 0.4480\n",
      "Epoch 83/100\n",
      "2500/2500 [==============================] - 0s 125us/step - loss: 0.4654 - acc: 0.8828 - val_loss: 1.2532 - val_acc: 0.4560\n",
      "Epoch 84/100\n",
      "2500/2500 [==============================] - 0s 130us/step - loss: 0.4544 - acc: 0.8928 - val_loss: 1.2489 - val_acc: 0.4840\n",
      "Epoch 85/100\n",
      "2500/2500 [==============================] - 0s 132us/step - loss: 0.4447 - acc: 0.8960 - val_loss: 1.2605 - val_acc: 0.4760\n",
      "Epoch 86/100\n",
      "2500/2500 [==============================] - 0s 133us/step - loss: 0.4294 - acc: 0.9020 - val_loss: 1.2988 - val_acc: 0.4640\n",
      "Epoch 87/100\n",
      "2500/2500 [==============================] - 0s 135us/step - loss: 0.4197 - acc: 0.9116 - val_loss: 1.2945 - val_acc: 0.4680\n",
      "Epoch 88/100\n",
      "2500/2500 [==============================] - 0s 134us/step - loss: 0.4108 - acc: 0.9140 - val_loss: 1.2851 - val_acc: 0.4800\n",
      "Epoch 89/100\n",
      "2500/2500 [==============================] - 0s 130us/step - loss: 0.3947 - acc: 0.9180 - val_loss: 1.2881 - val_acc: 0.4640\n",
      "Epoch 90/100\n",
      "2500/2500 [==============================] - 0s 128us/step - loss: 0.3867 - acc: 0.9208 - val_loss: 1.3049 - val_acc: 0.4680\n",
      "Epoch 91/100\n",
      "2500/2500 [==============================] - 0s 131us/step - loss: 0.3737 - acc: 0.9352 - val_loss: 1.3290 - val_acc: 0.4720\n",
      "Epoch 92/100\n",
      "2500/2500 [==============================] - 0s 130us/step - loss: 0.3621 - acc: 0.9336 - val_loss: 1.3406 - val_acc: 0.4600\n",
      "Epoch 93/100\n",
      "2500/2500 [==============================] - 0s 133us/step - loss: 0.3532 - acc: 0.9392 - val_loss: 1.3368 - val_acc: 0.4680\n",
      "Epoch 94/100\n",
      "2500/2500 [==============================] - 0s 131us/step - loss: 0.3438 - acc: 0.9440 - val_loss: 1.3452 - val_acc: 0.4520\n",
      "Epoch 95/100\n",
      "2500/2500 [==============================] - 0s 128us/step - loss: 0.3318 - acc: 0.9472 - val_loss: 1.3669 - val_acc: 0.4760\n",
      "Epoch 96/100\n",
      "2500/2500 [==============================] - 0s 126us/step - loss: 0.3233 - acc: 0.9484 - val_loss: 1.3858 - val_acc: 0.4600\n",
      "Epoch 97/100\n",
      "2500/2500 [==============================] - 0s 124us/step - loss: 0.3120 - acc: 0.9552 - val_loss: 1.3933 - val_acc: 0.4720\n",
      "Epoch 98/100\n",
      "2500/2500 [==============================] - 0s 131us/step - loss: 0.3050 - acc: 0.9536 - val_loss: 1.3996 - val_acc: 0.4880\n",
      "Epoch 99/100\n",
      "2500/2500 [==============================] - 0s 132us/step - loss: 0.2942 - acc: 0.9596 - val_loss: 1.4077 - val_acc: 0.4720\n",
      "Epoch 100/100\n",
      "2500/2500 [==============================] - 0s 130us/step - loss: 0.2857 - acc: 0.9660 - val_loss: 1.4143 - val_acc: 0.4760\n"
     ]
    }
   ],
   "source": [
    "hist = mine.fit(norm[:train], target_matrix[:train],\n",
    "             epochs=100,\n",
    "             validation_data=(norm[train:train+valid], target_matrix[train:train+valid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fefd3608588>]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvFvnyVgAAIABJREFUeJzs3Xd0VNX2wPHvTW+kN9KBQOiE0Ks0AQVEQVFEFCzYsPssP1Gwo75nf4rIQ8WCqFQBaYqA9F4SSO+9TjLJTKad3x8XA0hJgIRJOZ+1shYz9869ewbYc7LvPucqQggkSZKk5sXG2gFIkiRJ9U8md0mSpGZIJndJkqRmSCZ3SZKkZkgmd0mSpGZIJndJkqRmSCZ3SZKkZkgmd0mSpGZIJndJkqRmyM5aJ/b19RURERHWOr0kSVKTdPDgwSIhhF9t+1ktuUdERHDgwAFrnV6SJKlJUhQlvS77ybKMJElSMySTuyRJUjMkk7skSVIzJJO7JElSMySTuyRJUjMkk7skSVIzJJO7JElSM1RrclcUZbGiKAWKopyoZb8+iqKYFEW5tf7CkyRJakYq8mHnx5C6o8FPVZeR+9fA2EvtoCiKLfAOsKkeYpIkSWo+LGaIWwPfT4H3O8HmlyH59wY/ba0zVIUQ2xVFiahlt8eA5UCfeohJkiSp6TPq4egP6ki9NBVaBcGgJyD6TvBt3+Cnv+rlBxRFCQZuAYYjk7skSS2ZEJBzCI79DMd/hqoiCIqB61+FjuPBxvaahVIfa8t8CDwvhLAoinLJHRVFmQXMAggLC6uHU0uSJDUCujI48gMcWAzFiWDrAB3GQN9ZEDEEasmNDaE+kntv4MfTid0XuFFRFJMQYtU/dxRCLAQWAvTu3VvUw7klSZKsR18Ov78KR5aCsRJC+sKEj6HzTeDsZdXQrjq5CyHa/P1nRVG+BtZeKLFLkiQ1K0YdLL0DMvdC9zug7wMQFG3tqGrUmtwVRVkKDAN8FUXJAuYC9gBCiAUNGp0kSVJjZDbCzzMgfRdMXgTdGl8HeF26ZabW9WBCiBlXFY0kSVJjZ7HAqkcgYQOMe79RJnaQM1QlSZIuz9Y34PhPMPIV6HOftaO5KJncJUmS6ip2Jez4D8TcA4OftnY0lySTuyRJUl3knVDLMaH94Mb3rNLeeDlkcpckSaqNJgt+vBOcPGDKErBztHZEtbLaDbIlSZIanaTf1Q6Y1j0gpDeY9LDzI3WCkmIDM9ZDq0BrR1knMrlLkiRVlcDG/4OjS8/fZusAPe9S14XxirjmoV0pmdwlSWrZUrfD8vuhsgiG/gsGPg5FCZC1H6q1amJ3b23tKC+bTO6SJLVcZZmw7C5wC4RpP6vlGFBLMiG9rRvbVZLJXZKklslsghUPqJOS7vwRvNtaO6J6JZO7JEkt0/b3IGM3TPqy2SV2kK2QkiS1RGk7Yfu70ONO6D7F2tE0CJncJUlq+ooSoTynbvtmHYAfp4JXG7jx3YaNy4pkWUaSpKatLBM+GwAWo5qwwwdBjzugzZDz903bCT9MAVc/uHsVOLa69vFeI3LkLklS07bncxAWdSGvgC5wai18Mx6+nQS5R9XleQtOwcFv4LvJ4B4MM38Dz+Z9Nzg5cpckqenSlcLBr9Vld4c8oz5n1MP+L9UFvr4YCjb26qgeILA7TF8Jrr5WC/lakcldkqSm68Bi9fZ2Ax8785y9k/o45m7Y9yVUV4B/p9M/ncHW3nrxXkMyuUuS1DQZ9bD3C2g3AgK7nb/dyQOGPnvt42okZM1dkqSm6dgy0Oara75I55HJXZKkxsdivvR2vQZ2fazW0Ntcd21iamJkWUaSpMYlaQt8P0WtkUcMgYjBED4QXLzV7Qkb4dcnQZsHd/zQ6G+aYS0yuUuS1HgIAb+/Dm7+4OIDB7+CvZ+r2/w6qWupp2xV/3zHdxDcy7rxNmIyuUuSZB1Jv0PiZhg1T+1wAfVx7hG46RO128VUDdkH1RtopO+CwlPqsrxD/9Uk7oZkTTK5S5J0bQmh1su3zFMnHxm0ajIH2DYfPMKgx1T1sZ2jWpIJH2i1cJsqmdwlSbp2jDpY8zgc/wk63wyeobDrEwiOUWeMZh+E8R+2mF70htTkkntaUSVbTuYztW8Yro5NLnxJarkqi2Hp7eodjkbMgSHPqiP3glOw/jk1uXuEQvQ0a0faLDS5VshTeeW8se4kyYVaa4ciSVJdlaTC/66HvOMwZYlaM1cUsLGFyV+CRwiUJMPgp8DOwdrRNgu1JndFURYrilKgKMqJi2yfpijKMUVRjiuKsktRlB71H+YZkf5uACQVyOQuSU1C2l9qYteVwN2rofPEc7c7e6m3uBv6L/V+pVK9qMvI/Wtg7CW2pwLXCSG6Aa8DC+shrosK83bF1kaRI3dJauzyTqj96l+PA3sXuG8zhPW/8L6+7dVSjeyAqTe1Fq2FENsVRYm4xPZdZz3cA4RcfVgX52BnQ7iPixy5S1JjZDJA0mY4/D3Erwcndxj1KvSdBQ4u1o6uRanvK5L3Ab/V8zHP087PjeTCyoY+jSRJtbGY1d7zrP3qT/xvUFUMrv4w5Gl1dUZnL2tH2SLVW3JXFGU4anIffIl9ZgGzAMLCrnyh/Eh/N7aeKsBotmBv2+SuCUtS85AfBz/dDcWJ6mNnL2g7TL0vabsRYCu72aypXj59RVG6A4uAG4QQxRfbTwixkNM1+d69e4srPV+knxsmiyCjpIp2fm5XehhJkq7U8V9gzWPqbeom/hfCBoB3W7nOSyNy1cldUZQwYAUwXQiRcPUh1a7dWR0zMrlL0jVisUDWPji0BI58ryb0275W13uRGp1ak7uiKEuBYYCvoihZwFzAHkAIsQB4BfABPlPUb22TEKJ3QwUM0M7PFUB2zEjStWCogu3vwtFlUJEDto4wYLa6JoycSdpo1aVbZmot2+8H7q+3iOqglZM9Ae6OsmNGkupTURIsHqMusTv8JfDroN5g+pf71Lp61I3Q5VXoMFbtgpEatSZ7xSPSX3bMSFK9EQLWPQUmvboy48k10H60unKjqy9MXwXthls7SukyNNlWk3Z+biQXaBHiiq/LSpL0t+M/Q+p2uP41eOIo9HsYkrdChzHw8C6Z2JugJj1y11abKKioJsDdydrhSFLjU5QIh75RV18M7nXxThZdKWz8PwjuDb1mgo0NjH0LRs2VM0absCab3P/ukkkq0MrkLrVsuz6B9N3qAlwOarMBRh0su0udYLTrE/DvAj1uV1dedPYGZ09wcFOXBfjzbXXi0V0r1MT+N5nYm7Qmm9zPXkBsUKSvlaORJCvZuxA2zVH//Mt9cMf36kqLm15WE/uUJVBVAge/hs2vXPw4/R+F1t2vScjStdFkk7t/K0fcHO1kO6TUch37GX77F0SNgzZDYcPzanml7XDY/yX0f+TMCoy9Z0JFvjpC15WArgwMlWCsBBt76Habdd+LVO+abHJXFIV2/m6yHVJqfkwG9dZzzl4Xr5PH/warHoLwwXDrYvUepGUZsOe/cPAbCOgKI+ee+5pWAeqP1CI02eQO6jIEfyUVWjsMSao/Ffnwv1FqorZ1VGd/th0GI14GNz+1ZXHP57DpJWjdA6YuPXNz6dGvgyZDbV+cvOjM81KL1KSTezt/V5YfyqJcb8TdSc6Uk5oQvQaO/qiWU/w7qc+ZDPDzPaAthJGvqF0sZZlw5AeIXQXDX4T8WDj8LXQcD5MWnrmACmqt/bYloC8DF2/rvC+p0WjSyT3ydMdMbHY5A9r5WDkaSbqAuDVqN4pPJHS6CSIGwdGlsPNjNQnbOcHYt9UWxE0vQcZumPw/6HbrmWMUJqj19A0vqI+HPgfDXjy3s+VvNjYysUtAE0/uAyN98XC25+tdqTK5Sw3PbIK9CyB1G0TdAF0ng5PHhfc16tSLmwcWg28UZO5TZ33+rcNY6P+wmuTXPqXe3CL7gLpmy9mJHdRlAO5aAYmbAAU6jG6wtyg1H006ubs52nHPwAg+/j2RxPwK2ge0snZIUnOVcxjWPA55x8AtUE20G15Uu1H6PwJB0ep+QqjbNs+FwpMw8HG1Xm5jpybv1G3QZhiE9lH3jxgKuz+B31+DiCHqXYsuRFHU2aKSVEeKtabv9+7dWxw4cOCqj1NaaWDg/D+4oVsg70+JrofIJOks1VrY+hbs/Rxc/eDG99TySs4hdbR97CcwVKiJOepGOPwdFMSCRxhM+BAiR9btPJos9fhy4pBUC0VRDtZl5d0mu7bM37xcHZjaN4zVR3LILKmydjiSNQkBJ9eqifJSco+pKyDWJn4DfNZfbS+MuQce3aeO1BVFnc4//n14Ohaufx2Kk2HjiyDMcMsX8Pihuid2AI8QmdiletXkR+4AuRodQ9/dytS+Ybw2sWu9HFNqgo7/AsvvA0d3uOEd6DH13D5xixl2/Ee9wAnq7eCGPa9OyQe1WyX3CCRtUX+yD4JfR5jwEYT1v/S5TQZ1WVy/The+0ClJ9aSuI/cmXXP/W2sPZ26NDsR44FuSez9Du2B/a4fUNAhx+mbGl7F8g8XSMMlLiNpv0WY2QvpOdQJPfixc9zy0GaJu0xbC+n+pvd8ObrDqYXUU33smuAWAvTOsfxZS/lRnY7r6w/5FcPwnNYFX5EFlgXosxQaCYmD0m9B3Ftg51B6/nQMEdLmqj0CS6lOzSO4Az/rtxcd2Aa9/XkV171k8PrI9/q3kJI5L2vwK7P4U7lmrtuj9TVeqrlfS7yEI7Hbm+cIE+OoGdRQ7/kN1Us3VMhth3dNwYiWED4SosdDhBnBvfe5+R39UWwF1pWr7oJMHfH8b3LkM2l6nJm6DVi2J+EbBns/Ui5Tx684cw84JbvoEek5Xv0gGPAI73oeydPWCqHsw+HZQJw3JdkKpiWsWZRmMOvgoGrR5pLt2Z2Tpi9jb2nD/kDbMGtqWVk1lglNlMXw9DrpOguueu7pj6cvVNUS8Ii68fc/narJUbCBsIMw8KwlufElN+q7+cP9m9Ri6UvhypDrSN1apyXXCR9BxXO1xlGWok20cW4GTJ9ieHlMYquCXmZCwQZ2Uk3dcTbSKLURPVfu53QLUHu+DX6v37BwwW11b3FAF30yA0jToe7+68uGIl2Hos2fOXVkMJcmgzQdtgXrR06/DlX+mktQI1LUs0zyS+86PYfPLEDkKkraQOeMg7+wqZ+2xXLxc7Hl0eCTTB4TjaGdbP+drCELAzzMgbpX6+JYvoMcdtb/uyA9QGA/9HgT3IPW5xM3qnel1pfDQTvCNPPc1savUc3Ucp46WN/4f3POrOluyNA0+7aP+OesAuPjAzPWw8kFI2wkz1qo17ZWz1GQcdSMMeQZC/vFvTQg4sVwtlehKzjzv4Abhg9QEHbsKMvfCuP9An/vU1xSeUtdGObAYhAU8gtWYBj8Fw+ec+WIAqCyCb25Su1NaR8P9v5+7XZKaoeab3E3Vat20/Wj1V2t9OXzUXa2R3vAOfNobxr4D/R/ieJaGdzac4q+kIsJ9XJgzrjOjOvmj1FbbtYa/LwYOexHS/lKT3j2/XvpCXvwGWHoHIMDWAXrepZY5Dn97uo6cq67jPWPdmTp56g74bjIE9YS7VwEKfBwNXm3UJL7iAbVW/dhBtetkyU3qGifVGpj4X/UcoF5A3PmROsLXl6mj4k4ToFVr9Qthz2dwaq16A4j+D4PZoLYVFsWrd/gpSVZjnvQldLn5/PemyVYvfiZtgbHzoeONF/4MKovhz7fUEpJv+6v5G5CkJqH5JveD38Cvj0NIX/VOMak7YNt8mPWnmrA+H6SODu/bWPOSbQmFvL42jqQCLUPa+/LCDR3pEnSRmYXXQmWRejHPL0qtL+tK1ZY73/YwcwNUl8OikeoX17h/q+/H9vQFu78vfhachEXXg09bdZS/9wu1x1qYYdAT6pfEsZ9gzezTI+P7IXM/LJkInqEw87czdeW9C9WlY69/Ta3DD3lGXdsE4NR69aYP/R+GMW+e/16qK9SSye7/ql8mf7N1hBFzYMCj6pon/1SWoZZfPILr9aOVpOau+SZ3sxGOfA9/zleTiWILncarNyUA2PYebH0Dnj55pkwBGM0WvtuTzgebEyjXmxjZ0Z/ZIyLpGealdoD89b5abuj7QO1dG1cj6Xe1k0Obrz529FAvTGqy4aG/zpRQihJh0Sh1VPw3Wwf1lmk9bod1z6h151lb1R5pUDs+TPozdXYh4Nub1fLKpIWw8mFw9VETe6vAM8c16uHjnlCRo96l54kj506r15Wpd+65FItFrcdX5Kpx+EWBV/hVfVSSJJ2v+Sb3vxl1sG+hWred9OW5SfGs0sw/aXRGluxK4387UymrMjKwjSf/dl5MUMov6g7db4cJH6vLpRYlwR+vQXkudJ+ittA5uqtTyI8uVS/SDX/pzFTySzFVq90buz9VSyaTFqrJ8OgytbXv+nnQ+95zX6MrhdJ09QvNWKnud+QHdWRv6wAz1td+7tI0+GyAehHU4/SI3TP0/P32L1K/MMbOV0fpkiQ1Ss0/uV/KZwPByR3u3XD+tsoisLGl0qYVS3cnE7r9GcZYdvCtwxQig3wYkPY5ptYx2IX1VROenRN4hqsX7eyc1JFtRY464rZ3UhN873th2AtQkgpZ+9Qvg+g7IfD0hKqiJLUrJO8Y9HlAXXfb3vnK3lu1FmJXqKP1diPq9prD36ndMVOWgE+7C+9jMav17chRFy6jSJLUKLTs5L7tXdj6Jkz5Vl2wyaCFrP2Qsk29oAdqcnZyB00msZ2f5v8KRnI0S8MYm/28b/8ZToqRPR43ktbtSbp37EBXmxSUQ9+q5ZSuk9Rbm1mM8MebsO8LtbPjbzb26rb2Y9RulG3vqpNcJv639tZBSZKkS2jZyb0oSS3NcNZ7s3dRE22boWpvd1mG2g3ScVxNB4hGZyQ2W0Nq4gkSc0vZlO9OjkYPQJCHE6O7BDKmSyB9Irywsz1rlmbOYbX9MKALhPQBW3vYt0jtGNGVqJ0kkxaecw1AkiTpStRbclcUZTEwHigQQpy3cIui9hV+BNwIVAEzhBCHajtxgyZ3UKenV2vVxZjsndVWv7pMI/+HgnI92xOL2HAijx2JhVSbLHg42zOioz8jOvrTv60Pfq0usuCToVKNI7iXLHVIklQv6jO5DwW0wJKLJPcbgcdQk3s/4CMhRL/aTtzgyb0BVFab2JFYyOa4Av44lU9plRGAdn6uRId64e5sh6uDHf7ujtzaKwQXBzmhRpKk+lVvC4cJIbYrihJxiV0moiZ+AexRFMVTUZTWQojcS7ymSXJ1tGNs19aM7doak9nC8WwNe1NL2JNSzF9JhVRVm6kymjFbBAv+TGbuTV0Y3TmgcU6akiSpWauPoWUwkHnW46zTzzW75H42O1sbeoZ50TPMi4euO9OBIoTgYHopc1ad4MFvDzKkvS99IrwJ9HAiwN0Jdyc7XB3tcHeyJ8DdUSZ+SZIaxDWtGyiKMguYBRAWFnYtT33NKIpC7whvfn1sMN/sSmPh9hR2JBZdcN8Ad0cGRfoypL0vg9r54u8uV7GUJKl+1Klb5nRZZu1Fau5fAH8KIZaefhwPDKutLNMUa+5XSm80U1BeTX6FHq3ehLbaRGmVgb2pJexKKqqp3bf3d2NQpC9jugTSr403NjZyVC9J0rmu5c061gCzFUX5EfWCqqY51tuvhpO9LWE+LoT5uJzz/N0DIrBYBLE55exMLmJnUhE/7s/g611phHo7c2tMKN1C3HG0s8XJ3oZwH1d83eSt2CRJql1dumWWAsMAXyAfmAvYAwghFpxuhfwUGIvaCjlTCFHrkLwljdwvh85gZkNsLr8czGJXcjH//OvpGNiKIe19GRblT9823tjbylu6SVJL0rInMTUT+eV6cjV6qo1mdEYzsTnl7Egs5GB6KUazwMPZnpGd/OnXxhtvV0e8Xe0J9HAmyMNJXqiVpEbKaDFiNBtxsXepfecLkMm9GasymNieUMSm2Dy2nMynXG86Z7u7kx2dWrvTO8KLaf3CCfK8wnVsJEmqF0IIYotj+TX5V35L/Y1pnabxYI8Hr+hYLeoG2S2Ni4MdY7sGMrZrIEazhTyNnrIqIyVVBjJKqjiZW05cTjkLtqXwxbYUJvQIYvqAcLoEuTfuu1FJUhOnNWhxtHPE3ka9tWeKJoWNqRv5Le03UjWpONg4MCx0GDEBMQ0ei0zuTZy9rQ2h3i6EXuB+zlmlVXy1M40f92Ww8nA2tjYKbX1d6RLkzpD2fgyL8sNHXqCVpKuiN+nZkrGFlYkr2Ze3DwBPR09c7FzIqcxBQaFXQC/u7nw3oyNG4+7gfk3ikmWZFkCjM7I9oZD4vApO5ZVzJLOMIq0BRYHuIZ70CvOiR6gHHQPdKdcbydXo0VQZGNM1EP9WsvdekswWM8maZOJL4kkoTSBVk0pZdRnlhnLyK/OpMlUR4hbCuLbjsLWxpVhXjKZaQ7R/NNeHX4+/i3+9xSJr7tJFWSyCEzkatp4qZEdiIcezNVSbLOft18rRjmdGd+Cu/uHnroIpSS1AYVUhO7J3sCtnF3ty96Cp1gDgYONAhEcE3k7euDu44+3kzfXh19M7sDc2SsP/P5HJXaozo9lCfF4FSQVaPF3sCfJ0xmQWvP3bSXYkFtG5tTtT+4UxqJ0PbXxdZSeO1KzsyNrB7tzdeDt54+Pkg6Zaw+8Zv3O08CgCgb+zPwOCBtCvdT86+3Qm3D0cOxvrVbRlcpeumhCC9cfzeGfDKTJKqgAIdHeiZ5gnXYM96BLkTky4F+5O9laOVJIun0VY+OLYF3x25DPsbOwwWc50nXXy7sTIsJGMCBtBpGdkoxrQyG4Z6aopisK47q25sVsg6cVV7EouZndKMcezyvjtRB4AtjYK0aGeDG3vx5iuAXQMvDYXiyTpalQZq5izcw6b0zczoe0E5g6ci0VYKNYVY2djR6BrYO0HaeTkyF26IuV6IyeyNexKKmZHYiHHsjUIAV2D3bk1JoQJPYJkJ47UKFQaKzlaeJSjBUeJL40nVZNKRnkGFiw83etp7u58d6MamddGlmWka6pYW82vR3P4+WAWsTnl2CjQt403Y7sEMqCdLyFezrg6yl8UpYZlspjYkLaB2KJYMisyyajIIL08HYuwoKAQ7h5OW4+2tPVsy5DgIdek37y+yeQuWc3J3HJ+O57LbyfySCzQ1jzv4WxPpL8bA9r6MKCdD73CvXCyl5OqpKsnhGBb1jY+OPgBKZoUnO2cCW0VSmirUDp4dSDaP5ruvt1xc3CzdqhXTSZ3qVFILtQSm1NOdqmO7LIqYnPKOZalwWxR18Z5ZFg77hkYIZO8dEVK9aVsydjCmqQ1HCk8QoR7BE/GPMmIsBFNqtRyOWRylxqtCr2RfaklfLsnnT/jCwl0d2LW0Lb0b+tDhwA32VMvXVRGeQYnik6QUJpAbHEsB/IOYBImwt3DuavTXUzuMLlm6n9zJZO71CTsSSnm3Q2nOJRRBoCzvS3RoZ7c0jOYG7u3xk3W6SUgW5vNBwc/YGPaRgDsbOxo69GWwcGDGRsxlo7eHZvtSP2fZHKXmgwhBOnFVRzNKuNIZhnbEgpJKazExcGWsV0CGdrBj/5tfQj0kEshtCRGi5GEkgQ2pW/iu7jvsFFsmNF1BqPCRtHWoy32ts17hH4xss9dajIURSHC15UIX1cmRgcjhOBQRhk/H8hk/fFcVhzOBqCtnyu39Qrl9j6heLs6WDlqqb5la7M5UXSC2KJYjhcdJ7Y4Fp1JB8D4tuN5IuaJZtF/fq3IkbvUqJktgpO55exJKWZzXD57U0twsLNhfPfW3NC1NQPa+cjSTRN3MP8gnx/5nL15ewGwt7EnyiuKaP9oevj3oKdfTwJcA6wcZeMhyzJSs5SQX8G3u9NZeTgbbbUJe1uFXuFePDi0HcM71t/Ke1LDO154nI8OfcTevL34OPkwvfN0+gf1p4NnhxZbcqkLmdylZs1gsnAgvYTtCUX8diKX9OIqhkf5MWd8Z9r5Nf1e5uYsrzKPjw59xNqUtXg7eXNf1/u4Leo2nO3kHcPqQiZ3qcUwmCws2Z3GR1sS0RnN9Ar3onuIB91DPBnR0V/OjG0kinRFLIlbwtKTS7EIC3d3uZv7u92Pq72rtUNrUmRyl1qcwopqvtyRwt7UEk7mlGMwWwj2dOatSd24roOftcNrEQxmA+tS1rEkbgn5Vfl09+tOtF80JfoSViSuwGA2MLbNWJ6IeYJgt2Brh9skyeQutWgGk4X9aSW8svoEyYWV3NYrhGfHRBHgLtspG0JeZR6rk1azLH4ZhbpCOnh1oKtvV44VHiO5LBlbG1tuancTM7vMJMIjwtrhNmkyuUsSoDea+eSPRBZsS8FsEUQFtGJwe19GdQqgXxtvbGxaxsSXhmCymNiWuY1fEn9hV84uLMLCgNYDmNFlBgOCBtRMKio3lGOxWPB08rRyxM2DTO6SdJbkQi2b4/L5K7GIfWklGExqyWZSTDCTY0KI8JV137rK0eawOmk1vyT8QoGuAH8Xfya2m8gtkbcQ6h5q7fCaPZncJekidAYzm+LyWH4om78SC7EIGNDWhzv6hjKmS6BcxOwfqs3VrEtZx56cPRwpPEJuZS4KCgODB3J7h9sZEjLEqreda2lkcpekOsjT6Fl+KIsf92eQWaLDv5Ujz43tyKSewS2+ZFNtrmZF4goWHV9EQZU6Qu/p35Oe/j0ZGjxUjtKtpF6Tu6IoY4GPAFtgkRBi/j+2hwHfAJ6n93lBCLH+UseUyV1qTCwWwc7kIv6zKYEjmWV0C/bg5fGd6dvG29qhXRNCCFYnr+bfB/5NpaESe1t7LMJCtbmaGP8YHol+hL6BfVvM4lyNWb0ld0VRbIEE4HogC9gPTBVCxJ21z0LgsBDic0VROgPrhRARlzquTO5SY2SxCNYczeGdDafI1egZ0yWAF27oRJtmXJPXVGt4dferbE7fTIx/DDEBMRjNRszCzNCQofRv3V8m9UakPhcO6wskCSFSTh/4R2AiEHfWPgL4+87IHkDO5YUrSY2DjY3CzT2DGdMlkP/9lcJnfybz+8ltTB8QzhMj2+Pp0nwWLCvVl7IqaRXfxX1Hib6EJ2OeZEaXGdjayGsOzUFdknswkHnW4yyg3z/2mQdsUhTaVJEcAAAgAElEQVTlMcAVGFUv0UmSlTg72DJ7RHum9Anlg82JfLMrjRWHsnliZHumDwjHvgneUKRIV0RSWRKpmlSOFBxhS/oWDBYDMf4xfDzyY7r4dLF2iFI9qq9L3FOBr4UQ/1EUZQDwraIoXYUQlrN3UhRlFjALICwsrJ5OLUkNx7+VE29P6sY9A8N5c91JXlsbx+KdqdzQNZCRnQLoFe7V6BN9lbGKd/e/y/LE5TXPuTu4M6n9JKZETaG9V3srRic1lLrU3AcA84QQY04/fhFACPH2WfvEAmOFEJmnH6cA/YUQBRc7rqy5S02NEIKt8QV8tTONPSnFGM0CH1cHXpnQmZt6BDWKuvSe3D1sTttMV9+u9A7sTZm+jBd2vEBmRSbTO09nSMgQ2nq0xc/Zr1HEK12++rygaod6QXUkkI16QfVOIUTsWfv8BiwTQnytKEon4HcgWFzi4DK5S02ZttrEX4mFfL4thaOZZdzQNZA3bu6Kj5uj1WL6Kf4n3tr7FoqiYLKYap5v7dqatwa/Re/AWvOB1ATU2wVVIYRJUZTZwEbUNsfFQohYRVFeAw4IIdYAzwBfKoryFOrF1RmXSuyS1NS5OdoxtmtrRnUKYOGOFD7cnMielG3c3ieMKb1DaHsNlx22CAsfHvqQr058xeDgwbw39D3yq/LZn7ef0upSpnWahruDe+0HkpoVOYlJkupBfF4F7208xdb4QswWQd823rx4Q0d6hnk12DmNZiMb0zfyXdx3xBbHMqXDFF7s96KcLdrMyRmqkmQFBeV6lh/K5ptdaRRU6Ll3UBueGR2Fs8PVtRdWGatYeGwhJ4pOYGdrh72NPbFFsRTqColwj+Dervdyc+TNso7eAsjkLklWVKE3Mv+3U3y/N4MwbxfeuLkrQ69wTfmd2Tt5fc/rZGuz6ebbDVBXZPR38ef2qNsZFDwIG6Vxd+xI9Ucmd0lqBPakFPPiiuOkFlUyoUcQL4/rhH8d15SvNFby1t63WJO8hjYebZg7YC69Ano1cMRSYyeTuyQ1EnqjmQXbkvnsz2QcbW24rXcoN/cMoluwR00ZxSIsKCg1j2OLYvnX9n+Rrc3m/m7382D3B3GwbT6zY6UrJ5O7JDUyqUWVvLfxFFviCjCYLbTxdWHaEBvyxA7Wp6jr7LXxaENrt9b8nvE7vs6+zB8yX47WpXPI5C5JjZCmWsOOjH2sPLWNQ4X7MNnmYos9YyKup5VjK1I0KaRr0ukZ0JOX+7+Mh6OHtUOWGpn6XDhMkqSrZBEWvo37lk8Of0K1uRpHW0d6BvVArxnLrqNhZBlD+GBKdJ3r8ZJUG5ncJamBZVVkMWfnHA7mH2RY6DBmdplJV9+uNTX0n9plMmfVCQa/u5VbooO5b0gbOgS0snLUUlMnk7sk1bO8yjwWHF1AZkUmeZV55FTm4GTrxOuDXmdiu4nn9aJP6R1KnwhvFu1IYfmhLJYdyOTGboG8NK4zwZ7OVnoXUlMna+6SVI/25+3n2W3PojPpiPKKItA1kCC3IG6Pup0gt6BaX19SaeCbXWl8sT0ZBYXZIyK5f0gbHO3kGuuSSl5QlaQGpjPpyNXmYjm9svXOnJ18cPADwtzD+Gj4R7TxaHPFx84qreKNtSfZEJtHhwA3PpkaQ1SgLNVIMrlLUoPYlbOLFYkriC+JJ708HcG5/39GhI7gzcFv4uZQPwuHbT1VwL9+OUa53siccZ2Y3j9cLjHQwsnkLkn1KEebw3v732NLxhZ8nX3p7tudKO8owt3DaxbqcrN3Y0DQgHpfCqCwopp//XKUP+MLiQnzZEKPIEZ3CZT1+BZKJndJugrJZclsTt9MXmUeeVV5HMg7gILCrO6zuLvL3TjaXtt12y0Wwfd70/l2TzoJ+VoAhrT35cPbo626hrx07cnkLklXQAjBL4m/MH/vfAwWA95O3gS6BtLRuyMPdX+I1m6trR0iKYVa1h3L5dOtSfi1cmTRPb3pGCjXa28pZHKXpDoyWUzoTDq0Bi0fHvqQ9anrGRg0kDcHv4mvs6+1w7uoo5llPLDkAJXVJv4zJZqxXQOtHZJ0DcjkLkm1OFZ4jLf2vkVscc0dI7FRbJgdPZv7ut3XJJbRzdPomfXtAY5laRjbJZA54zsR4uVi7bCkBiSTuySdJoTgQP4BNNUafJx9cHdwZ+mppfwU/xN+zn7c0v4WWjm0wtnOma6+Xens09naIV+WapOZRTtS+fSPJCxCMHt4JA8Na4e9beP/cpIun0zukgScKDrBfw78hwP55/5bs1FsuLPjnTwa/Wi9tS1aW06ZjjfXnWTd8Vy6BXvwwe09iPSXvfHNjUzuUotkspiIL43nSMER9uTs4c+sP/F28uahHg8R7RdNsb6YEn0JHb070sGrg7XDbRAbTuTy4orjVBnMPDe2IzMGRmBrI3vjmwuZ3KVmrdJYyb7cfezO3a2WXPQaqkxVVJmqamaM+rv4c0vkLczoMqPZjM7rqqBCzwvLj/PHqQI6tXZn3oTO9GvrY+2wpHogk7vULGkNWhYeX8j3cd9jsBhwtnMmxj+GANcAXOxccLZzpoNXB6L9owl0bdndI0II1h/P4811ceRo9EzoEcSrN3XB21Xe0akpk+u5S03e+pT1LItfRrBbMB28OmBrY8ui44so0Zcwoe0Ebo68mWj/aHn7uYtQFIVx3VszoqM/n29LZsGfyexNKeaD26MZFNl4Wzyl+iFH7pJVaQ1aPjn8CUcLjzKu7ThuibwFG8WGt/a+xerk1YS7h6Mz6ijQFQAQ4x/Dc32eo4tvFytH3vTE5mh4fOlhUooqmTW0Lc+OjpIdNU2QLMtIjYpFWEgqS6JUX0qgayABLgHsztnNG3vfoLCqkEivSBJLE3Gxc8HT0ZPcylxmdZ/FQz0ews7GjlJ9KYW6Qtp7tpcLZ10FncHM6+vi+GFvBr3DvfjvtBgC5N2fmhSZ3KVrKqM8gxJ9CS72at27wlBBZkUmmRWZxBbFciD/AGXVZee9LtIzklcHvkp3v+7EFsXy/cnvSdWk8nTvp+kT2McK76RlWH0kmxeWH8fV0ZZPpsYwoJ282NpU1GtyVxRlLPARYAssEkLMv8A+U4B5gACOCiHuvNQxZXJvXPQmPUcKj9A7oHfNKoe1MZqN/J7xO8vil53XR362INcg+gT2oU9gHwJdA8mvyievMg93B3cmt5+Mva19fb0N6TIk5lfw0HcHSSuu4tFh7Zg9oj0OdrJM09jVW3JXFMUWSACuB7KA/cBUIUTcWfu0B34CRgghShVF8RdCFFzquDK5Nw5ag5Zl8ctYEreEEn0JQ0OG8t7Q93CxV6ewCyE4VHCIpNIkMioyyKrIolBXSLGumCJdEQaLgWC3YG7rcBtR3lHoTDqqjFW42LsQ2iqUELeQFteG2JRoq03MXR3L8kNZdAly5z9TeshFyBq5+kzuA4B5Qogxpx+/CCCEePusfd4FEoQQi+oaoEzu14ZFWNiauZX9eftxsHXAxc4FBYVsbTaZFZmcKjmF1qhlYNBAuvt1Z+GxhUR5RfHpyE9J1aTy8aGPOVZ0DABHW0dC3ELwd/HHx9kHHycf+rXux6DgQU1iHRbp4jbG5vHSyuOU60w8MLQNDw+LxM1RNtM1RvXZChkMZJ71OAvo9499Opw+6U7U0s08IcSGOsYqNYAyfRmbMzazJHYJaeVpONs5Y7aYMVgMAPg6+xLiFsLoiNHc1uE2uvp2BaC7b3ee3fYs41eOR2fSEegayLwB8xgSMgRfZ1+ZxJupMV0C6R3uxRvrTvLfrcn8dCCLf42OYnKvEDm7tYmqy8j9VmCsEOL+04+nA/2EELPP2mctYASmACHAdqCbEKLsH8eaBcwCCAsL65Wenl6Pb6XlqTZXo6nWkFuZS6omlRRNCkmlScSXxlNQpVbFOnl3YmbXmVwffj12NnYYLUYswnLJm02cKjnFe/vfY3jocG6Luu2a35hCsq7DGaW8tjaOwxll9Azz5J3J3ekQINeoaSyudVlmAbBXCPHV6ce/Ay8IIfZf7LiyLHNxRouRg/kH2Za5DRd7F6L9ounh34P8yny2ZGxha8ZWUjQpVJurz3mdvY094e7hRHlHEeUVRbR/NNF+0bJ1ULpsQghWHs7m9bVxaKtNPDwskkeHt8PRztbaobV49Znc7VAvqI4EslEvqN4phIg9a5+xqBdZ71EUxRc4DEQLIYovdtyWlNwtwkJWRRaZFZkEuQUR1ioMW5sz/0n0Jj3xpfGcKDrB8aLj/JX9F5pqDY62jpgsJszCXLOvgkK0fzTdfbvj6eSJu4M7fs5+tPVsS7BbcJ07XSSpLoq11byx7iQrD2cT7uPC/93YidGdA+SAwYrqreYuhDApijIb2IhaT18shIhVFOU14IAQYs3pbaMVRYkDzMC/LpXYmxuLsFBYVYiTnRMudi4YLAYO5R9if/5+DucfJqE0gSpTVc3+znbORLhHoDfrKdYVU24or9nm7eTN0OChjAwbycDggQghOFF0gqOFR/Fw9GB46HD8XPys8TalFsjHzZEPbo9mUkwwr/0ax4PfHmRAWx9ev7krkf6yC6oxk5OY6shkMXGy+CQFugICXAIIdA2kwlDB2pS1rEtZR7Y2+7zX2NnY0c23G528OxHlHUVoq1CytdkklCaQoknBxc4FHycffJ19ifSMpItvFwJc5KhIapxMZgs/7Mvg/c0JmM2CT+7sybAof2uH1eLIGaqnCSEoW/YTroMH4xASXPN8tjabN/e8SXJZMkaLEaPFSFirMMZEjGFMxBg8HD2ILY7lcMFhDuUf4lDBISqNlecd30axoX/r/gwNGYoQomaE3t2vOz38euBs59zg71GSrqWcMh33fXOA+LxyXh7fmRkDI+SA5BpqtsndXF6OMTe35rGNkxMO4eEX3b984yayn3gCl969Cft2CQBrU9by5t43UVAYETYCext77GzsOFZ4jJMlJ1FQsLWxxWQxAdDGow19A/vSO7A3oW6hNTMsFUVhZNhI/F2ax+jFVFyMnc/509BNJSXYennJ/8BSjcpqE08tO8KmuHxu6RnMy+M7y6WEr5Fmm9zLN2wg+8mnznkubPH/cB048Lx9hdHIyRtGYy4oxM5gZv1D0RyLcuBQwSFi/GN4a8hbBLsFn/OaNE0aG9M2ojPpiPaPpodfD7ycvC47zqamcu8+MmbMUD/LAQNqnjdkZJAybjyB8+biOXlyg8dhKi2lcucu3G+8AcWmafbUWwwGyn9di8eE8SgOzTfhWSyCj/9I5NM/knB3tmfOuE7c0jNYDgIaWF2Te5P73+McHU3wxx/V/Nj5+VG8+Ktz9hFCcDD/IJ+/NgklK4/PbnakyNeB3itOoq+u5ImYJ1g8ZvF5iR0gwiOCB3s8yJO9nmRY6LArTuwWnY6S778n8+FHKFu+AovBcEXHuRBhNiPM5tp3PPs1RuMlt5f98gsIQekPP5z7/M8/I4xGytetv+w4Qf27uJxYC+bPJ+fZZ8l59tnzPrOLvQdhMl1RbA2l5KuvyX3pJcpWrbJ2KA3KxkbhyVEdWPf4EMJ9XHj6p6PM+Go/BRV6a4cm0QSTu31gIO6jR9f8eE27k8q//qI6KQmzxcyW9C3c9dtdPLz6HnqvS6asSyjvvLSVHnP/jX9BNQuqp3B/t/vPaUWsTxadjsKPPyFp+AjyX38D3bFj5L70EskjR1G08Ess+iv/h28uK6NowQISrxtG6m23YSosrPU1xoICMh98iISBg6jcs+fCx9VWUrF5M4qTExV/bMVYoE6AEgYDZStWgo0Nlfv2YS4vv+DrL3XutFtvI3PWg9TlN0RDRgaatetw7NSJ8vW/kfXQQ5i1lVTu2UPGA7M4FdOL8o2bznlN5Z49JPTrT/H/Fl9WbA3FVFJC8cKFAGjWrLFyNNdGVGArlj80kHkTOrMnpZgbP9rBn/GXXFpKugaaXHL/J88pU1AcHYld8B4TV0/kqT+fokRXwjsZA2hVJYie9z7uju60GjUK55gYCj/9BH1cHGW//ELOnDlkPfZ4zU/Jkm8ve0R8NiEEua/Mpeizz3Du1YvwH76n/V87CP3fIhzbt6fw/fdJvWUSumPH6ny86tRUypavIOfF/yNx+AgKP/wIx/aRGFLTSLtzGoaMjIu+XrNuHSkTbqJyzx5sPT3JfGAW5Rs2nrdfxebNCL2ewFdeAbMZzYqV6vN/bMVcXIzvQw+CyYR22/Y6fxaG9HTS75yGPjaWyp07qdq7t9bXFH/5JYqtLaFfLKD1229TuXcfSdddR8aMmejj4nAIDyPnueeoOnQYAH1CAlmzH0MYjRS89x6atevqHF9DKfrscyx6PR4Tb0J34CCGrCxrh3RN2NgozBjUhjWzB+Pj6siMr/Yzb00sGt2lf2OUGpAQwio/vXr1EvUhV5srlt87Uhzu0lHc8d04sSF1gyjfu0ec7N5DZD39zDn7Vh46JOKiOtb8xPftJ5LHTxDJ4yeIxNGjRVxUR5F65zRRnZFxyXNaLBZRnZkp9Mkp5zxfumKliIvqKAr++98Lvk67c6dIuG6YiOvcReS//4Go2PGXqNjxl9Du2iXMOt05+1ZnZoqkceNqYj3Vt5/Ifv4FoTsVL4QQourIERHfr7+IHzhIaH7bUHOs0l+Wi+yXXhJJN9wo4qI6ipQpU4Q+OUWYSktF6h1TRVzHTqJk6dJzzpV2zwyReP1oYbFYRNrd94jEESOFxWwW6TPvFQnDhguLwSDiBw8WmY8/ceHPo7paVB07VhOD5rcNIn7gIBHfr7+o3L9fJAweItKm333Oa/I//FAk33xLzWdtyM4WcV27idxXX6vZp3zrVpF213RRsmyZMOv1wlhSIpJGjxHx/foL7e49IuG6YSJhyFBRnZoq0qbdJU527Sa0e/cKU1mZKPx8gUgcPkIUfPLpefEa8vOFPjFRWMzmC74fIYQwFhUJQ3b2RbdfSHVqqojr0lXkvDJXGLKyLvlvoTnTGUxi7uoTIuKFtSLmtU3ih73pwmS2WDusZgN1flGtObbJXVA9W4omhalrp9I638jbC3V4P/k4Lu07kP3U09gHBxP29dfYB5zbyaJZswZhNOLcMwaHNmdauIQQaFavJv+NNxEWCx7jx6PYn7/OuKmwkKrDhzAXFgHgNW0a/s8+gzE3j9TJk3Hu1o2wrxaj2F647GMuLyf/zbfQrF59zvMObdoQ9M58nLt3R3/qFBkPPIAwGPF/6ilc+vTGoU2b8y4wVqekkHH//Zhycs953sbDA5foaFyHDsHr9ttR7NS5ahadjuynnkb755+0fustPCfdgjE3l6QRI/F99FH8Zj+KZt06cp55lsC5r5D36mv4PjYbv0cfJXfuPDS//kqH3buwcXRECEHpt99Rvmkj+uMnENXnLoVgF9SasEWLcGzblpJvviH/7fmEf/8dLr16UbF1K1kPPwKKgq2vD2GLFlG2bBmlP/9C5MYN2AcFXfTv3JCRQdodUzGXlGDj4kL499/h1KkTZo2GtDunYcrLU/9xV1VhHx6GMT2DwFdfxev2KQBod/xF1hNPIKqqsPHwwDm6By49e+LcMwbnbl0x5uVRvHgx5avXYNOqFZFb/8DGsW5r62Q98STaHTuI3LgBOz8/0u++B2N+Hu02bGiRFxlPZGt49ddY9qeV0j3Eg/en9CDSX65Rc7WabbfM2d7b/x4/nPqB1RNXw5Ovojt+HEtlJU5duxL6xQLsvC7/YqgxN5e8ea+iO3LkgtvPTgjVySmUfvcdDuHhKA72mAqLaLN6FfYBAbWepzopCXN5BQCmgnzy57+DqbAQz9tupXztOmzc3Ahb9CWOkZGXPI5Zq6U6IbHmsa2nBw4RERftNBFGI5kPPkTlvn2ELliAPi6Owvffp93mTTiEhmIxGEi6bhhmrRbMZiK3/oF9QADaHTvIfGAWIQs+p9WwYZStXEXuiy/i2KkTrn374twzGjv/M+/bsUN7bN3UGYwWnY6kkaNw6tSJ1m+9SerEm7Fr3ZqgN98g8+FHsFRVIaqr8Zh4E61ff73Wz0539Ci5c+fh/+yzuA0eVPO8MTubzEdn4xTVAe9778WxXTsyH32Uyh1/EfLZf7FUaMl58UUcIyPxvmsauqNHqTp0GENy8ukPzxbMZhRHR1wHDkS7dStB772Hx4Txl4xHCEHZsmXkzXsV39mz8Zv9KABly5eT+9IcIn5cinN0dK3vqzaWqiqERWDr5nrVx7pWhBCsOZrDvDWxVBnM/N+Nnbh7QHiL/LKrL3VN7k22LGMym8SwZcPEY78/JoQQomLbNhEX1VGk33e/MGu1V3Xsy6HdvVskDB8u4qI6ivLf/7ji45g0GpH93PMiLqqjSLrhxssuCVzWuSoqRPJNE8WpmF4iYdhwkXrntHO2573zroiL6igyHn6k5jlLdbU41au3yJkzR+hTUsTJnjEibfrdwmIy1emchQsXqu9t3DhxsmdMTUnLkJ0tksbeIOK6dK21HHYlzFqtSJk0WZzs3kPERXUUadPvFqby8nP2MZWWioo//xT5738gCj9fIIzFxcJiNovEUdeLtLumX/L4hrx8kf7AA+qxZ8wQ5srKM8etqBAnu/cQOfPmXfIYFotFaDZtEvqUlIvuU751q4gfPFgkDL1O6BMS6vDOG5f8cp24Z/FeEf78WjH9f3tFfrmu9hdJF0QdyzJNNrnvzN4pun7dVWxM3VjzXNWJE8JiMFzVca+EqaJCVB0/US/Hqjp2TJg0mno51qUY8vLU+n9UR1GybNk526rT00V8/wFCu3vPOc9nPfW0iB8wUCTfcouI79dfGPLy6nw+U4VWxPftJ+KiOorS5Sv+sa1C6BMTr/zN1MJYUCCSbrhRZD31lDDr9XV+XeEX6hfSP6+t/K3q6FFxqm8/cbJHtCj+9rsL1vCznn5GnOrbTxgLC4WptPS8LxaL2Szy3p6vXlfp01dUHjhwznZTRYXIfuklERfVUSSPnyASBg8Rp/r2E5UHD9XpPVRnZIiyVatEzty5IuPhR4SxqKiO777+WSwWsWR3moias17EvLZJ/HEy32qxNGV1Te5Ntizz0l8vsTVjK1tv3yrXG79C1YmJlC79Ef9nnsbGtfZf9cvXryf76WcACPn8M1oNH35Z5yvftAlDaho+sx645r+WCyEu+5ymoiIShw3He/p0Ap5/7tzjGQykTJqEpbKKsMX/w7FNmwse4+9y1tmcY2Lwue9eXIcMIXfOHMrX/IrnbbdStf8Axtxcgj/4AOee0ZT9+CMl332PubQUn/vuxfexxzAVFJJ5330Y8/MJmj+fVmNGX/B9CbOZ/Dffqpm3YOPqisVgwHVAf0IXLLDqBLHE/AoeW3qYU3kVzBgYwdOjO+DuJO+jW1fNuuZeZaxi+E/DuaHNDcwbOK9+A5MuyqzVkjR8BJ6TJxPwwvPWDueayHriSar27iVy+zZszpptWrRgAYUfflRzDeJixOkL9ZbT11fM2go0vyzHmJODjZsbFq0WvyefwOfBBzGXlpI560H0J0+iODggdDpcrxuK3yOP4NyjR80xTcXF6n6xsTh26oTPvTNxHzu2pgHAYjCQ89zzVGzYgNe0aXhOuQ3HyEhKf/yR/NffwP/55/GZOaNBPq+60hvNzP/tFF/vSsPb1YGnRrVnat8w7GybfHd2g2vWyX19ynqe3/E8i8cspk9gn3qOTLoUS2UliotLi7kgpt25k8z77ifoP//GY9w4AAxpaaTcNBG3ESMI+fCDyz6mMJmo2LSJ0p9+xmP8ODxvvbVmm6WyktxX5qI4OuIzcwaO7dtf8BjqEge/Urz4KwzJydh6euIcE4NLTE+0O3dStXsP/s89h8+9M8+cVwiyHnsM7bbtRCxdiq2nByVff0PFli24DhyIz70za72AX9+OZ2l4Y10ce1NLaO/vxmfTYmjfTO76VJ2UpDZbXKDr7mo06+T+8JaHSSpLYuPkjfKenlKDEhYLyaPHYNOqFX6PP4ZzdDTZTz+N/vgJ2q5bd16rrTXi027fTsWGjeiOHMGQlgZ2dgS9+QYeEyeet7+5rIyUm2/BotNhqagAW1tc+/al6uBBhF6P23XX4X3fvbj06VPzBW4sKECzYgWmojO3aLAPCcalZ0+cOnW66vVzLBYL23/exB+rt7Gy7WD+M7UXo7sEnr+fwUDJ19/g0rsXLjExV3XOhiSEoOiTTyn67DNcBw0i5OOP6lT2rKtmm9yLdEWM+nkUM7rM4MleTzZAZJJ0rrJVq8id8zKctYZN4NxX8Jo61YpRXZippARhNF3yS6fq0CFy/+8lWo0aidf06dgHBGAqLaX0hx8oPV3jd+rWDa877qDq0EHK1/yKMJmwcXdXD2CxqF8MgOLoiNvQIXjPvBeXmJ41xy/5+hssOh1e0+7EbejQC9b4hcVCxYYNFC/+Cv2JEwD8eP19fOPaiadGdWD2iMhzbs5d+Ol/Kfr0U0BdY8r73pm0GjXqotcPDJmZ5L/xJg6R7fB/5pl6vc5g1miw9fA4/z2ZzeS9/jplPy7DpX9/qvbtO6c1WwiBMTsHxc4W+8Dzv8Dqotkm95WJK3ll1yusvGklkV7X9ldIqeWy6HToT5yg6vARhF6H7+zZTXbVykux6PVoVq2i+KuvMKZnoDg64jHpFnxmzDhnaW1jfgG6w4epOnAAza+/YtFoanr5dUeOYOvhgeLsjCkvD4fIdvg99jjuY0afc66Cf/+b4kX/wyE8HO+ZMyn94QcsRiOf3/0Gy4/m0ivci3cmdyPSvxXVKSmkTrwZtxEjcOnbh5Kvv8GYmYlLnz60fvstHEJCao4rhKDsp5/Jf+cdMJkQBgPuN95A6/nzz7luIoTAlJPD/7d359FV1FkCx783KxAiIQtIQkICCSrShtA0BkQWd0HADYVWxFZh+ogKgSPQwJkedbBlpAVsODo0tAgzA9jINiDaCshmY7OEQTZJQmKTBE3CnrBk4c4f74EBCYkhj9Xx/dMAAAylSURBVJdU7uecnLyqV3l1f7nJPVW/+tWvTqfuxKdxEI179KhSd+OROX8h7+23aTZuLGHPPvvj766oiNzxEzj12WeEDR1KxKgUCtetc91U2aIFgW3bciY1ldL8fMKGvkCz0aOrlSPHFndVJe14Gm2btvVAVMYYcB2Bntm5k4C4OPxCQ6+67fnTpzn+8RKOzp8PAqGDnyHk0UcQf39OfvopR/48m3Pp6cTM/YCgzp0BOLtvH5mPD6BJv360mPTviI/PxdFYkVPfYU3zX/D6yr2cPlfGiLvj6f3BGxR/+y1tPlmFX3g4WlbG8SVLyHtrMqjSbOxYAmKiOZ2aStHGTZxJTaVRl2QiJ03i5OrV5L09haCuXQh/6WXOfrOL06k7ObNjB6V5P05wFnzvvdz42r/hFxpK6dGjHPufBRRnZXHjxAn4hoQArhvosp56Gt/gYNcopqEvEDFqFGd27CD3d+MpOXSI5uPGEjpkyMXPPb19OzkjU5CAABomJdGwYxJByV0IbH3lEVaVcWxxN8bULeeLish89DHOnz1L3LKl+N5wA1kDB1GSm0ubT1Zd7N7QsjIOPtQXCQggbukSCopK+P2K3ZSsXMGo1I8oHjmWxN8+e8lnl+TkkDt+wo8T04kQGB9PyMAnaTpo0MWzq+NLl3F44kRwTwzoHxl5sdA27NCB01u+Jn/aNHyCg2ncvTsnV692Tanh50dAbCtiZs/GJyiIzIcfAVViP15M/vTpHF+4iIaJiZzZtQv/qCgi//AmjX7100Ee1RmKWxEr7saYWuPMnj18N3AQQd26EdS1Kz9MmkTklCk0eajPJdudWL6c3LHjaDlzBo3vuouiTZvJShnNwUbhjOz6W36dHMur991Mk0Y/jkDR8+cpXLsWCWxAw8Tb8L1wbeDyGHbvoSQ7m4ZJHa44RcjZAwfIHTeO4rR0bujfj7Df/IbSgiNkDx+OT3AwgQnxFG3+ilb/NZ9GSUmuC6czZlIwcyYhTzxBszFjrsvUEFbcjTG1ytF58/jhzT+4RugkJxM9+88/OZrV0lIyevdBfHyQgADOHTiAX/PmhL8/iz+llzL3q0yCG/jzyt0JDE5uRYBfzV73UFW0pOSSvvmz+/bxz6HDKCsoICIlhfB/ufSmtLLCwovzKF0PVtyNMbWKqpL94nCKtmyh9YrlBERHX3G740uWcnj8eAITEgh97jma9Ol9cbjl/u9PMmnVPjamFdAqrBF/HJBIp9irXxOoCcXZ2RRt2kzIEwO8fiHdirsxptbR0lLKjh3DLyKi4m1UKc7MumRK7sutP5DP75fvJvfEWf44IJG+iRVPE+00jn2GqjGm7hI/v6sWdgARIbB13FUvQPZoG8HSF+8gsWUTXl6QyntfZuCtA9Xayoq7MaZOahoUwPznb6dvYiSTP93Pc3O3kp53ytth1RpW3I0xdVYDf1+mP9mBiX1uYVvWMe6ftpEJS78h71T1H0TvFFUq7iLygIh8KyLpIjLuKts9JiIqIpU/JcQYY2qAj4/wwp2tWT+mF0/fHsOirYfo/h/rmLRqLwWF5yr/AIeqtLiLiC8wE3gQaAcMEpF2V9guGBgBVP6Ye2OMqWGhQQG81r89n4/qQe/2LZizKZM7J69jzqZMb4fmFVU5cu8MpKvqQVUtBhYCP51uDt4AJgN2PmSM8Zq48CDeebIDn4/qQdc2Ybyxci9vfrKv3l1wrUpxjwIOlVvOdq+7SEQ6AtGquqoGYzPGmGprE9GYWc90YnByK2ZtOMiYxbsoLTvv7bCuG79r/QAR8QHeAZ6twrbDgGEAMTEx17prY4y5Kl8f4fX+txIaFMD0NWmk5xfyYs947r65GT4+zn7gTFWO3HOA8reStXSvuyAYaA98KSJZQDKw4koXVVV1lqp2UtVOEZWMdTXGmJogIqTc25YpAxLJO3mOofO2cc/U9az4v1xvh+ZRVSnuW4EEEYkTkQBgILDiwpuqekJVw1U1VlVjgS1AP1W120+NMbXG479syfpXe/LuoCQa+PnyyoJURi3aSdG50sp/uA6qtLirainwEvAZsA/4SFX3iMjrItLP0wEaY0xN8fP1oV9iJP/7cjdG3pPAsp059P3TJvbknvB2aDXO5pYxxtRbf884wshFqRwtKualXgm82KsN/r61+95Om1vGGGMq0aVNGKtHdOfB9i2Y+sUB+s3YzO4cZxzFW3E3xtRroUEBvDsoiVmDf0lB4Tn6ztjEKwtSycgv9HZo1+Sah0IaY4wT3HfrjdweF8b7GzKYuzmLlbtyeTgpipR72hId2sjb4f1s1udujDGXKSg8x3+uz2De379DFZ5KjuGlXvGENQ70dmjW526MMdUV3jiQCX3a8eWrPXm0YxQffpVFz7e/5K/bDtWZaQysuBtjTAVaNGnIW4/dxt9SunNL5A28ungXQ+dtJ/9U7Z9t0oq7McZUIr5ZMAuHJjOxzy1sSMvnvqnreX99BoW1+AYoK+7GGFMFF+aNX/lyN9pHNeGt1fu54621TP8ijdPFta/IW3E3xpifoW3zYOY/fzvLht/Br2JDmfrFAe6ftoGNafneDu0SVtyNMaYaOkSHMHtIJxYNS8bfx4fBc/7BqI928t2RIm+HBthQSGOMuWZnS8qYsTad99dnUHpe6RYfzqDOMTzQ/kZ8a3hq4aoOhbTibowxNeSHk2f5aOshFm49RM7xM9waeQOTHvkFHaJDamwfVtyNMcZLys4rq745zKRVe8k7dY5fd45h9H03ERoUcM2fXdXibtMPGGNMDfP1EfolRtLrpgimfp7G3K8yWbIjh6eTYxjavTXNght4PAa7oGqMMR4S3MCff+3bjr+ldOf+W5szZ1Mmd05ex+yNBz2+byvuxhjjYfHNgpk2MIm1o3vycIcoWjZt6PF9WreMMcZcJ7HhQUx+/Lbrsi87cjfGGAey4m6MMQ5kxd0YYxzIirsxxjiQFXdjjHEgK+7GGONAVtyNMcaBrLgbY4wDeW3iMBHJB76r5o+HAwU1GE5dUR/bXR/bDPWz3fWxzfDz291KVSMq28hrxf1aiMi2qsyK5jT1sd31sc1QP9tdH9sMnmu3dcsYY4wDWXE3xhgHqqvFfZa3A/CS+tju+thmqJ/tro9tBg+1u072uRtjjLm6unrkbowx5irqXHEXkQdE5FsRSReRcd6OxxNEJFpE1onIXhHZIyIj3OtDReRzEUlzf2/q7Vg9QUR8RSRVRFa6l+NE5Gt3zheJyLU/iLIWEZEQEVksIvtFZJ+IdKkPuRaRFPff924RWSAiDZyYaxH5i4jkicjucuuumF9xedfd/l0i0rG6+61TxV1EfIGZwINAO2CQiLTzblQeUQqMVtV2QDIw3N3OccAaVU0A1riXnWgEsK/c8mRgqqrGA8eA570SledMBz5V1ZuBRFxtd3SuRSQKeAXopKrtAV9gIM7M9VzggcvWVZTfB4EE99cw4L3q7rROFXegM5CuqgdVtRhYCPT3ckw1TlUPq+oO9+tTuP7Zo3C19UP3Zh8CD3snQs8RkZZAH2C2e1mAu4DF7k0c1W4RaQJ0B+YAqGqxqh6nHuQa15PgGoqIH9AIOIwDc62qG4Cjl62uKL/9gXnqsgUIEZEW1dlvXSvuUcChcsvZ7nWOJSKxQBLwNdBcVQ+73/oeaO6lsDxpGjAGOO9eDgOOq2qpe9lpOY8D8oEP3F1Rs0UkCIfnWlVzgCnAP3EV9RPAdpyd6/Iqym+N1bi6VtzrFRFpDHwMjFTVk+XfU9cwJ0cNdRKRh4A8Vd3u7ViuIz+gI/CeqiYBRVzWBePQXDfFdZQaB0QCQfy066Je8FR+61pxzwGiyy23dK9zHBHxx1XY/1tVl7hX/3DhFM39Pc9b8XnIHUA/EcnC1eV2F67+6BD3qTs4L+fZQLaqfu1eXoyr2Ds91/cAmaqar6olwBJc+XdyrsurKL81VuPqWnHfCiS4r6gH4LoAs8LLMdU4dz/zHGCfqr5T7q0VwBD36yHA8usdmyep6u9UtaWqxuLK7VpVfQpYBzzu3sxR7VbV74FDInKTe9XdwF4cnmtc3THJItLI/fd+od2OzfVlKsrvCuAZ96iZZOBEue6bn0dV69QX0Bs4AGQAE7wdj4fa2A3XadouYKf7qzeu/uc1QBrwBRDq7Vg9+DvoCax0v24N/ANIB/4KBHo7vhpuawdgmzvfy4Cm9SHXwGvAfmA3MB8IdGKugQW4riuU4DpTe76i/AKCa0RgBvANrtFE1dqv3aFqjDEOVNe6ZYwxxlSBFXdjjHEgK+7GGONAVtyNMcaBrLgbY4wDWXE3xhgHsuJujDEOZMXdGGMc6P8BwBvCLOYAwA4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.plot(hist.history['acc'])\n",
    "plt.plot(hist.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2500 samples, validate on 250 samples\n",
      "Epoch 1/5\n",
      "2500/2500 [==============================] - 1s 475us/step - loss: 1.2345 - acc: 0.4584 - val_loss: 1.0523 - val_acc: 0.5560\n",
      "Epoch 2/5\n",
      "2500/2500 [==============================] - 0s 127us/step - loss: 1.1124 - acc: 0.5304 - val_loss: 1.0204 - val_acc: 0.5600\n",
      "Epoch 3/5\n",
      "2500/2500 [==============================] - 0s 130us/step - loss: 1.0827 - acc: 0.5416 - val_loss: 1.0259 - val_acc: 0.5800\n",
      "Epoch 4/5\n",
      "2500/2500 [==============================] - 0s 130us/step - loss: 1.0664 - acc: 0.5496 - val_loss: 1.0366 - val_acc: 0.5560\n",
      "Epoch 5/5\n",
      "2500/2500 [==============================] - 0s 130us/step - loss: 1.0522 - acc: 0.5624 - val_loss: 1.0093 - val_acc: 0.5600\n"
     ]
    }
   ],
   "source": [
    "mine = keras.models.Sequential()\n",
    "mine.add(FeatureSelectionLayer(input_shape=(74,)))\n",
    "mine.add(keras.layers.Dense(123, activation='sigmoid', kernel_regularizer=keras.regularizers.L1L2(l1=3e-5)))\n",
    "mine.add(keras.layers.Dense(4, activation='softmax'))\n",
    "mine.compile('adam', 'categorical_crossentropy', metrics=['acc'])\n",
    "hist = mine.fit(norm[:train], target_matrix[:train],\n",
    "             epochs=5,\n",
    "             validation_data=(norm[train:train+valid], target_matrix[train:train+valid]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions (so far...)\n",
    "\n",
    "First of all - we don't know whether any meaningful features are in the dataset. The model overfits so easily. Model with my layer trains visibly faster (in terms of epochs). Let's try old and tested contender..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "booster = xgboost.XGBClassifier(objective='multiclass:logistic', n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "booster.fit(norm[:train], target_matrix[:train].argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.564"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "booster.score(norm[train:train+valid], target_matrix[train:train+valid].argmax(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of this dataset\n",
    "\n",
    "Looks like we cannot reliably predict Herpes from the variables we've chosen. The dataset is easily overfitted. To check capabilities of our layer lets try some better dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io.wavfile as sio\n",
    "\n",
    "cleans = sorted(os.listdir(\"DASHclean\"))[:600]\n",
    "noisy = sorted(os.listdir(\"DASHnoisy\"))[:600]\n",
    "cleans = [sio.read(\"DASHclean/\" + x)[1].astype(np.float32) / 2**15 for x in cleans]\n",
    "noisy = [sio.read(\"DASHnoisy/\" + x)[1].astype(np.float32) / 2**15 for x in noisy]\n",
    "cleans = [np.abs(librosa.stft(x, n_fft=512, hop_length=128).T) for x in cleans for i in range(6)]\n",
    "noisy = [np.abs(librosa.stft(x, n_fft=512, hop_length=128).T) for record in noisy for x in record.T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = max([x.shape[0] for x in cleans])\n",
    "cleans = np.stack([np.pad(x, ((maxlen - x.shape[0], 0), (0, 0)), mode='constant', constant_values=0) for x in cleans])\n",
    "noisy = np.stack([np.pad(x, ((maxlen - x.shape[0], 0), (0, 0)), mode='constant', constant_values=0) for x in noisy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = np.clip(cleans / (noisy + 2e-12), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_10 (LSTM)               (None, None, 512)         1576960   \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, None, 257)         131841    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, None, 257)         0         \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, None, 512)         1576960   \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, None, 257)         131841    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, None, 257)         0         \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, None, 512)         1576960   \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, None, 257)         131841    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, None, 257)         0         \n",
      "=================================================================\n",
      "Total params: 5,126,403\n",
      "Trainable params: 5,126,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, LSTM, Conv2D, LeakyReLU, BatchNormalization, Flatten, TimeDistributed, Lambda\n",
    "from keras.regularizers import L1L2\n",
    "\n",
    "model = keras.models.Sequential() \n",
    "model.add(LSTM(512, input_shape=(None, 257), return_sequences=True, kernel_regularizer=L1L2(l1=1e-6), recurrent_regularizer=L1L2(l1=1e-6))) \n",
    "model.add(Dense(257)) \n",
    "model.add(LeakyReLU(0.01)) \n",
    "model.add(LSTM(512, return_sequences=True, kernel_regularizer=L1L2(l1=1e-6), recurrent_regularizer=L1L2(l1=1e-6))) \n",
    "model.add(Dense(257)) \n",
    "model.add(LeakyReLU(0.01)) \n",
    "model.add(LSTM(512, return_sequences=True, kernel_regularizer=L1L2(l1=1e-6), recurrent_regularizer=L1L2(l1=1e-6)))  \n",
    "model.add(Dense(257)) \n",
    "model.add(LeakyReLU(0.01)) \n",
    "model.compile(keras.optimizers.Adam(1e-3, clipnorm=1.), 'mse') \n",
    "model.summary()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500 samples, validate on 40 samples\n",
      "Epoch 1/50\n",
      "500/500 [==============================] - 218s 436ms/step - loss: 0.1900 - val_loss: 0.1629\n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 214s 429ms/step - loss: 0.1369 - val_loss: 0.1360\n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 214s 429ms/step - loss: 0.1136 - val_loss: 0.1188\n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 214s 428ms/step - loss: 0.0993 - val_loss: 0.1000\n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 213s 427ms/step - loss: 0.0886 - val_loss: 0.0909\n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 215s 430ms/step - loss: 0.0825 - val_loss: 0.0854\n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 214s 428ms/step - loss: 0.0775 - val_loss: 0.0814\n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 213s 427ms/step - loss: 0.0739 - val_loss: 0.0810\n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 212s 424ms/step - loss: 0.0715 - val_loss: 0.0770\n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 215s 430ms/step - loss: 0.0699 - val_loss: 0.0743\n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 214s 428ms/step - loss: 0.0675 - val_loss: 0.0712\n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 213s 426ms/step - loss: 0.0658 - val_loss: 0.0701\n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 214s 428ms/step - loss: 0.0643 - val_loss: 0.0738\n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 215s 429ms/step - loss: 0.0639 - val_loss: 0.0686\n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 214s 427ms/step - loss: 0.0619 - val_loss: 0.0679\n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 213s 426ms/step - loss: 0.0617 - val_loss: 0.0664\n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 214s 429ms/step - loss: 0.0604 - val_loss: 0.0654\n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 215s 429ms/step - loss: 0.0599 - val_loss: 0.0653\n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 213s 426ms/step - loss: 0.0595 - val_loss: 0.0642\n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 214s 428ms/step - loss: 0.0585 - val_loss: 0.0644\n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 213s 427ms/step - loss: 0.0584 - val_loss: 0.0646\n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 215s 430ms/step - loss: 0.0577 - val_loss: 0.0633\n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 213s 425ms/step - loss: 0.0571 - val_loss: 0.0634\n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 214s 428ms/step - loss: 0.0567 - val_loss: 0.0654\n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 214s 428ms/step - loss: 0.0568 - val_loss: 0.0630\n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 214s 429ms/step - loss: 0.0562 - val_loss: 0.0629\n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 214s 428ms/step - loss: 0.0558 - val_loss: 0.0621\n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 214s 429ms/step - loss: 0.0556 - val_loss: 0.0624\n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 213s 426ms/step - loss: 0.0553 - val_loss: 0.0613\n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 213s 427ms/step - loss: 0.0558 - val_loss: 0.0619\n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 214s 428ms/step - loss: 0.0553 - val_loss: 0.0612\n",
      "Epoch 32/50\n",
      "500/500 [==============================] - 215s 429ms/step - loss: 0.0546 - val_loss: 0.0611\n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 214s 428ms/step - loss: 0.0543 - val_loss: 0.0610\n",
      "Epoch 34/50\n",
      "500/500 [==============================] - 214s 428ms/step - loss: 0.0541 - val_loss: 0.0602\n",
      "Epoch 35/50\n",
      "500/500 [==============================] - 215s 430ms/step - loss: 0.0540 - val_loss: 0.0601\n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 214s 428ms/step - loss: 0.0539 - val_loss: 0.0613\n",
      "Epoch 37/50\n",
      "500/500 [==============================] - 215s 430ms/step - loss: 0.0537 - val_loss: 0.0605\n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 214s 427ms/step - loss: 0.0532 - val_loss: 0.0600\n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 215s 429ms/step - loss: 0.0530 - val_loss: 0.0594\n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 213s 427ms/step - loss: 0.0534 - val_loss: 0.0617\n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 214s 428ms/step - loss: 0.0540 - val_loss: 0.0606\n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 215s 430ms/step - loss: 0.0536 - val_loss: 0.0619\n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 215s 429ms/step - loss: 0.0530 - val_loss: 0.0592\n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 214s 429ms/step - loss: 0.0522 - val_loss: 0.0599\n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 214s 429ms/step - loss: 0.0526 - val_loss: 0.0609\n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 214s 428ms/step - loss: 0.0532 - val_loss: 0.0593\n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 215s 429ms/step - loss: 0.0520 - val_loss: 0.0598\n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 216s 432ms/step - loss: 0.0523 - val_loss: 0.0587\n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 214s 428ms/step - loss: 0.0520 - val_loss: 0.0592\n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 214s 427ms/step - loss: 0.0518 - val_loss: 0.0594\n"
     ]
    }
   ],
   "source": [
    "# TODO: train on proper noisy ones\n",
    "history1 = model.fit(noisy[:500], masks[:500], epochs=50, batch_size=16, validation_data=[noisy[500:], masks[500:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_13 (LSTM)               (None, None, 500)         1516000   \n",
      "_________________________________________________________________\n",
      "feature_selection_layer_16 ( (None, None, 500)         2500      \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, None, 257)         128757    \n",
      "_________________________________________________________________\n",
      "feature_selection_layer_17 ( (None, None, 257)         1285      \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, None, 500)         1516000   \n",
      "_________________________________________________________________\n",
      "feature_selection_layer_18 ( (None, None, 500)         2500      \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, None, 257)         128757    \n",
      "_________________________________________________________________\n",
      "feature_selection_layer_19 ( (None, None, 257)         1285      \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, None, 500)         1516000   \n",
      "_________________________________________________________________\n",
      "feature_selection_layer_20 ( (None, None, 500)         2500      \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, None, 257)         128757    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, None, 257)         0         \n",
      "=================================================================\n",
      "Total params: 4,944,341\n",
      "Trainable params: 4,944,341\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, LSTM, Conv2D, LeakyReLU, BatchNormalization, Flatten, TimeDistributed, Lambda\n",
    "from keras.regularizers import L1L2\n",
    "\n",
    "model2 = keras.models.Sequential()\n",
    "model2.add(LSTM(500, input_shape=(None, 257), return_sequences=True, kernel_regularizer=L1L2(l1=1e-6), recurrent_regularizer=L1L2(l1=1e-6))) \n",
    "model2.add(FeatureSelectionLayer())\n",
    "model2.add(Dense(257)) \n",
    "model2.add(FeatureSelectionLayer())\n",
    "model2.add(LSTM(500, return_sequences=True, kernel_regularizer=L1L2(l1=1e-6), recurrent_regularizer=L1L2(l1=1e-6))) \n",
    "model2.add(FeatureSelectionLayer())\n",
    "model2.add(Dense(257)) \n",
    "model2.add(FeatureSelectionLayer())\n",
    "model2.add(LSTM(500, return_sequences=True, kernel_regularizer=L1L2(l1=1e-6), recurrent_regularizer=L1L2(l1=1e-6)))  \n",
    "model2.add(FeatureSelectionLayer())\n",
    "model2.add(Dense(257)) \n",
    "model2.add(LeakyReLU(0.01)) \n",
    "model2.compile(keras.optimizers.Adam(1e-3, clipnorm=1.), 'mse') \n",
    "model2.summary()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500 samples, validate on 40 samples\n",
      "Epoch 1/50\n",
      "500/500 [==============================] - 227s 454ms/step - loss: 0.2701 - val_loss: 0.1892\n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 223s 445ms/step - loss: 0.1728 - val_loss: 0.1601\n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 224s 448ms/step - loss: 0.1539 - val_loss: 0.1508\n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 223s 445ms/step - loss: 0.1495 - val_loss: 0.1482\n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 222s 445ms/step - loss: 0.1480 - val_loss: 0.2995\n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 223s 446ms/step - loss: 0.2913 - val_loss: 0.2978\n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 223s 445ms/step - loss: 0.1946 - val_loss: 0.1567\n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 223s 447ms/step - loss: 0.1527 - val_loss: 0.1504\n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 224s 448ms/step - loss: 0.1505 - val_loss: 0.1491\n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 224s 448ms/step - loss: 0.1495 - val_loss: 0.1482\n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 224s 447ms/step - loss: 0.1480 - val_loss: 0.1478\n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 223s 447ms/step - loss: 0.1468 - val_loss: 0.1467\n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 224s 447ms/step - loss: 0.1478 - val_loss: 0.1477\n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 223s 446ms/step - loss: 0.1469 - val_loss: 0.1469\n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 222s 444ms/step - loss: 0.1459 - val_loss: 0.1457\n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 223s 446ms/step - loss: 0.1462 - val_loss: 0.1452\n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 225s 450ms/step - loss: 0.1455 - val_loss: 0.1453\n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 225s 450ms/step - loss: 0.1451 - val_loss: 0.1445\n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 224s 448ms/step - loss: 0.1447 - val_loss: 0.1455\n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 224s 447ms/step - loss: 0.2302 - val_loss: 0.2946\n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 225s 450ms/step - loss: 0.1749 - val_loss: 0.1494\n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 223s 446ms/step - loss: 0.1488 - val_loss: 0.1475\n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 225s 449ms/step - loss: 0.1467 - val_loss: 0.1450\n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 224s 448ms/step - loss: 0.1588 - val_loss: 0.1522\n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 223s 446ms/step - loss: 0.1515 - val_loss: 0.1503\n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 223s 447ms/step - loss: 0.1496 - val_loss: 0.1510\n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 225s 450ms/step - loss: 0.1489 - val_loss: 0.1473\n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 223s 446ms/step - loss: 0.1475 - val_loss: 0.1468\n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 224s 448ms/step - loss: 0.1478 - val_loss: 0.1466\n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 224s 447ms/step - loss: 0.1487 - val_loss: 0.1491\n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 224s 448ms/step - loss: 0.1472 - val_loss: 0.1460\n",
      "Epoch 32/50\n",
      "500/500 [==============================] - 224s 449ms/step - loss: 0.1461 - val_loss: 0.1450\n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 225s 449ms/step - loss: 0.1450 - val_loss: 0.1453\n",
      "Epoch 34/50\n",
      "500/500 [==============================] - 224s 448ms/step - loss: 0.1452 - val_loss: 0.1455\n",
      "Epoch 35/50\n",
      "500/500 [==============================] - 225s 450ms/step - loss: 0.1452 - val_loss: 0.1444\n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 224s 449ms/step - loss: 0.1444 - val_loss: 0.1442\n",
      "Epoch 37/50\n",
      "500/500 [==============================] - 225s 450ms/step - loss: 0.1442 - val_loss: 0.1447\n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 225s 451ms/step - loss: 0.1445 - val_loss: 0.1440\n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 224s 448ms/step - loss: 0.1438 - val_loss: 0.1435\n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 225s 451ms/step - loss: 0.1458 - val_loss: 0.1451\n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 225s 449ms/step - loss: 0.1450 - val_loss: 0.1486\n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 224s 447ms/step - loss: 0.1459 - val_loss: 0.1445\n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 224s 448ms/step - loss: 0.1460 - val_loss: 0.1468\n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 225s 450ms/step - loss: 0.1551 - val_loss: 0.1498\n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 226s 452ms/step - loss: 0.1497 - val_loss: 0.1486\n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 224s 448ms/step - loss: 0.1567 - val_loss: 0.1516\n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 223s 447ms/step - loss: 0.1504 - val_loss: 0.1501\n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 224s 448ms/step - loss: 0.1487 - val_loss: 0.1589\n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 224s 448ms/step - loss: 0.1627 - val_loss: 0.1544\n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 224s 448ms/step - loss: 0.1531 - val_loss: 0.1514\n"
     ]
    }
   ],
   "source": [
    "# TODO: train on proper noisy ones\n",
    "history2 = model2.fit(noisy[:500], masks[:500], epochs=50, batch_size=16, validation_data=[noisy[500:], masks[500:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fec82db7710>]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history1.history['loss'])\n",
    "plt.plot(history1.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fec82d720b8>]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history2.history['loss'])\n",
    "plt.plot(history2.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So is it usable after all?\n",
    "\n",
    "Unless we specifically know that the non-linearities are present - no. It adds some complexity to the model, but the selection process is simultaneous with learning. With the cost of compute rescaling the features manually after some fitting tests could be better is separated from the learning process (as with simple datasets we probably better would make use of some xgboosted trees on a larger share of features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
